import uuid
import random
import os
import json
import re
import logging
from dotenv import load_dotenv
from openai import AsyncOpenAI
from typing import List, Dict, Optional

# Import prompt templates
from prompts import (
    get_prompt,
    TUTOR_SYSTEM_BASE,
    TUTOR_METADATA_RULE,
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Mock AI Service with capabilities to switch to Real API
class AIService:
    """
    Abstraction layer for AI model interactions.
    Supports switching between different models based on task complexity.
    """
    def __init__(self):
        # Configure API Key via environment variable
        self.api_key = os.getenv("AI_API_KEY")
        self.api_base = os.getenv("AI_API_BASE", "https://api-inference.modelscope.cn/v1")
        
        # Hybrid Model Strategy
        # Smart Model: For complex reasoning, creative writing, and detailed explanations.
        self.model_smart = os.getenv("AI_MODEL", "Qwen/Qwen3-32B")
        
        # Fast Model: For summarization, classification, and simple tasks.
        # Default to a smaller, faster model if not specified.
        self.model_fast = os.getenv("AI_MODEL_FAST", "Qwen/Qwen3-32B")
        
        self.client = AsyncOpenAI(
            base_url=self.api_base,
            api_key=self.api_key,
        )

    def _extract_json(self, text: str) -> Optional[Dict]:
        """
        Robust JSON extraction from LLM response.
        Handles Markdown blocks, plain text, and potential noise.
        """
        logger.info(f"Raw AI Response for JSON extraction: {text[:200]}...")

        try:
            # First try direct parsing
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Try to find JSON block in markdown
        # Relaxed regex to capture content between ```json and ```
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError as e:
                logger.warning(f"Markdown JSON decode error: {e}")
                pass

        # Try to find any code block
        code_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if code_match:
            try:
                return json.loads(code_match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find the first '{' and the last '}'
        try:
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx+1]
                return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"Substring JSON decode error: {e}")
            pass

        logger.warning(f"Failed to extract JSON from: {text[:500]}...")
        
        # Debug: Write failed text to file
        try:
            with open("debug_failed_json.txt", "w", encoding="utf-8") as f:
                f.write(text)
        except Exception:
            pass
            
        return None

    def _clean_mermaid_syntax(self, text: str) -> str:
        """
        Fix common Mermaid syntax errors in the text.
        """
        # Regex to find mermaid blocks
        pattern = r'```mermaid(.*?)```'
        
        def fix_mermaid_block(match):
            content = match.group(1)
            
            def quote_if_needed(text, type_char):
                # Check if already quoted (simple check)
                if text.startswith('"') and text.endswith('"'):
                    return text
                
                # Escape quotes inside the text
                text = text.replace('"', '\\"')
                return f'"{text}"'

            # Fix 1: [Text] -> ["Text"] (Rectangular nodes)
            # Exclude content starting with (, [, /, \, < to avoid breaking shapes
            content = re.sub(r'\[(?![(\[/\\<])([^\[\]\n]+?)\]', 
                             lambda m: f'[{quote_if_needed(m.group(1), "[")}]', 
                             content)
            
            # Fix 2: (Text) -> ("Text") (Round nodes)
            # Exclude content starting with ( to avoid breaking shapes
            content = re.sub(r'\((?!\()([^()\n]+?)\)', 
                             lambda m: f'({quote_if_needed(m.group(1), "(")})', 
                             content)
            
            # Fix 3: {Text} -> {"Text"} (Rhombus nodes)
            # Exclude {{Hexagon}}
            content = re.sub(r'\{(?![{!])([^{}\n]+?)\}', 
                             lambda m: f'{{{quote_if_needed(m.group(1), "{")}}}', 
                             content)
            
            return f'```mermaid{content}```'

        return re.sub(pattern, fix_mermaid_block, text, flags=re.DOTALL)

    def clean_response_text(self, text: str) -> str:
        """
        Cleans LLM response: strips markdown wrapper and fixes LaTeX and Mermaid.
        """
        clean_text = text.strip()
        # Strip ```markdown wrapper
        if clean_text.startswith("```markdown") and clean_text.endswith("```"):
            clean_text = clean_text[11:-3].strip()
            
        # Fix LaTeX
        pattern = r'(?<!\$)(?<!\$\$)\\begin\{(matrix|pmatrix|bmatrix|vmatrix|Vmatrix|array|align|equation|cases)\}.*?\\end\{\1\}(?!\$)(?!\$\$)'
        clean_text = re.sub(pattern, lambda m: f"\n$$\n{m.group(0)}\n$$\n", clean_text, flags=re.DOTALL)
        
        # Fix Mermaid
        clean_text = self._clean_mermaid_syntax(clean_text)
        
        return clean_text

    async def _call_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.", use_fast_model: bool = False) -> str:
        """
        Generic function to call LLM using OpenAI client.
        Supports Model Routing (Smart vs Fast).
        
        Args:
            prompt: User input prompt
            system_prompt: System instruction
            use_fast_model: If True, uses the lighter/faster model (e.g. for simple summaries)
        """
        if not self.api_key:
            return None # Signal to use mock fallback
        
        try:
            extra_body = {
                "enable_thinking": False
            }
            
            # Select Model
            model_id = self.model_fast if use_fast_model else self.model_smart
            
            response = await self.client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
                extra_body=extra_body
            )
            
            full_content = ""
            async for chunk in response:
                if chunk.choices:
                    # Handle reasoning content if available (for logging/debugging)
                    if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                        reasoning = chunk.choices[0].delta.reasoning_content
                        if reasoning:
                            # Log thinking process to console to match user expectation
                            print(reasoning, end='', flush=True)
                            
                    delta = chunk.choices[0].delta
                    if delta.content:
                        full_content += delta.content
            
            logger.info(f"AI Response Complete (Model: {model_id})")
            return full_content
        except Exception as e:
            logger.error(f"AI API Call Error: {e}")
            return None

    async def generate_course(self, keyword: str, difficulty: str = "medium", style: str = "academic", requirements: str = "") -> Dict:
        system_prompt = get_prompt("generate_course").format(
            difficulty=difficulty,
            style=style,
            requirements=requirements if requirements else "æ— "
        )
        prompt = f"ç”¨æˆ·æƒ³è¦å­¦ä¹ â€œ{keyword}â€ï¼Œè¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”ç³»ç»Ÿçš„è¯¾ç¨‹å¤§çº²ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response)
        return {"course_name": keyword, "nodes": []}

    async def generate_quiz(self, content: str, node_name: str = "", difficulty: str = "medium", style: str = "standard", user_persona: str = "", question_count: int = 3) -> List[Dict]:
        system_prompt = get_prompt("generate_quiz").format(
            difficulty=difficulty,
            style=style,
            question_count=question_count
        )
        
        content_text = content
        if not content or len(content) < 50:
            content_text = f"Topic: {node_name}\n(The detailed content is missing, please generate general questions based on this topic)"
        
        # Explicitly mention question count in the user prompt as well to reinforce it
        prompt = f"Content:\n{content_text}\n\nPlease generate exactly {question_count} questions in JSON format. Remember to use Markdown tables or Mermaid diagrams in 'explanation' if helpful for understanding."
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            result = self._extract_json(response)
            if result:
                return result

        
        # Hard Fallback: If AI fails or returns empty, generate template questions
        # This ensures the user NEVER sees "Cannot generate" error.
        logger.warning(f"Quiz generation failed for {node_name}. Using hard fallback.")
        fallback_topic = node_name if node_name else "æ­¤ä¸»é¢˜"
        fallback_questions = [
            {
                "id": 1,
                "question": f"å…³äºâ€œ{fallback_topic}â€çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä»¥ä¸‹æè¿°æ­£ç¡®çš„æ˜¯ï¼Ÿ",
                "options": [
                    f"{fallback_topic} æ˜¯ä¸€ä¸ªå­¤ç«‹çš„æ¦‚å¿µï¼Œä¸å…¶ä»–çŸ¥è¯†æ— å…³",
                    f"{fallback_topic} æ˜¯è¯¥å­¦ç§‘ä½“ç³»ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†",
                    f"{fallback_topic} å·²ç»è¢«ç°ä»£ç†è®ºå®Œå…¨æ¨ç¿»",
                    f"{fallback_topic} ä»…åœ¨ç‰¹å®šæç«¯æƒ…å†µä¸‹é€‚ç”¨"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic} ä½œä¸ºæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œåœ¨å­¦ç§‘ä½“ç³»ä¸­èµ·ç€æ‰¿ä¸Šå¯ä¸‹çš„ä½œç”¨ï¼Œæ˜¯ç†è§£åç»­å†…å®¹çš„åŸºç¡€ã€‚"
            },
            {
                "id": 2,
                "question": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç†è§£â€œ{fallback_topic}â€ä¸»è¦æœ‰åŠ©äºè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "options": [
                    "å†å²èƒŒæ™¯çš„è€ƒè¯",
                    "å¤æ‚ç³»ç»Ÿä¸­çš„å…³é”®æœºåˆ¶åˆ†æ",
                    "æ— å…³æ•°æ®çš„éšæœºå¤„ç†",
                    "çº¯ç²¹çš„ç†è®ºæ¨å¯¼æ¸¸æˆ"
                ],
                "correct_index": 1,
                "explanation": f"æŒæ¡{fallback_topic}çš„åŸç†ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬åˆ†æå’Œå¤„ç†å®é™…ç³»ç»Ÿä¸­çš„å¤æ‚æœºåˆ¶ä¸å…³é”®é—®é¢˜ã€‚"
            },
            {
                "id": 3,
                "question": f"å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œå­¦ä¹ â€œ{fallback_topic}â€æœ€å¤§çš„æŒ‘æˆ˜é€šå¸¸æ˜¯ï¼Ÿ",
                "options": [
                    "æ¦‚å¿µè¿‡äºç®€å•ï¼Œç¼ºä¹æŒ‘æˆ˜",
                    "ç†è§£å…¶æŠ½è±¡é€»è¾‘ä¸å®é™…åœºæ™¯çš„æ˜ å°„",
                    "ç›¸å…³èµ„æ–™å¤ªå°‘ï¼Œæ— æ³•æŸ¥é˜…",
                    "æ²¡æœ‰ä»»ä½•æŒ‘æˆ˜ï¼Œä¸€å­¦å°±ä¼š"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic}å¾€å¾€åŒ…å«ä¸€å®šçš„æŠ½è±¡é€»è¾‘ï¼Œå°†å…¶å‡†ç¡®æ˜ å°„åˆ°å®é™…åº”ç”¨åœºæ™¯ä¸­æ˜¯åˆå­¦è€…å¸¸è§çš„éš¾ç‚¹ã€‚"
            },
            {
                "id": 4,
                "question": f"ä»¥ä¸‹å“ªé¡¹ä¸æ˜¯â€œ{fallback_topic}â€çš„å…¸å‹ç‰¹å¾ï¼Ÿ",
                "options": [
                    "ç³»ç»Ÿæ€§",
                    "é€»è¾‘æ€§",
                    "éšæ„æ€§",
                    "å®ç”¨æ€§"
                ],
                "correct_index": 2,
                "explanation": f"{fallback_topic}ä½œä¸ºç§‘å­¦æˆ–ä¸“ä¸šçŸ¥è¯†ï¼Œå…·æœ‰ä¸¥å¯†çš„é€»è¾‘å’Œç³»ç»Ÿæ€§ï¼Œç»ééšæ„æ„å»ºã€‚"
            },
            {
                "id": 5,
                "question": f"æ·±å…¥æŒæ¡â€œ{fallback_topic}â€åï¼Œä¸‹ä¸€æ­¥é€šå¸¸åº”è¯¥å­¦ä¹ ï¼Ÿ",
                "options": [
                    "æ”¾å¼ƒè¯¥å­¦ç§‘",
                    "è¯¥é¢†åŸŸçš„è¿›é˜¶ç†è®ºæˆ–ç›¸å…³äº¤å‰å­¦ç§‘",
                    "å®Œå…¨ä¸ç›¸å…³çš„é¢†åŸŸ",
                    "é‡å¤å­¦ä¹ åŸºç¡€æ¦‚å¿µ"
                ],
                "correct_index": 1,
                "explanation": f"åœ¨æŒæ¡åŸºç¡€åï¼Œè¿›é˜¶ç†è®ºæˆ–äº¤å‰å­¦ç§‘çš„åº”ç”¨æ˜¯æ·±å…¥ç ”ç©¶çš„å¿…ç»ä¹‹è·¯ã€‚"
            }
        ]
        
        return fallback_questions[:question_count]

    async def generate_sub_nodes(self, node_name: str, node_level: int, node_id: str, course_name: str = "", parent_context: str = "") -> List[Dict]:
        system_prompt = get_prompt("generate_sub_nodes").format(
            course_name=course_name if course_name else "æœªçŸ¥è¯¾ç¨‹",
            parent_context=parent_context if parent_context else "æ— "
        )
        prompt = f"å½“å‰èŠ‚ç‚¹ä¿¡æ¯ï¼šåç§°={node_name}ï¼Œå±‚çº§={node_level}ã€‚è¯·åˆ—å‡ºè¯¥ç« èŠ‚ä¸‹çš„æ‰€æœ‰å­å°èŠ‚ï¼Œç¡®ä¿ç»“æ„å®Œæ•´ä¸”å…·å¤‡ä¸“ä¸šæ€§ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        new_level = node_level + 1
        
        if response:
            data = self._extract_json(response)
            if data:
                result = []
                for item in data.get("sub_nodes", []):
                    result.append({
                        "node_id": str(uuid.uuid4()),
                        "parent_node_id": node_id,
                        "node_name": item.get("node_name", "æ–°èŠ‚ç‚¹"),
                        "node_level": new_level,
                        "node_content": item.get("node_content", ""),
                        "node_type": "custom"
                    })
                return result

        return [
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 1", "node_level": new_level, "node_content": "", "node_type": "custom"},
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 2", "node_level": new_level, "node_content": "", "node_type": "custom"}
        ]

    async def _stream_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.", use_fast_model: bool = False):
        """
        Generator function to stream LLM response chunks.
        """
        if not self.api_key:
            yield "AI Service not configured."
            return

        try:
            extra_body = {
                "enable_thinking": False
            }
            
            # Select Model
            model_id = self.model_fast if use_fast_model else self.model_smart

            response = await self.client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
                extra_body=extra_body
            )
            
            async for chunk in response:
                if chunk.choices:
                    # Handle reasoning content if available (for logging/debugging)
                    if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                        reasoning = chunk.choices[0].delta.reasoning_content
                        if reasoning:
                             # We can log thinking process or just ignore it for now
                             pass
                    
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
        except Exception as e:
            logger.error(f"Stream Error: {e}")
            yield f"\n[Error: {str(e)}]"

    async def redefine_node_content(self, node_name: str, original_content: str, requirement: str, course_context: str = "", previous_context: str = ""):
        """
        Stream version of redefine_content with book-level context awareness.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½èµ„æ·±å­¦ç§‘ä¸“å®¶ã€ä¸–ç•Œé¡¶å°–å¤§å­¦çš„ç»ˆèº«æ•™æˆï¼Œå¹¶æ‹¥æœ‰ä¸€çº¿å¤§å‚çš„é¦–å¸­æ¶æ„å¸ˆèƒŒæ™¯ã€‚

## å­¦æœ¯å®šä½
- **å—ä¼—**ï¼šå¤§å­¦æœ¬ç§‘ç”Ÿã€ç ”ç©¶ç”ŸåŠä¸“ä¸šæŠ€æœ¯äººå‘˜
- **ç›®æ ‡**ï¼šæ„å»ºç³»ç»ŸåŒ–ã€ç†è®ºè”ç³»å®é™…çš„çŸ¥è¯†ä½“ç³»ï¼Œä¸ä»…è®²â€œæ˜¯ä»€ä¹ˆâ€ï¼Œæ›´è®²â€œä¸ºä»€ä¹ˆâ€å’Œâ€œæ€ä¹ˆåšâ€
- **æ ‡å‡†**ï¼šç¬¦åˆå­¦æœ¯è§„èŒƒå’Œè¡Œä¸šæ ‡å‡†
- **é£æ ¼**ï¼šä¸“ä¸šä¸¥è°¨ï¼Œæ·±å…¥æµ…å‡ºï¼Œæ‹’ç»ç§‘æ™®æ€§è´¨çš„æµ…å±‚ä»‹ç»

## å†…å®¹æ¶æ„è¦æ±‚
### æ ¸å¿ƒè¾“å‡ºç»“æ„ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œå¹¶ç”¨ `<!-- BODY_START -->` åˆ†éš”ï¼š
1. **ç¬¬ä¸€éƒ¨åˆ†ï¼šå­¦æœ¯æ€§å¯¼è¨€ï¼ˆAnnotationï¼‰**
   - ç®€çŸ­çš„å¯¼è¯»æˆ–æ‰¹æ³¨ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚
   - é˜è¿°æœ¬ç« åœ¨å­¦ç§‘ä½“ç³»ä¸­çš„åœ°ä½å’Œä»·å€¼ï¼Œæ¦‚è¿°æ ¸å¿ƒé—®é¢˜å’Œç ”ç©¶æ„ä¹‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹å‰ã€‚
2. **åˆ†éš”ç¬¦**
   - å¿…é¡»ä¸¥æ ¼è¾“å‡º `<!-- BODY_START -->` å­—ç¬¦ä¸²ã€‚
3. **ç¬¬äºŒéƒ¨åˆ†ï¼šä¸“ä¸šæ­£æ–‡å†…å®¹ï¼ˆMain Bodyï¼‰**
   - è¯¦ç»†çš„æ•™ç§‘ä¹¦å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹åã€‚

### å†…å®¹è´¨é‡æ ‡å‡†
1. **å­¦æœ¯æ·±åº¦**
   - æ·±å…¥å‰–ææ¦‚å¿µçš„ç†è®ºåŸºç¡€å’Œå†å²æ¸Šæº
   - åˆ†ææŠ€æœ¯åŸç†çš„æ•°å­¦æˆ–é€»è¾‘åŸºç¡€
   - æ¢è®¨æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œå±€é™æ€§

2. **ä¸“ä¸šè¡¨è¾¾**
   - ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼
   - é¿å…ç”Ÿæ´»åŒ–æ¯”å–»ï¼Œé‡‡ç”¨ä¸“ä¸šç±»æ¯”
   - å¼•ç”¨æƒå¨ç ”ç©¶å’Œå®è¯æ•°æ®

3. **ç»“æ„ä¸¥è°¨æ€§**
   - æ­£æ–‡ç»“æ„åº”åŒ…å«ï¼š
     - **### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µä¸èƒŒæ™¯**ï¼šæ¸…æ™°å®šä¹‰ + äº§ç”ŸèƒŒæ™¯/æ ¸å¿ƒä»·å€¼ï¼ˆå…³é”®åè¯ä½¿ç”¨ **åŠ ç²—** å¼ºè°ƒï¼‰
     - **### ğŸ” æ·±åº¦åŸç†/åº•å±‚æœºåˆ¶**ï¼šæ·±å…¥å‰–æå·¥ä½œåŸç†ã€åº•å±‚é€»è¾‘ã€æ•°å­¦æ¨¡å‹æˆ–æ¼”åŒ–é€»è¾‘ï¼ˆé‡ä¸­ä¹‹é‡ï¼‰
     - **### ğŸ› ï¸ æŠ€æœ¯å®ç°/æ–¹æ³•è®º**ï¼šå…·ä½“çš„æ¨å¯¼è¿‡ç¨‹ã€ç®—æ³•æ­¥éª¤æˆ–æ‰§è¡Œç»†èŠ‚
     - **### ğŸ¨ å¯è§†åŒ–å›¾è§£**ï¼š**å¿…é¡»**åŒ…å«è‡³å°‘ä¸€ä¸ª Mermaid å›¾è¡¨ï¼ˆæµç¨‹å›¾æˆ–æ—¶åºå›¾ï¼‰ã€‚IDçº¯è‹±æ–‡æ— ç©ºæ ¼ï¼Œæ–‡æœ¬åŒå¼•å·åŒ…è£¹ã€‚
     - **### ğŸ­ å®æˆ˜æ¡ˆä¾‹/è¡Œä¸šåº”ç”¨**ï¼šç»“åˆçœŸå®äº§ä¸šç•Œçš„è½åœ°æ¡ˆä¾‹è¿›è¡Œåˆ†æ
     - **### âœ… æ€è€ƒä¸æŒ‘æˆ˜**ï¼šæä¾› 1-2 ä¸ªèƒ½å¼•å‘æ·±åº¦æ€è€ƒçš„é—®é¢˜

### æŠ€æœ¯è§„èŒƒ
- **å›¾è¡¨ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**ï¼šæ¯ç« **å¿…é¡»**åŒ…å«è‡³å°‘ä¸€å¼  Mermaid å›¾è¡¨ï¼ˆå¦‚æµç¨‹å›¾ã€æ—¶åºå›¾ã€ç±»å›¾æˆ–æ€ç»´å¯¼å›¾ï¼‰ï¼Œç”¨äºç›´è§‚è§£é‡Šæ ¸å¿ƒæ¦‚å¿µæˆ–æµç¨‹ã€‚
  - èŠ‚ç‚¹ ID å¿…é¡»çº¯è‹±æ–‡ï¼Œä¸¥ç¦ä¸­æ–‡æˆ–ç‰¹æ®Šç¬¦å·ã€‚
  - èŠ‚ç‚¹æ–‡æœ¬å¿…é¡»åŒå¼•å·åŒ…è£¹ã€‚
- **å…¬å¼è§„èŒƒï¼ˆç»å¯¹ä¸¥æ ¼æ‰§è¡Œï¼‰**
  - è¡Œå†…å…¬å¼ï¼šå¿…é¡»ä½¿ç”¨ `$å…¬å¼$` æ ¼å¼ï¼Œå†…éƒ¨ä¸è¦æœ‰ç©ºæ ¼ï¼ˆä¾‹å¦‚ `$E=mc^2$`ï¼‰
  - å—çº§å…¬å¼ï¼šå¿…é¡»ä½¿ç”¨ `$$` åŒ…è£¹ï¼Œä¸”ç‹¬å ä¸€è¡Œ
  - ä¸¥ç¦è£¸å†™ LaTeX å‘½ä»¤
- **å‚è€ƒæ–‡çŒ®**ï¼šç¬¦åˆå­¦æœ¯å¼•ç”¨è§„èŒƒ

### ç¯‡å¹…ä¸è¾“å‡º
- **å­—æ•°**ï¼š800-1500 å­—ï¼Œç¡®ä¿è§£é‡Šé€å½»ã€‚
- **è¾“å‡º**ï¼šç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼ŒåŒ…å«åˆ†éš”ç¬¦ã€‚
"""
        # Inject Style and Difficulty context from requirement string if possible
        # Since 'requirement' is just a string, we append it directly.
        
        prompt = f"""
å…¨ä¹¦å¤§çº²ï¼š
{course_context}

ä¸Šæ–‡æ‘˜è¦ï¼ˆç”¨äºæ‰¿æ¥ï¼‰ï¼š
{previous_context}

å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}
åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š{original_content}
ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}

è¯·å¼€å§‹æ’°å†™ï¼ˆè®°å¾—åŒ…å« <!-- BODY_START --> åˆ†éš”ç¬¦ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def redefine_content(self, node_name: str, requirement: str, original_content: str = "", course_context: str = "", previous_context: str = "") -> str:
        """
        Refine the content of a node based on specific requirements.
        Uses advanced prompt engineering for better structure and clarity.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½èµ„æ·±å­¦ç§‘ä¸“å®¶ã€ä¸–ç•Œé¡¶å°–å¤§å­¦çš„ç»ˆèº«æ•™æˆï¼Œå¹¶æ‹¥æœ‰ä¸€çº¿å¤§å‚çš„é¦–å¸­æ¶æ„å¸ˆèƒŒæ™¯ã€‚

## å­¦æœ¯å®šä½
- **å—ä¼—**ï¼šå¤§å­¦æœ¬ç§‘ç”Ÿã€ç ”ç©¶ç”ŸåŠä¸“ä¸šæŠ€æœ¯äººå‘˜
- **ç›®æ ‡**ï¼šæ„å»ºç³»ç»ŸåŒ–ã€ç†è®ºè”ç³»å®é™…çš„çŸ¥è¯†ä½“ç³»ï¼Œä¸ä»…è®²â€œæ˜¯ä»€ä¹ˆâ€ï¼Œæ›´è®²â€œä¸ºä»€ä¹ˆâ€å’Œâ€œæ€ä¹ˆåšâ€
- **æ ‡å‡†**ï¼šç¬¦åˆå­¦æœ¯è§„èŒƒå’Œè¡Œä¸šæ ‡å‡†
- **é£æ ¼**ï¼šä¸“ä¸šä¸¥è°¨ï¼Œæ·±å…¥æµ…å‡ºï¼Œæ‹’ç»ç§‘æ™®æ€§è´¨çš„æµ…å±‚ä»‹ç»

## æ ¸å¿ƒä»»åŠ¡
æ ¹æ®ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ï¼Œé‡æ–°æ’°å†™æˆ–è°ƒæ•´ç« èŠ‚å†…å®¹ã€‚

## å¤„ç†åŸåˆ™
1. **ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§**ï¼šå³ä½¿è°ƒæ•´é£æ ¼ï¼Œä¹Ÿä¸é™ä½å†…å®¹è´¨é‡
2. **å“åº”ç”¨æˆ·éœ€æ±‚**ï¼šä¼˜å…ˆæ»¡è¶³ç”¨æˆ·çš„æ˜ç¡®è¦æ±‚
3. **ç»´æŒç»“æ„å®Œæ•´æ€§**ï¼šä¿æŒåŸæœ‰çš„ç« èŠ‚ç»“æ„å’Œé€»è¾‘æ¡†æ¶
4. **è¡”æ¥ä¸Šä¸‹æ–‡**ï¼šç¡®ä¿ä¸å‰åç« èŠ‚å†…å®¹çš„è¿è´¯æ€§

## å†…å®¹è´¨é‡æ ‡å‡†
1. **ä¸“ä¸šä¸¥è°¨**ï¼šå‡†ç¡®ä½¿ç”¨å­¦æœ¯æœ¯è¯­ï¼Œå®šä¹‰æ¸…æ™°ï¼Œæ¨å¯¼ä¸¥å¯†
2. **æ·±åº¦è§£æ**ï¼šä¸ä»…åœç•™åœ¨è¡¨é¢å®šä¹‰ï¼Œæ·±å…¥å‰–æèƒŒåçš„åŸç†å’Œæœºåˆ¶
3. **åœºæ™¯åŒ–è§£é‡Š**ï¼šä½¿ç”¨å…·ä½“çš„è¡Œä¸šåº”ç”¨åœºæ™¯æˆ–æŠ€æœ¯åœºæ™¯è¾…åŠ©è§£é‡Šï¼Œè€Œéç®€å•çš„ç”Ÿæ´»ç±»æ¯”
4. **é€»è¾‘è¿è´¯**ï¼šæ®µè½ä¹‹é—´è¿‡æ¸¡è‡ªç„¶ï¼Œè®ºè¯ä¸¥å¯†

## ç»“æ„åŒ–å†™ä½œè¦æ±‚
- **### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µä¸èƒŒæ™¯**ï¼šæ¸…æ™°å®šä¹‰ + äº§ç”ŸèƒŒæ™¯/æ ¸å¿ƒä»·å€¼ï¼ˆå…³é”®åè¯ä½¿ç”¨ **åŠ ç²—** å¼ºè°ƒï¼‰
- **### ğŸ” æ·±åº¦åŸç†/åº•å±‚æœºåˆ¶**ï¼šæ·±å…¥å‰–æå·¥ä½œåŸç†ã€åº•å±‚é€»è¾‘ã€æ•°å­¦æ¨¡å‹æˆ–æ¼”åŒ–é€»è¾‘ï¼ˆé‡ä¸­ä¹‹é‡ï¼‰
- **### ğŸ› ï¸ æŠ€æœ¯å®ç°/æ–¹æ³•è®º**ï¼šå…·ä½“çš„æ¨å¯¼è¿‡ç¨‹ã€ç®—æ³•æ­¥éª¤æˆ–æ‰§è¡Œç»†èŠ‚
- **### ğŸ¨ å¯è§†åŒ–å›¾è§£**ï¼š**å¿…é¡»**åŒ…å«è‡³å°‘ä¸€ä¸ª Mermaid å›¾è¡¨ï¼ˆæµç¨‹å›¾æˆ–æ—¶åºå›¾ï¼‰ã€‚IDçº¯è‹±æ–‡æ— ç©ºæ ¼ï¼Œæ–‡æœ¬åŒå¼•å·åŒ…è£¹ã€‚
- **### ğŸ­ å®æˆ˜æ¡ˆä¾‹/è¡Œä¸šåº”ç”¨**ï¼šç»“åˆçœŸå®äº§ä¸šç•Œçš„è½åœ°æ¡ˆä¾‹è¿›è¡Œåˆ†æ
- **### âœ… æ€è€ƒä¸æŒ‘æˆ˜**ï¼šæä¾› 1-2 ä¸ªèƒ½å¼•å‘æ·±åº¦æ€è€ƒçš„é—®é¢˜

## æŠ€æœ¯è§„èŒƒ
- **å›¾è¡¨ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**ï¼šæ¯ç« **å¿…é¡»**åŒ…å«è‡³å°‘ä¸€å¼  Mermaid å›¾è¡¨ã€‚
- **å…¬å¼è§„èŒƒï¼ˆç»å¯¹ä¸¥æ ¼æ‰§è¡Œï¼‰**
  - è¡Œå†…å…¬å¼ï¼šå¿…é¡»ä½¿ç”¨ `$å…¬å¼$` æ ¼å¼ï¼Œå†…éƒ¨ä¸è¦æœ‰ç©ºæ ¼ï¼ˆä¾‹å¦‚ `$E=mc^2$`ï¼‰
  - å—çº§å…¬å¼ï¼šå¿…é¡»ä½¿ç”¨ `$$` åŒ…è£¹ï¼Œä¸”ç‹¬å ä¸€è¡Œ
  - ä¸¥ç¦è£¸å†™ LaTeX å‘½ä»¤

## ç¯‡å¹…è¦æ±‚
**800-1500å­—**ï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚å¯é€‚å½“è°ƒæ•´ã€‚

## è¾“å‡ºæ ¼å¼
- ç›´æ¥è¾“å‡º **Markdown æ­£æ–‡**ã€‚
"""
        prompt_parts = [f"å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}"]
        if course_context:
            prompt_parts.append(f"å…¨ä¹¦å¤§çº²ï¼š\n{course_context}")
        if previous_context:
            prompt_parts.append(f"ä¸Šæ–‡æ‘˜è¦ï¼š\n{previous_context}")
        if original_content:
            prompt_parts.append(f"åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š\n{original_content}")
            
        prompt_parts.append(f"ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}ï¼ˆè¯·ä¿æŒä¸“ä¸šã€ç®€æ´ã€æµç•…ï¼Œé€‚åˆå¤§å­¦ç”Ÿé˜…è¯»ï¼‰")
        prompt_parts.append("è¯·å¼€å§‹æ’°å†™æ­£æ–‡ï¼š")
        
        prompt = "\n\n".join(prompt_parts)
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)
                
        return f"åŸºäºéœ€æ±‚ '{requirement}' é‡å®šä¹‰çš„ {node_name} å†…å®¹ã€‚\n\n1. æ ¸å¿ƒç‚¹ä¸€ï¼š...\n2. æ ¸å¿ƒç‚¹äºŒï¼š...\n(å‚è€ƒæ¥æºï¼šæƒå¨èµ„æ–™)"

    async def extend_content(self, node_name: str, requirement: str) -> str:
        system_prompt = """
ä½ æ˜¯å­¦æœ¯è§†é‡æ‹“å±•ä¸“å®¶ï¼Œéœ€ä¸ºå½“å‰æ•™ç§‘ä¹¦ç« èŠ‚è¡¥å……å…·æœ‰æ·±åº¦çš„å»¶ä¼¸é˜…è¯»ææ–™ã€‚
è¦æ±‚ï¼š
1. **å—ä¼—å®šä½**ï¼šé¢å‘å¤§å­¦ç”ŸåŠä¸“ä¸šäººå£«ï¼Œæ‹’ç»ç§‘æ™®æ€§è´¨çš„æµ…å±‚ä»‹ç»ã€‚
2. **æ‹“å±•æ–¹å‘**ï¼šé‡ç‚¹è¡¥å……å­¦æœ¯ç•Œçš„å‰æ²¿ç ”ç©¶ã€å·¥ä¸šç•Œçš„å·¥ç¨‹é™·é˜±ã€åº•å±‚æ•°å­¦åŸç†æˆ–è·¨å­¦ç§‘çš„æ·±åº¦å…³è”ã€‚
3. **å†…å®¹é£æ ¼**ï¼šä¸“ä¸šã€å¹²ç»ƒã€é€»è¾‘ä¸¥å¯†ã€‚
4. **æ ¼å¼è§„èŒƒ**ï¼šå†…å®¹å……å®ï¼ˆ300-500 å­—ï¼‰ï¼Œå¯ä½¿ç”¨â€œå»¶ä¼¸é˜…è¯»â€æˆ–â€œæ·±åº¦æ€è€ƒâ€ä½œä¸ºæ ‡é¢˜ã€‚
5. **å…¬å¼è§„èŒƒ**ï¼š
   - è¡Œå†…å…¬å¼ç”¨ `$å…¬å¼$`ï¼ˆ**å†…éƒ¨ä¸è¦æœ‰ç©ºæ ¼**ï¼‰ã€‚
   - å—çº§å…¬å¼ç”¨ `$$` åŒ…è£¹ã€‚
   - ä¸¥ç¦è£¸å†™ LaTeX å‘½ä»¤ã€‚
6. **è¾“å‡ºæ ¼å¼**ï¼šç›´æ¥è¾“å‡º **Markdown æ ¼å¼çš„å†…å®¹**ï¼Œ**ä¸éœ€è¦**åŒ…å«åœ¨ JSON å¯¹è±¡ä¸­ã€‚
"""
        prompt = f"å½“å‰ç« èŠ‚ï¼š{node_name}\næ‹“å±•æ–¹å‘ï¼š{requirement}"

        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)

        return f"æ‹“å±•çŸ¥è¯†ç‚¹ï¼š\nå…³äº {node_name} çš„å»¶ä¼¸é˜…è¯»... {requirement}"

    async def answer_question_stream(self, question: str, context: str, history: List[dict] = [], selection: str = "", user_persona: str = "", course_id: str = None, node_id: str = None, user_notes: str = ""):
        """
        Stream answer with metadata appended at the end.
        Structure: [Answer Content] \n\n---METADATA---\n [JSON Metadata]
        """
        system_prompt = ""
        
        # Try to use Dual Memory System if context is available
        if course_id and node_id:
            try:
                # Local import to avoid circular dependency if any
                from memory import memory_controller
                
                # 1. Optimize History (Context Compression)
                # Pass the summarizer method from this instance to avoid circular dependency
                optimized_history = await memory_controller.optimize_history(history, self.summarize_history)
                
                # 2. Build Dual Memory Prompt
                system_prompt = memory_controller.build_tutor_prompt(course_id, node_id, question, optimized_history)
                
                # Use optimized history for prompt construction
                history = optimized_history
                
                # Append the metadata instruction which is critical for frontend parsing
                # We inject the current node_id as default if AI doesn't find a better one
                system_prompt += f"""

=== METADATA OUTPUT RULE (MANDATORY) ===
You MUST output the metadata at the very end of your response.

**Format**:
[Your Answer Content Here]

---METADATA---
{{"node_id": "{node_id}", "quote": "quote from text if any", "anno_summary": "Core knowledge points summary in Markdown bullet points (3-5 points)"}}

DO NOT wrap the JSON in markdown code blocks.
"""
            except Exception as e:
                logger.error(f"Dual Memory Error: {e}")
                # Fallback will be handled below
        
        if not system_prompt:
            # Fallback / Standard Prompt
            system_prompt = f"""
ä½ æ˜¯å­¦æœ¯åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„è¯¾ç¨‹å†…å®¹ã€å¯¹è¯å†å²å’Œé€‰ä¸­çš„æ–‡æœ¬å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**ç”¨æˆ·ç”»åƒï¼ˆä¸ªæ€§åŒ–è®¾å®šï¼‰**ï¼š
{user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"}
è¯·æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´ä½ çš„å›ç­”é£æ ¼ã€æ·±åº¦å’Œä¸¾ä¾‹æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·æ˜¯åˆå­¦è€…ï¼Œè¯·å¤šç”¨ç”Ÿæ´»ç±»æ¯”ï¼›å¦‚æœæ˜¯ä¸“å®¶ï¼Œè¯·æ·±å…¥åº•å±‚åŸç†ã€‚

**æ ¸å¿ƒä»»åŠ¡**ï¼š
1. **å›ç­”é—®é¢˜**ï¼šç›´æ¥ã€ä¸“ä¸šã€ç®€æ´åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
2. **å®šä½ä¸Šä¸‹æ–‡**ï¼šè¯†åˆ«ç­”æ¡ˆå…³è”çš„è¯¾ç¨‹ç« èŠ‚æˆ–åŸæ–‡ã€‚
3. **æ ¼å¼åŒ–è¾“å‡º**ï¼š
   - **è¡¨æ ¼**ï¼šå‡¡æ˜¯æ¶‰åŠå¯¹æ¯”ã€æ•°æ®åˆ—ä¸¾ã€æ­¥éª¤è¯´æ˜çš„å†…å®¹ï¼Œ**å¿…é¡»ä½¿ç”¨ Markdown è¡¨æ ¼**å±•ç¤ºã€‚
   - **å›¾è¡¨**ï¼šå‡¡æ˜¯æ¶‰åŠæµç¨‹ã€æ¶æ„ã€æ€ç»´å¯¼å›¾çš„å†…å®¹ï¼Œ**å¿…é¡»ä½¿ç”¨ Mermaid ä»£ç å—**å±•ç¤ºã€‚
   - **ä»£ç **ï¼šä»£ç ç‰‡æ®µè¯·ä½¿ç”¨æ ‡å‡†ä»£ç å—ã€‚

**æ•™å¸ˆæ¨¡å¼ï¼ˆTEACHER MODE - å¢å¼ºç‰ˆï¼‰**ï¼š
è¯·åƒä¸€ä½çœŸå®çš„è‹æ ¼æ‹‰åº•å¼å¯¼å¸ˆï¼ˆSocratic Tutorï¼‰ä¸€æ ·ï¼š
1. **å¯å‘å¼æ•™å­¦**ï¼š
   - ä¸è¦ç›´æ¥ç»™å‡ºä¸€å±‚ä¸å˜çš„ç­”æ¡ˆã€‚
   - å›ç­”å®Œé—®é¢˜åï¼Œ**å¿…é¡»**ä¸»åŠ¨æå‡ºä¸€ä¸ªç›¸å…³çš„ã€æœ‰æ·±åº¦çš„åç»­é—®é¢˜ï¼ˆFollow-up Questionï¼‰ï¼Œå¼•å¯¼ç”¨æˆ·è¿›ä¸€æ­¥æ€è€ƒã€‚
   - é—®é¢˜åº”è¯¥åŸºäºå½“å‰çš„çŸ¥è¯†ç‚¹ï¼Œæˆ–è€…æ˜¯å°†ç†è®ºè”ç³»å®é™…çš„åœºæ™¯é¢˜ã€‚
2. **å…³è”è®°å¿†ï¼ˆMemory Recallï¼‰**ï¼š
   - å¦‚æœç”¨æˆ·ä¹‹å‰é—®è¿‡ç±»ä¼¼é—®é¢˜æˆ–çŠ¯è¿‡ç±»ä¼¼é”™è¯¯ï¼ˆå‚è€ƒå¯¹è¯å†å²ï¼‰ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºï¼šâ€œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„...â€æˆ–â€œæ³¨æ„ä¸è¦æ··æ·†...â€ã€‚
3. **å®šä½åŸæ–‡ï¼ˆLocateï¼‰**ï¼š
   - å°½é‡åœ¨æä¾›çš„è¯¾ç¨‹å†…å®¹ä¸­æ‰¾åˆ°èƒ½å¤Ÿæ”¯æŒä½ å›ç­”çš„**åŸå¥**ã€‚
   - å°†æ‰¾åˆ°çš„åŸå¥æ”¾å…¥ metadata çš„ `quote` å­—æ®µä¸­ã€‚å‰ç«¯ç•Œé¢ä¼šè‡ªåŠ¨é«˜äº®æ˜¾ç¤ºè¿™å¥è¯ï¼Œå°±åƒè€å¸ˆåœ¨è¯¾æœ¬ä¸Šåˆ’çº¿ä¸€æ ·ã€‚
   - å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŸå¥ï¼Œä¸è¦ç¼–é€ ã€‚
4. **æ€»ç»“ç¬”è®°ï¼ˆNote Takingï¼‰**ï¼š
   - åœ¨ `anno_summary` ä¸­ç”Ÿæˆä¸€ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹æ¦‚æ‹¬ï¼ˆMarkdown åˆ—è¡¨ï¼Œ3-5ç‚¹ï¼‰ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿå›é¡¾ã€‚

**åˆ›æ–°æƒ³æ³•æ•æ‰ï¼ˆInnovation Captureï¼‰**ï¼š
- å¦‚æœç”¨æˆ·æå‡ºäº†æ–°çš„è§£æ³•ã€æ€è·¯æˆ–ç‹¬ç‰¹çš„è§è§£ï¼Œè¯·äºˆä»¥ç§¯æåé¦ˆã€‚
- å¸®åŠ©ç”¨æˆ·å®Œå–„æ€è·¯ï¼Œå¹¶æ ‡è®°è¿™æ˜¯ä¸€ä¸ªâ€œåˆ›æ–°æƒ³æ³•â€ã€‚
- åœ¨ metadata çš„ `anno_summary` ä¸­ï¼Œä½¿ç”¨ `ğŸ’¡ æƒ³æ³•ï¼š` å¼€å¤´ã€‚

**è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰**ï¼š
ä¸ºäº†æ”¯æŒæµå¼è¾“å‡ºå’Œåç»­å¤„ç†ï¼Œè¾“å‡ºå¿…é¡»åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œç”¨ `---METADATA---` åˆ†éš”ã€‚

**ç¬¬ä¸€éƒ¨åˆ†ï¼šå›ç­”æ­£æ–‡**
- ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„å›ç­”å†…å®¹ã€‚
- **è¡¨æ ¼æ”¯æŒï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**ï¼šå‡¡æ˜¯æ¶‰åŠå¯¹æ¯”ï¼ˆVSï¼‰ã€å‚æ•°åˆ—è¡¨ã€æ­¥éª¤è¯´æ˜æˆ–æ•°æ®å±•ç¤ºçš„å†…å®¹ï¼Œ**å¿…é¡»**ä½¿ç”¨ Markdown è¡¨æ ¼å‘ˆç°ã€‚
- **å›¾è¡¨æ”¯æŒï¼ˆå¼ºçƒˆæ¨èï¼‰**ï¼šå‡¡æ˜¯æ¶‰åŠæµç¨‹ã€æ—¶åºã€ç±»å…³ç³»æˆ–æ€ç»´å¯¼å›¾ï¼Œè¯·ä½¿ç”¨ Mermaid ä»£ç å—ï¼ˆ```mermaid ... ```ï¼‰å±•ç¤ºã€‚
- **ä¸¥ç¦**å°†æ•´ä¸ªå›ç­”åŒ…è£¹åœ¨ä»£ç å—ä¸­ã€‚
- å›ç­”ç»“æŸåï¼Œ**å¦èµ·ä¸€æ®µ**ï¼Œç”¨åŠ ç²—å­—ä½“å†™å‡ºä½ çš„åç»­æé—®ï¼š**æ€è€ƒé¢˜ï¼š...**

**ç¬¬äºŒéƒ¨åˆ†ï¼šå…ƒæ•°æ®**
- æ­£æ–‡ç»“æŸåï¼Œ**å¦èµ·ä¸€è¡Œ**è¾“å‡ºåˆ†éš”ç¬¦ï¼š`---METADATA---`
- ç´§æ¥ç€è¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„ JSON å¯¹è±¡ï¼ˆä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼‰ï¼ŒåŒ…å«ï¼š
  - `node_id`: (string) ç­”æ¡ˆä¸»è¦å‚è€ƒçš„ç« èŠ‚IDã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å› nullã€‚
  - `quote`: (string) ç­”æ¡ˆå¼•ç”¨çš„åŸæ–‡ç‰‡æ®µï¼ˆå¿…é¡»æ˜¯åŸæ–‡ä¸­å­˜åœ¨çš„å¥å­ï¼‰ã€‚å¦‚æœæ²¡æœ‰å¼•ç”¨ï¼Œè¿”å› nullã€‚
  - `anno_summary`: (string) æ ¸å¿ƒçŸ¥è¯†ç‚¹æ¦‚æ‹¬ï¼Œä½¿ç”¨ Markdown æ— åºåˆ—è¡¨æ ¼å¼ï¼ˆ3-5ç‚¹ï¼‰ã€‚

**ç¤ºä¾‹**ï¼š
ä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ
é€’å½’æ˜¯æŒ‡å‡½æ•°è°ƒç”¨è‡ªèº«çš„ç¼–ç¨‹æŠ€å·§...ï¼ˆè§£é‡Šå†…å®¹ï¼‰

**æ€è€ƒé¢˜ï¼šä½ èƒ½æƒ³åˆ°ç”Ÿæ´»ä¸­æœ‰ä»€ä¹ˆç°è±¡æ˜¯ç±»ä¼¼äºé€’å½’çš„å—ï¼Ÿ**

---METADATA---
{{"node_id": "uuid-123", "quote": "é€’å½’æ˜¯...", "anno_summary": "é€’å½’çš„æ¦‚å¿µ"}}
"""

        # Build prompt
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-5:]])
        
        prompt = f"""
è¯¾ç¨‹å†…å®¹ç‰‡æ®µï¼ˆæ­£æ–‡çŸ¥è¯†ï¼‰ï¼š
{context}

ç”¨æˆ·ç¬”è®°ï¼ˆå­¦ä¹ è¶³è¿¹ï¼‰ï¼š
{user_notes if user_notes else "æ— "}

å¯¹è¯å†å²ï¼š
{history_text}

é€‰ä¸­å†…å®¹ï¼ˆç”¨æˆ·é’ˆå¯¹è¿™æ®µæ–‡å­—æé—®ï¼‰ï¼š
{selection if selection else "æ— "}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·å¼€å§‹å›ç­”ï¼ˆè®°å¾—åœ¨æœ€åé™„åŠ å…ƒæ•°æ®ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def summarize_note(self, content: str) -> str:
        """
        Generate a concise title/summary for a note content.
        """
        system_prompt = get_prompt("summarize_note").format()
        
        # If content contains Q&A structure, try to summarize the Question primarily
        prompt = f"ç¬”è®°å†…å®¹ï¼š\n{content[:2000]}\n\nè¯·ç”Ÿæˆæ ‡é¢˜ï¼š"
        
        # Use Fast Model
        response = await self._call_llm(prompt, system_prompt, use_fast_model=True)
        return response if response else (content[:20] + "...")

    async def summarize_chat(self, history: List[dict], course_context: str = "", user_persona: str = "") -> Dict:
        system_prompt = get_prompt("summarize_chat").format(
            user_persona=user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"
        )
        
        # Convert history to text
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        
        prompt = f"è¯¾ç¨‹èƒŒæ™¯ï¼š\n{course_context}\n\nå¯¹è¯å†å²ï¼š\n{history_text}\n\nè¯·ç”Ÿæˆè¯¦ç»†çš„å¤ç›˜æŠ¥å‘Šï¼Œç¡®ä¿å†…å®¹ä¸°å¯Œå……å®ï¼š"
        
        # Use standard model for better quality summary
        response = await self._call_llm(prompt, system_prompt, use_fast_model=False)
        if response:
            return self._extract_json(response) or {"title": "å¯¹è¯æ€»ç»“", "content": response}
        return {"title": "æ€»ç»“å¤±è´¥", "content": "æ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"}

    async def summarize_history(self, history: List[Dict]) -> str:
        """
        Summarizes conversation history using LLM.
        """
        system_prompt = get_prompt("summarize_history").format()
        history_text = "\n".join([f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" for msg in history])
        
        prompt = f"Please summarize the following conversation:\n\n{history_text}"
        
        # Use Fast Model for summarization
        response = await self._call_llm(prompt, system_prompt, use_fast_model=True)
        return response if response else "Previous conversation summary (auto-generated failed)."

    async def generate_knowledge_graph(self, course_name: str, course_context: str, nodes: List[Dict]) -> Dict:
        """
        Generate a knowledge graph structure based on course content.
        
        Args:
            course_name: Name of the course
            course_context: Full course outline/context
            nodes: List of course nodes with their content
            
        Returns:
            Dictionary containing nodes and edges for the knowledge graph
        """
        from prompts import get_prompt
        
        # Build course context summary
        nodes_summary = []
        for node in nodes[:50]:  # Increased limit to cover full course structure
            nodes_summary.append({
                "id": node.get("node_id", ""),
                "name": node.get("node_name", ""),
                "level": node.get("node_level", 1),
                "content": node.get("node_content", "")[:200]  # Increased content context
            })
        
        context_text = f"""
è¯¾ç¨‹åç§°ï¼š{course_name}

è¯¾ç¨‹å¤§çº²ï¼š
{course_context}

ç« èŠ‚åˆ—è¡¨ï¼š
{json.dumps(nodes_summary, ensure_ascii=False, indent=2)}
"""
        
        # Get the knowledge graph prompt template
        prompt_template = get_prompt("generate_knowledge_graph")
        system_prompt = prompt_template.format(
            course_name=course_name,
            course_context=context_text
        )
        
        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹è¯¾ç¨‹å†…å®¹ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼š

è¯¾ç¨‹åç§°ï¼š{course_name}

ä¸»è¦ç« èŠ‚ï¼š
{chr(10).join([f"- {n.get('node_name', '')}: {n.get('node_content', '')[:50]}..." for n in nodes_summary[:15]])}

è¯·ç”ŸæˆåŒ…å«èŠ‚ç‚¹å’Œå…³ç³»çš„çŸ¥è¯†å›¾è°±JSONã€‚"""
        
        response = await self._call_llm(user_prompt, system_prompt)
        
        if response:
            result = self._extract_json(response)
            if result and "nodes" in result and "edges" in result and len(result["nodes"]) > 0:
                # Self-Healing: Validate and fix chapter_ids
                valid_chapter_ids = {n.get("node_id") for n in nodes}
                
                for graph_node in result["nodes"]:
                    chapter_id = graph_node.get("chapter_id")
                    
                    # If invalid or missing
                    if not chapter_id or chapter_id not in valid_chapter_ids:
                        # Try to find a match by name similarity (simple substring check for now)
                        node_label = graph_node.get("label", "")
                        best_match_id = None
                        
                        # Priority 1: Exact match
                        for n in nodes:
                            if n.get("node_name", "") == node_label:
                                best_match_id = n.get("node_id")
                                break
                                
                        # Priority 2: Substring match
                        if not best_match_id:
                            for n in nodes:
                                if node_label in n.get("node_name", "") or n.get("node_name", "") in node_label:
                                    best_match_id = n.get("node_id")
                                    break
                        
                        # Fallback to the first available node if no match found
                        if not best_match_id and nodes:
                            best_match_id = nodes[0].get("node_id")
                            
                        if best_match_id:
                            graph_node["chapter_id"] = best_match_id
                            
                return result
        
        # Fallback: Generate a simple graph based on node hierarchy
        logger.warning("Knowledge graph generation failed, using fallback")
        return self._generate_fallback_knowledge_graph(nodes)
    
    def _generate_fallback_knowledge_graph(self, nodes: List[Dict]) -> Dict:
        """
        Generate a simple fallback knowledge graph based on node hierarchy.
        """
        graph_nodes = []
        graph_edges = []
        
        # Create nodes
        for node in nodes[:15]:
            node_id = node.get("node_id", str(uuid.uuid4()))
            node_level = node.get("node_level", 1)
            
            # Determine node type based on level
            if node_level == 1:
                node_type = "module"
            else:
                node_type = "concept"
            
            graph_nodes.append({
                "id": node_id,
                "label": node.get("node_name", "Unknown"),
                "type": node_type,
                "description": node.get("node_content", "")[:50],
                "chapter_id": node_id
            })
        
        # Add Root Node
        root_id = "root_" + str(uuid.uuid4())[:8]
        graph_nodes.insert(0, {
            "id": root_id,
            "label": "è¯¾ç¨‹æ ¸å¿ƒ",
            "type": "root",
            "description": "è¯¾ç¨‹æ ¹èŠ‚ç‚¹",
            "chapter_id": nodes[0].get("node_id") if nodes else ""
        })
        
        # Connect Root to Level 1 Modules
        for node in graph_nodes:
             if node["type"] == "module":
                graph_edges.append({
                    "source": root_id,
                    "target": node["id"],
                    "relation": "contains",
                    "label": "åŒ…å«"
                })

        # Create edges based on parent-child relationships
        node_map = {n["id"]: n for n in graph_nodes}
        for node in nodes[:15]:
            node_id = node.get("node_id", "")
            parent_id = node.get("parent_node_id", "")
            
            if parent_id and parent_id in node_map and node_id in node_map:
                graph_edges.append({
                    "source": parent_id,
                    "target": node_id,
                    "relation": "contains",
                    "label": "åŒ…å«"
                })
        
        # Add some cross-references between same-level nodes
        level_groups = {}
        for node in graph_nodes:
            level = node.get("type", "basic")
            if level not in level_groups:
                level_groups[level] = []
            level_groups[level].append(node)
        
        # Connect nodes within same level
        for level, group in level_groups.items():
            for i in range(len(group) - 1):
                if len(graph_edges) < 30:  # Limit total edges
                    graph_edges.append({
                        "source": group[i]["id"],
                        "target": group[i + 1]["id"],
                        "relation": "related",
                        "label": "å…³è”"
                    })
        
        return {
            "nodes": graph_nodes,
            "edges": graph_edges
        }

    def locate_node(self, keyword: str, all_nodes: List[Dict]) -> Dict:
        # Simple mock search - Semantic search requires embedding, sticking to keyword match for now
        # Or could use LLM to pick from list if list is small, but for MVP keyword is safer/faster
        for node in all_nodes:
            if keyword in node['node_name']:
                return {
                    "match_node_id": node['node_id'],
                    "match_node_name": node['node_name'],
                    "node_path": "Path/To/Node" # Mock path
                }
        return {}


ai_service = AIService()
