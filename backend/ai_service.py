import uuid
import random
import os
import json
import re
import logging
from dotenv import load_dotenv
from openai import AsyncOpenAI, APIStatusError
from typing import List, Dict, Optional

# å¯¼å…¥æç¤ºæ¨¡æ¿
from prompts import (
    get_prompt,
    TUTOR_SYSTEM_BASE,
    TUTOR_METADATA_RULE,
)

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å…·æœ‰åˆ‡æ¢åˆ°çœŸå® API åŠŸèƒ½çš„æ¨¡æ‹Ÿ AI æœåŠ¡
class AIService:
    """
    AI æ¨¡å‹äº¤äº’çš„æŠ½è±¡å±‚ã€‚
    æ”¯æŒæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´åˆ‡æ¢ã€‚
    æ”¯æŒå¤š Token è‡ªåŠ¨æ•…éšœè½¬ç§»ï¼ˆAuto-Failoverï¼‰ã€‚
    """
    def __init__(self):
        # 1. ä¼˜å…ˆåŠ è½½ Token åˆ—è¡¨ï¼ˆæ”¯æŒå¤š Token è½®è¯¢ï¼‰
        keys_str = os.getenv("AI_API_KEYS", "")
        if keys_str:
            self.api_keys = [k.strip() for k in keys_str.split(",") if k.strip()]
        else:
            # 2. å›é€€åˆ°å• Token æ¨¡å¼
            single_key = os.getenv("AI_API_KEY")
            self.api_keys = [single_key] if single_key else []

        self.current_key_index = 0
        self.api_base = os.getenv("AI_API_BASE", "https://api-inference.modelscope.cn/v1")
        
        # æ··åˆæ¨¡å‹ç­–ç•¥
        self.model_smart = os.getenv("AI_MODEL", "Qwen/Qwen3-32B")
        self.model_fast = os.getenv("AI_MODEL_FAST", "Qwen/Qwen3-32B")
        
        self.client = None
        self._refresh_client()

    def _refresh_client(self):
        """æ ¹æ®å½“å‰ç´¢å¼•åˆ·æ–° OpenAI å®¢æˆ·ç«¯"""
        if not self.api_keys:
            self.client = None
            logger.warning("No API Keys configured.")
            return

        current_key = self.api_keys[self.current_key_index]
        # logger.info(f"Using API Key index: {self.current_key_index} (Ends with {current_key[-4:]})")
        
        self.client = AsyncOpenAI(
            base_url=self.api_base,
            api_key=current_key,
        )

    def _rotate_key(self):
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª API Key"""
        if len(self.api_keys) <= 1:
            return False # åªæœ‰ä¸€ä¸ª keyï¼Œæ— æ³•åˆ‡æ¢

        old_index = self.current_key_index
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        logger.warning(f"âš ï¸ Switching API Key: {old_index} -> {self.current_key_index}")
        self._refresh_client()
        return True

    def _extract_json(self, text: str) -> Optional[Dict]:
        """
        ä» LLM å“åº”ä¸­ç¨³å¥åœ°æå– JSONã€‚
        å¤„ç† Markdown å—ã€çº¯æ–‡æœ¬å’Œæ½œåœ¨çš„å¹²æ‰°ä¿¡æ¯ã€‚
        """
        # logger.info(f"Raw AI Response for JSON extraction: {text[:200]}...")

        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # å°è¯•åœ¨ markdown ä¸­æŸ¥æ‰¾ JSON å—
        # å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼ä»¥æ•è· ```json å’Œ ``` ä¹‹é—´çš„å†…å®¹
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError as e:
                logger.warning(f"Markdown JSON decode error: {e}")
                pass

        # å°è¯•æŸ¥æ‰¾ä»»ä½•ä»£ç å—
        code_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if code_match:
            try:
                return json.loads(code_match.group(1))
            except json.JSONDecodeError:
                pass

        # å°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}'
        try:
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx+1]
                return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"Substring JSON decode error: {e}")
            pass

        logger.warning(f"Failed to extract JSON from: {text[:500]}...")
        
        # è°ƒè¯•ï¼šå°†å¤±è´¥çš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
        try:
            with open("debug_failed_json.txt", "w", encoding="utf-8") as f:
                f.write(text)
        except Exception:
            pass
            
        return None

    def _clean_mermaid_syntax(self, text: str) -> str:
        """
        Fix common Mermaid syntax errors in the text.
        """
        # Regex to find mermaid blocks
        pattern = r'```mermaid(.*?)```'
        
        def fix_mermaid_block(match):
            content = match.group(1)
            
            def quote_if_needed(text, type_char):
                # Check if already quoted (simple check)
                if text.startswith('"') and text.endswith('"'):
                    return text
                
                # Escape quotes inside the text
                text = text.replace('"', '\\"')
                return f'"{text}"'

            # Fix 1: [Text] -> ["Text"] (Rectangular nodes)
            # Exclude content starting with (, [, /, \, < to avoid breaking shapes
            content = re.sub(r'\[(?![(\[/\\<])([^\[\]\n]+?)\]', 
                             lambda m: f'[{quote_if_needed(m.group(1), "[")}]', 
                             content)
            
            # Fix 2: (Text) -> ("Text") (Round nodes)
            # Exclude content starting with ( to avoid breaking shapes
            content = re.sub(r'\((?!\()([^()\n]+?)\)', 
                             lambda m: f'({quote_if_needed(m.group(1), "(")})', 
                             content)
            
            # Fix 3: {Text} -> {"Text"} (Rhombus nodes)
            # Exclude content starting with {{Hexagon}}
            content = re.sub(r'\{(?![{!])([^{}\n]+?)\}', 
                             lambda m: f'{{{quote_if_needed(m.group(1), "{")}}}', 
                             content)
            
            return f'```mermaid{content}```'

        return re.sub(pattern, fix_mermaid_block, text, flags=re.DOTALL)

    def clean_response_text(self, text: str) -> str:
        """
        Cleans LLM response: strips markdown wrapper and fixes LaTeX and Mermaid.
        """
        clean_text = text.strip()
        # Strip ```markdown wrapper
        if clean_text.startswith("```markdown") and clean_text.endswith("```"):
            clean_text = clean_text[11:-3].strip()
            
        # Fix LaTeX
        pattern = r'(?<!\$)(?<!\$\$)\\begin\{(matrix|pmatrix|bmatrix|vmatrix|Vmatrix|array|align|equation|cases)\}.*?\\end\{\1\}(?!\$)(?!\$\$)'
        clean_text = re.sub(pattern, lambda m: f"\n$$\n{m.group(0)}\n$$\n", clean_text, flags=re.DOTALL)
        
        # Fix Mermaid
        clean_text = self._clean_mermaid_syntax(clean_text)
        
        return clean_text

    async def _call_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.", use_fast_model: bool = False) -> str:
        """
        Generic function to call LLM using OpenAI client.
        Supports Auto-Failover for Rate Limits (429) or Auth Errors (401/403).
        """
        if not self.client:
            return None 
        
        max_retries = len(self.api_keys)
        # å¦‚æœåªæœ‰ä¸€ä¸ª keyï¼Œé‡è¯•ä¸€æ¬¡å³å¯ï¼ˆæˆ–è€…ä¸é‡è¯•ï¼Œç›´æ¥æŠ¥é”™ï¼‰
        # è¿™é‡Œè®¾ç½®ä¸º max(1, len) ç¡®ä¿è‡³å°‘å°è¯•ä¸€æ¬¡
        attempts = 0
        
        while attempts < max_retries:
            attempts += 1
            try:
                extra_body = {
                    "enable_thinking": False
                }
                
                # Select Model
                model_id = self.model_fast if use_fast_model else self.model_smart
                
                response = await self.client.chat.completions.create(
                    model=model_id,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    stream=True,
                    extra_body=extra_body
                )
                
                full_content = ""
                async for chunk in response:
                    if chunk.choices:
                        # Handle reasoning content if available (for logging/debugging)
                        if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                            reasoning = chunk.choices[0].delta.reasoning_content
                            if reasoning:
                                # Log thinking process to console
                                print(reasoning, end='', flush=True)
                                
                        delta = chunk.choices[0].delta
                        if delta.content:
                            full_content += delta.content
                
                logger.info(f"AI Response Complete (Model: {model_id})")
                return full_content

            except APIStatusError as e:
                # åªåœ¨é‡åˆ°é™æµ(429)æˆ–æƒé™(401/403)é”™è¯¯æ—¶åˆ‡æ¢ Token
                if e.status_code in [429, 401, 403]:
                    logger.error(f"âš ï¸ API Error ({e.status_code}): {e.message}. Trying next token...")
                    if self._rotate_key():
                        continue # Retry with new key
                    else:
                        logger.error("âŒ All tokens exhausted or only one token available.")
                        raise e # No more tokens to try
                else:
                    # å…¶ä»–é”™è¯¯ï¼ˆå¦‚ 500, 400ï¼‰ç›´æ¥æŠ›å‡ºï¼Œä¸æµªè´¹ Token
                    logger.error(f"AI API Call Error (Non-retryable): {e}")
                    raise e
            except Exception as e:
                logger.error(f"AI API Unexpected Error: {e}")
                return None
        
        return None

    async def generate_course(self, keyword: str, difficulty: str = "medium", style: str = "academic", requirements: str = "") -> Dict:
        system_prompt = get_prompt("generate_course").format(
            difficulty=difficulty,
            style=style,
            requirements=requirements if requirements else "æ— "
        )
        prompt = f"ç”¨æˆ·æƒ³è¦å­¦ä¹ â€œ{keyword}â€ï¼Œè¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”ç³»ç»Ÿçš„è¯¾ç¨‹å¤§çº²ã€‚"
        
        try:
            response = await self._call_llm(prompt, system_prompt)
            if response:
                data = self._extract_json(response)
                if data and "nodes" in data:
                    # Ensure unique UUIDs for nodes to prevent collision between courses
                    for node in data["nodes"]:
                        node["node_id"] = str(uuid.uuid4())
                return data
        except Exception:
            pass
            
        return {"course_name": keyword, "nodes": []}

    async def generate_quiz(self, content: str, node_name: str = "", difficulty: str = "medium", style: str = "standard", user_persona: str = "", question_count: int = 3) -> List[Dict]:
        system_prompt = get_prompt("generate_quiz").format(
            difficulty=difficulty,
            style=style,
            question_count=question_count
        )
        
        content_text = content
        if not content or len(content) < 50:
            content_text = f"Topic: {node_name}\n(The detailed content is missing, please generate general questions based on this topic)"
        
        # Explicitly mention question count in the user prompt as well to reinforce it
        prompt = f"Content:\n{content_text}\n\nPlease generate exactly {question_count} questions in JSON format. Remember to use Markdown tables or Mermaid diagrams in 'explanation' if helpful for understanding."
        
        try:
            response = await self._call_llm(prompt, system_prompt)
            if response:
                result = self._extract_json(response)
                if result:
                    return result
        except Exception:
            pass

        
        # Hard Fallback: If AI fails or returns empty, generate template questions
        # This ensures the user NEVER sees "Cannot generate" error.
        logger.warning(f"Quiz generation failed for {node_name}. Using hard fallback.")
        fallback_topic = node_name if node_name else "æ­¤ä¸»é¢˜"
        fallback_questions = [
            {
                "id": 1,
                "question": f"å…³äºâ€œ{fallback_topic}â€çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä»¥ä¸‹æè¿°æ­£ç¡®çš„æ˜¯ï¼Ÿ",
                "options": [
                    f"{fallback_topic} æ˜¯ä¸€ä¸ªå­¤ç«‹çš„æ¦‚å¿µï¼Œä¸å…¶ä»–çŸ¥è¯†æ— å…³",
                    f"{fallback_topic} æ˜¯è¯¥å­¦ç§‘ä½“ç³»ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†",
                    f"{fallback_topic} å·²ç»è¢«ç°ä»£ç†è®ºå®Œå…¨æ¨ç¿»",
                    f"{fallback_topic} ä»…åœ¨ç‰¹å®šæç«¯æƒ…å†µä¸‹é€‚ç”¨"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic} ä½œä¸ºæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œåœ¨å­¦ç§‘ä½“ç³»ä¸­èµ·ç€æ‰¿ä¸Šå¯ä¸‹çš„ä½œç”¨ï¼Œæ˜¯ç†è§£åç»­å†…å®¹çš„åŸºç¡€ã€‚"
            },
            {
                "id": 2,
                "question": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç†è§£â€œ{fallback_topic}â€ä¸»è¦æœ‰åŠ©äºè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "options": [
                    "å†å²èƒŒæ™¯çš„è€ƒè¯",
                    "å¤æ‚ç³»ç»Ÿä¸­çš„å…³é”®æœºåˆ¶åˆ†æ",
                    "æ— å…³æ•°æ®çš„éšæœºå¤„ç†",
                    "çº¯ç²¹çš„ç†è®ºæ¨å¯¼æ¸¸æˆ"
                ],
                "correct_index": 1,
                "explanation": f"æŒæ¡{fallback_topic}çš„åŸç†ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬åˆ†æå’Œå¤„ç†å®é™…ç³»ç»Ÿä¸­çš„å¤æ‚æœºåˆ¶ä¸å…³é”®é—®é¢˜ã€‚"
            },
            {
                "id": 3,
                "question": f"å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œå­¦ä¹ â€œ{fallback_topic}â€æœ€å¤§çš„æŒ‘æˆ˜é€šå¸¸æ˜¯ï¼Ÿ",
                "options": [
                    "æ¦‚å¿µè¿‡äºç®€å•ï¼Œç¼ºä¹æŒ‘æˆ˜",
                    "ç†è§£å…¶æŠ½è±¡é€»è¾‘ä¸å®é™…åœºæ™¯çš„æ˜ å°„",
                    "ç›¸å…³èµ„æ–™å¤ªå°‘ï¼Œæ— æ³•æŸ¥é˜…",
                    "æ²¡æœ‰ä»»ä½•æŒ‘æˆ˜ï¼Œä¸€å­¦å°±ä¼š"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic}å¾€å¾€åŒ…å«ä¸€å®šçš„æŠ½è±¡é€»è¾‘ï¼Œå°†å…¶å‡†ç¡®æ˜ å°„åˆ°å®é™…åº”ç”¨åœºæ™¯ä¸­æ˜¯åˆå­¦è€…å¸¸è§çš„éš¾ç‚¹ã€‚"
            },
            {
                "id": 4,
                "question": f"ä»¥ä¸‹å“ªé¡¹ä¸æ˜¯â€œ{fallback_topic}â€çš„å…¸å‹ç‰¹å¾ï¼Ÿ",
                "options": [
                    "ç³»ç»Ÿæ€§",
                    "é€»è¾‘æ€§",
                    "éšæ„æ€§",
                    "å®ç”¨æ€§"
                ],
                "correct_index": 2,
                "explanation": f"{fallback_topic}ä½œä¸ºç§‘å­¦æˆ–ä¸“ä¸šçŸ¥è¯†ï¼Œå…·æœ‰ä¸¥å¯†çš„é€»è¾‘å’Œç³»ç»Ÿæ€§ï¼Œç»ééšæ„æ„å»ºã€‚"
            },
            {
                "id": 5,
                "question": f"æ·±å…¥æŒæ¡â€œ{fallback_topic}â€åï¼Œä¸‹ä¸€æ­¥é€šå¸¸åº”è¯¥å­¦ä¹ ï¼Ÿ",
                "options": [
                    "æ”¾å¼ƒè¯¥å­¦ç§‘",
                    "è¯¥é¢†åŸŸçš„è¿›é˜¶ç†è®ºæˆ–ç›¸å…³äº¤å‰å­¦ç§‘",
                    "å®Œå…¨ä¸ç›¸å…³çš„é¢†åŸŸ",
                    "é‡å¤å­¦ä¹ åŸºç¡€æ¦‚å¿µ"
                ],
                "correct_index": 1,
                "explanation": f"åœ¨æŒæ¡åŸºç¡€åï¼Œè¿›é˜¶ç†è®ºæˆ–äº¤å‰å­¦ç§‘çš„åº”ç”¨æ˜¯æ·±å…¥ç ”ç©¶çš„å¿…ç»ä¹‹è·¯ã€‚"
            }
        ]
        
        return fallback_questions[:question_count]

    async def generate_sub_nodes(self, node_name: str, node_level: int, node_id: str, course_name: str = "", parent_context: str = "", course_outline: str = "") -> List[Dict]:
        system_prompt = get_prompt("generate_sub_nodes").format(
            course_name=course_name if course_name else "æœªçŸ¥è¯¾ç¨‹",
            parent_context=parent_context if parent_context else "æ— ",
            course_outline=course_outline if course_outline else "æ— "
        )
        prompt = f"å½“å‰èŠ‚ç‚¹ä¿¡æ¯ï¼šåç§°={node_name}ï¼Œå±‚çº§={node_level}ã€‚è¯·åˆ—å‡ºè¯¥ç« èŠ‚ä¸‹çš„æ‰€æœ‰å­å°èŠ‚ï¼Œç¡®ä¿ç»“æ„å®Œæ•´ä¸”å…·å¤‡ä¸“ä¸šæ€§ã€‚"
        
        try:
            response = await self._call_llm(prompt, system_prompt)
            new_level = node_level + 1
            
            if response:
                data = self._extract_json(response)
                if data:
                    result = []
                    for item in data.get("sub_nodes", []):
                        result.append({
                            "node_id": str(uuid.uuid4()),
                            "parent_node_id": node_id,
                            "node_name": item.get("node_name", "æ–°èŠ‚ç‚¹"),
                            "node_level": new_level,
                            "node_content": item.get("node_content", ""),
                            "node_type": "custom"
                        })
                    return result
        except Exception:
            pass

        new_level = node_level + 1
        return [
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 1", "node_level": new_level, "node_content": "", "node_type": "custom"},
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 2", "node_level": new_level, "node_content": "", "node_type": "custom"}
        ]

    async def _stream_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.", use_fast_model: bool = False):
        """
        Generator function to stream LLM response chunks.
        Supports Auto-Failover.
        """
        if not self.client:
            yield "AI Service not configured."
            return

        max_retries = len(self.api_keys)
        attempts = 0

        while attempts < max_retries:
            attempts += 1
            try:
                extra_body = {
                    "enable_thinking": False
                }
                
                # Select Model
                model_id = self.model_fast if use_fast_model else self.model_smart

                response = await self.client.chat.completions.create(
                    model=model_id,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    stream=True,
                    extra_body=extra_body
                )
                
                async for chunk in response:
                    if chunk.choices:
                        # Handle reasoning content if available (for logging/debugging)
                        if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                            reasoning = chunk.choices[0].delta.reasoning_content
                            if reasoning:
                                 # We can log thinking process or just ignore it for now
                                 pass
                        
                        delta = chunk.choices[0].delta
                        if delta.content:
                            yield delta.content
                
                # Success! Break loop.
                return 

            except APIStatusError as e:
                # åªåœ¨é‡åˆ°é™æµ(429)æˆ–æƒé™(401/403)é”™è¯¯æ—¶åˆ‡æ¢ Token
                if e.status_code in [429, 401, 403]:
                    logger.error(f"âš ï¸ Stream API Error ({e.status_code}): {e.message}. Trying next token...")
                    if self._rotate_key():
                        continue # Retry with new key
                    else:
                        yield f"\n[Error: Token Exhausted - {str(e)}]"
                        return
                else:
                    logger.error(f"Stream API Error (Non-retryable): {e}")
                    yield f"\n[Error: {str(e)}]"
                    return
            except Exception as e:
                logger.error(f"Stream Error: {e}")
                yield f"\n[Error: {str(e)}]"
                return

    async def redefine_node_content(self, node_name: str, original_content: str, requirement: str, course_context: str = "", previous_context: str = ""):
        """
        Stream version of redefine_content with book-level context awareness.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½èµ„æ·±å­¦ç§‘ä¸“å®¶ã€ä¸–ç•Œé¡¶å°–å¤§å­¦çš„ç»ˆèº«æ•™æˆï¼Œå¹¶æ‹¥æœ‰ä¸€çº¿å¤§å‚çš„é¦–å¸­æ¶æ„å¸ˆèƒŒæ™¯ã€‚

## å­¦æœ¯å®šä½
- **å—ä¼—**ï¼šå¤§å­¦æœ¬ç§‘ç”Ÿã€ç ”ç©¶ç”ŸåŠä¸“ä¸šæŠ€æœ¯äººå‘˜
- **ç›®æ ‡**ï¼šæ„å»ºç³»ç»ŸåŒ–ã€ç†è®ºè”ç³»å®é™…çš„çŸ¥è¯†ä½“ç³»ï¼Œä¸ä»…è®²â€œæ˜¯ä»€ä¹ˆâ€ï¼Œæ›´è®²â€œä¸ºä»€ä¹ˆâ€å’Œâ€œæ€ä¹ˆåšâ€
- **æ ‡å‡†**ï¼šç¬¦åˆå­¦æœ¯è§„èŒƒå’Œè¡Œä¸šæ ‡å‡†
- **é£æ ¼**ï¼šä¸“ä¸šä¸¥è°¨ï¼Œæ·±å…¥æµ…å‡ºï¼Œæ‹’ç»ç§‘æ™®æ€§è´¨çš„æµ…å±‚ä»‹ç»

## å†…å®¹æ¶æ„è¦æ±‚
### æ ¸å¿ƒè¾“å‡ºç»“æ„ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œå¹¶ç”¨ `<!-- BODY_START -->` åˆ†éš”ï¼š
1. **ç¬¬ä¸€éƒ¨åˆ†ï¼šå­¦æœ¯æ€§å¯¼è¨€ï¼ˆAnnotationï¼‰**
   - ç®€çŸ­çš„å¯¼è¯»æˆ–æ‰¹æ³¨ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚
   - é˜è¿°æœ¬ç« åœ¨å­¦ç§‘ä½“ç³»ä¸­çš„åœ°ä½å’Œä»·å€¼ï¼Œæ¦‚è¿°æ ¸å¿ƒé—®é¢˜å’Œç ”ç©¶æ„ä¹‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹å‰ã€‚
2. **åˆ†éš”ç¬¦**
   - å¿…é¡»ä¸¥æ ¼è¾“å‡º `<!-- BODY_START -->` å­—ç¬¦ä¸²ã€‚
3. **ç¬¬äºŒéƒ¨åˆ†ï¼šä¸“ä¸šæ­£æ–‡å†…å®¹ï¼ˆMain Bodyï¼‰**
   - è¯¦ç»†çš„æ•™ç§‘ä¹¦å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹åã€‚

### å†…å®¹è´¨é‡æ ‡å‡†
1. **å­¦æœ¯æ·±åº¦**
   - æ·±å…¥å‰–ææ¦‚å¿µçš„ç†è®ºåŸºç¡€å’Œå†å²æ¸Šæº
   - åˆ†ææŠ€æœ¯åŸç†çš„æ•°å­¦æˆ–é€»è¾‘åŸºç¡€
   - æ¢è®¨æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œå±€é™æ€§

2. **ä¸“ä¸šè¡¨è¾¾**
   - ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼
   - é¿å…ç”Ÿæ´»åŒ–æ¯”å–»ï¼Œé‡‡ç”¨ä¸“ä¸šç±»æ¯”
   - å¼•ç”¨æƒå¨ç ”ç©¶å’Œå®è¯æ•°æ®

3. **ç»“æ„ä¸¥è°¨æ€§**
   - æ­£æ–‡ç»“æ„åº”åŒ…å«ï¼š
     - **### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µä¸èƒŒæ™¯**ï¼šæ¸…æ™°å®šä¹‰ + äº§ç”ŸèƒŒæ™¯/æ ¸å¿ƒä»·å€¼ï¼ˆå…³é”®åè¯ä½¿ç”¨ **åŠ ç²—** å¼ºè°ƒï¼‰
     - **### ğŸ” æ·±åº¦åŸç†/åº•å±‚æœºåˆ¶**ï¼šæ·±å…¥å‰–æå·¥ä½œåŸç†ã€åº•å±‚é€»è¾‘ã€æ•°å­¦æ¨¡å‹æˆ–æ¼”åŒ–é€»è¾‘ï¼ˆé‡ä¸­ä¹‹é‡ï¼‰
     - **### ğŸ› ï¸ æŠ€æœ¯å®ç°/æ–¹æ³•è®º**ï¼šå…·ä½“çš„æ¨å¯¼è¿‡ç¨‹ã€ç®—æ³•æ­¥éª¤æˆ–æ‰§è¡Œç»†èŠ‚
     - **### ğŸ¨ å¯è§†åŒ–å›¾è§£**ï¼š**å¿…é¡»**åŒ…å«è‡³å°‘ä¸€ä¸ª Mermaid å›¾è¡¨ï¼ˆæµç¨‹å›¾æˆ–æ—¶åºå›¾ï¼‰ã€‚IDçº¯è‹±æ–‡æ— ç©ºæ ¼ï¼Œæ–‡æœ¬åŒå¼•å·åŒ…è£¹ã€‚
     - **### ğŸ­ å®æˆ˜æ¡ˆä¾‹/è¡Œä¸šåº”ç”¨**ï¼šç»“åˆçœŸå®äº§ä¸šç•Œçš„è½åœ°æ¡ˆä¾‹è¿›è¡Œåˆ†æ
     - **### âœ… æ€è€ƒä¸æŒ‘æˆ˜**ï¼šæä¾› 1-2 ä¸ªèƒ½å¼•å‘æ·±åº¦æ€è€ƒçš„é—®é¢˜

### æŠ€æœ¯è§„èŒƒ
- **å›¾è¡¨ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**ï¼šæ¯ç« **å¿…é¡»**åŒ…å«è‡³å°‘ä¸€å¼  Mermaid å›¾è¡¨ï¼ˆå¦‚æµç¨‹å›¾ã€æ—¶åºå›¾ã€ç±»å›¾æˆ–æ€ç»´å¯¼å›¾ï¼‰ï¼Œç”¨äºç›´è§‚è§£é‡Šæ ¸å¿ƒæ¦‚å¿µæˆ–æµç¨‹ã€‚
  - èŠ‚ç‚¹ ID å¿…é¡»çº¯è‹±æ–‡ï¼Œä¸¥ç¦ä¸­æ–‡æˆ–ç‰¹æ®Šç¬¦å·ã€‚
  - èŠ‚ç‚¹æ–‡æœ¬å¿…é¡»åŒå¼•å·åŒ…è£¹ã€‚
- **å…¬å¼è§„èŒƒï¼ˆç»å¯¹ä¸¥æ ¼æ‰§è¡Œï¼‰**
  - è¡Œå†…å…¬å¼ï¼šå¿…é¡»ä½¿ç”¨ `$å…¬å¼$` æ ¼å¼ï¼Œå†…éƒ¨ä¸è¦æœ‰ç©ºæ ¼ï¼ˆä¾‹å¦‚ `$E=mc^2$`ï¼‰
  - å—çº§å…¬å¼ï¼šå¿…é¡»ä½¿ç”¨ `$$` åŒ…è£¹ï¼Œä¸”ç‹¬å ä¸€è¡Œ
  - ä¸¥ç¦è£¸å†™ LaTeX å‘½ä»¤
- **å‚è€ƒæ–‡çŒ®**ï¼šç¬¦åˆå­¦æœ¯å¼•ç”¨è§„èŒƒ

### ç¯‡å¹…ä¸è¾“å‡º
- **å­—æ•°**ï¼š800-1500 å­—ï¼Œç¡®ä¿è§£é‡Šé€å½»ã€‚
- **è¾“å‡º**ï¼šç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼ŒåŒ…å«åˆ†éš”ç¬¦ã€‚
"""
        # å¦‚æœå¯èƒ½ï¼Œä»éœ€æ±‚å­—ç¬¦ä¸²ä¸­æ³¨å…¥æ ·å¼å’Œéš¾åº¦ä¸Šä¸‹æ–‡
        # ç”±äº 'requirement' åªæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬ç›´æ¥é™„åŠ å®ƒã€‚
        
        prompt = f"""
å…¨ä¹¦å¤§çº²ï¼š
{course_context}

ä¸Šæ–‡æ‘˜è¦ï¼ˆç”¨äºæ‰¿æ¥ï¼‰ï¼š
{previous_context}

å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}
åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š{original_content}
ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}

è¯·å¼€å§‹æ’°å†™ï¼ˆè®°å¾—åŒ…å« <!-- BODY_START --> åˆ†éš”ç¬¦ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def chat_with_tutor(self, message: str, history: List[Dict], context: str = "", user_notes: str = "", selection: str = "", user_persona: str = "") -> str:
        """
        Chat with AI tutor.
        """
        system_prompt = TUTOR_SYSTEM_BASE.format(
            user_persona=user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"
        )
        
        # Build prompt
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-5:]])
        
        prompt = f"""
è¯¾ç¨‹å†…å®¹ç‰‡æ®µï¼ˆæ­£æ–‡çŸ¥è¯†ï¼‰ï¼š
{context}

ç”¨æˆ·ç¬”è®°ï¼ˆå­¦ä¹ è¶³è¿¹ï¼‰ï¼š
{user_notes if user_notes else "æ— "}

å¯¹è¯å†å²ï¼š
{history_text}

é€‰ä¸­å†…å®¹ï¼ˆç”¨æˆ·é’ˆå¯¹è¿™æ®µæ–‡å­—æé—®ï¼‰ï¼š
{selection if selection else "æ— "}

ç”¨æˆ·é—®é¢˜ï¼š{message}

è¯·å¼€å§‹å›ç­”ï¼ˆè®°å¾—åœ¨æœ€åé™„åŠ å…ƒæ•°æ®ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def generate_knowledge_graph(self, course_name: str, course_context: str, nodes: List[Dict]) -> Dict:
        """
        Generate a knowledge graph for the course.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„è¯¾ç¨‹å†…å®¹ï¼Œæ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ã€‚
è¾“å‡ºå¿…é¡»æ˜¯åˆæ³•çš„ JSON æ ¼å¼ï¼ŒåŒ…å« 'nodes' å’Œ 'edges' ä¸¤ä¸ªæ•°ç»„ã€‚

Nodes æ ¼å¼: { "id": "uuid", "label": "æ¦‚å¿µåç§°", "category": "æ¦‚å¿µç±»å‹", "chapter_id": "å¯¹åº”ç« èŠ‚ID" }
Edges æ ¼å¼: { "source": "source_id", "target": "target_id", "relation": "å…³ç³»æè¿°" }

é‡è¦ï¼š
1. å°½é‡å¤ç”¨å·²æœ‰çš„ç« èŠ‚ä½œä¸ºæ ¸å¿ƒèŠ‚ç‚¹ã€‚
2. è‡ªåŠ¨æå–ç« èŠ‚å†…å®¹ä¸­çš„å…³é”®æ¦‚å¿µä½œä¸ºå­èŠ‚ç‚¹ã€‚
3. ç¡®ä¿ JSON æ ¼å¼æ­£ç¡®ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚
"""
        
        # Simplify nodes for context to save tokens
        nodes_summary = []
        for n in nodes:
            nodes_summary.append({
                "node_id": n.get("node_id"),
                "node_name": n.get("node_name"),
                "node_content": n.get("node_content", "")[:100]
            })
            
        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹è¯¾ç¨‹å†…å®¹ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼š

è¯¾ç¨‹åç§°ï¼š{course_name}

ä¸»è¦ç« èŠ‚ï¼š
{chr(10).join([f"- {n.get('node_name', '')}: {n.get('node_content', '')[:50]}..." for n in nodes_summary[:15]])}

è¯·ç”ŸæˆåŒ…å«èŠ‚ç‚¹å’Œå…³ç³»çš„çŸ¥è¯†å›¾è°±JSONã€‚"""
        
        try:
            response = await self._call_llm(user_prompt, system_prompt)
            
            if response:
                result = self._extract_json(response)
                if result and "nodes" in result and "edges" in result and len(result["nodes"]) > 0:
                    # Self-Healing: Validate and fix chapter_ids
                    valid_chapter_ids = {n.get("node_id") for n in nodes}
                    
                    for graph_node in result["nodes"]:
                        chapter_id = graph_node.get("chapter_id")
                        
                        # If invalid or missing
                        if not chapter_id or chapter_id not in valid_chapter_ids:
                            # Try to find a match by name similarity (simple substring check for now)
                            node_label = graph_node.get("label", "")
                            best_match_id = None
                            
                            # Priority 1: Exact match
                            for n in nodes:
                                if n.get("node_name", "") == node_label:
                                    best_match_id = n.get("node_id")
                                    break
                                    
                            # Priority 2: Substring match
                            if not best_match_id:
                                for n in nodes:
                                    if node_label in n.get("node_name", "") or n.get("node_name", "") in node_label:
                                        best_match_id = n.get("node_id")
                                        break
                            
                            # If match found, update chapter_id
                            if best_match_id:
                                graph_node["chapter_id"] = best_match_id
                            # If still no match, maybe it's a sub-concept, link to nearest parent? 
                            # For now, leave as is or assign to root? 
                            # Let's leave it, frontend handles missing links gracefully.

                    return result
        except Exception:
            pass
            
        return {"nodes": [], "edges": []}

    def locate_node(self, keyword: str, all_nodes: List[Dict]) -> Dict:
        # Simple mock search - Semantic search requires embedding, sticking to keyword match for now
        # Or could use LLM to pick from list if list is small, but for MVP keyword is safer/faster
        for node in all_nodes:
            if keyword in node['node_name']:
                return {
                    "match_node_id": node['node_id'],
                    "match_node_name": node['node_name'],
                    "node_path": "Path/To/Node" # Mock path
                }
        return {}


ai_service = AIService()
