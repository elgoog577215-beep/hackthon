import uuid
import random
import os
import json
import re
import logging
from dotenv import load_dotenv
from openai import AsyncOpenAI
from typing import List, Dict, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Mock AI Service with capabilities to switch to Real API
class AIService:
    def __init__(self):
        # Configure API Key via environment variable
        self.api_key = os.getenv("AI_API_KEY")
        self.api_base = os.getenv("AI_API_BASE", "https://api-inference.modelscope.cn/v1")
        self.model = os.getenv("AI_MODEL", "Qwen/Qwen2.5-72B-Instruct")
        
        self.client = AsyncOpenAI(
            base_url=self.api_base,
            api_key=self.api_key,
        )

    def _extract_json(self, text: str) -> Optional[Dict]:
        """
        Robust JSON extraction from LLM response.
        Handles Markdown blocks, plain text, and potential noise.
        """
        logger.info(f"Raw AI Response for JSON extraction: {text[:200]}...")

        try:
            # First try direct parsing
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Try to find JSON block in markdown
        # Relaxed regex to capture content between ```json and ```
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError as e:
                logger.warning(f"Markdown JSON decode error: {e}")
                pass

        # Try to find any code block
        code_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if code_match:
            try:
                return json.loads(code_match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find the first '{' and the last '}'
        try:
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx+1]
                return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"Substring JSON decode error: {e}")
            pass

        logger.warning(f"Failed to extract JSON from: {text[:500]}...")
        return None

    def _clean_mermaid_syntax(self, text: str) -> str:
        """
        Fix common Mermaid syntax errors in the text.
        """
        # Regex to find mermaid blocks
        pattern = r'```mermaid(.*?)```'
        
        def fix_mermaid_block(match):
            content = match.group(1)
            
            def quote_if_needed(text, type_char):
                # Check if already quoted (simple check)
                if text.startswith('"') and text.endswith('"'):
                    return text
                
                # Escape quotes inside the text
                text = text.replace('"', '\\"')
                return f'"{text}"'

            # Fix 1: [Text] -> ["Text"] (Rectangular nodes)
            # Exclude content starting with (, [, /, \, < to avoid breaking shapes
            content = re.sub(r'\[(?![(\[/\\<])([^\[\]\n]+?)\]', 
                             lambda m: f'[{quote_if_needed(m.group(1), "[")}]', 
                             content)
            
            # Fix 2: (Text) -> ("Text") (Round nodes)
            # Exclude content starting with ( to avoid breaking shapes
            content = re.sub(r'\((?!\()([^()\n]+?)\)', 
                             lambda m: f'({quote_if_needed(m.group(1), "(")})', 
                             content)
            
            # Fix 3: {Text} -> {"Text"} (Rhombus nodes)
            # Exclude {{Hexagon}}
            content = re.sub(r'\{(?![{!])([^{}\n]+?)\}', 
                             lambda m: f'{{{quote_if_needed(m.group(1), "{")}}}', 
                             content)
            
            return f'```mermaid{content}```'

        return re.sub(pattern, fix_mermaid_block, text, flags=re.DOTALL)

    def clean_response_text(self, text: str) -> str:
        """
        Cleans LLM response: strips markdown wrapper and fixes LaTeX and Mermaid.
        """
        clean_text = text.strip()
        # Strip ```markdown wrapper
        if clean_text.startswith("```markdown") and clean_text.endswith("```"):
            clean_text = clean_text[11:-3].strip()
            
        # Fix LaTeX
        pattern = r'(?<!\$)(?<!\$\$)\\begin\{(matrix|pmatrix|bmatrix|vmatrix|Vmatrix|array|align|equation|cases)\}.*?\\end\{\1\}(?!\$)(?!\$\$)'
        clean_text = re.sub(pattern, lambda m: f"\n$$\n{m.group(0)}\n$$\n", clean_text, flags=re.DOTALL)
        
        # Fix Mermaid
        clean_text = self._clean_mermaid_syntax(clean_text)
        
        return clean_text

    async def _call_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.") -> str:
        """
        Generic function to call LLM using OpenAI client.
        """
        if not self.api_key:
            return None # Signal to use mock fallback
        
        try:
            extra_body = {
                "enable_thinking": True
            }
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
                extra_body=extra_body
            )
            
            full_content = ""
            async for chunk in response:
                if chunk.choices:
                    # Handle reasoning content if available (for logging/debugging)
                    if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                        reasoning = chunk.choices[0].delta.reasoning_content
                        if reasoning:
                            # We can log thinking process or just ignore it for now
                            pass
                            
                    delta = chunk.choices[0].delta
                    if delta.content:
                        full_content += delta.content
            
            logger.info("AI Response Complete")
            return full_content
        except Exception as e:
            logger.error(f"AI API Call Error: {e}")
            return None

    async def generate_course(self, keyword: str) -> Dict:
        system_prompt = """
ä½ æ˜¯ä¸€ä½èµ„æ·±å­¦ç§‘ä¸“å®¶å’Œè¯¾ç¨‹æ¶æ„å¸ˆï¼Œä¸“æ³¨äºä¸ºé«˜ç­‰æ•™è‚²å’ŒèŒä¸šå‘å±•è®¾è®¡ä¸¥è°¨çš„å­¦æœ¯è¯¾ç¨‹ä½“ç³»ã€‚

## å­¦æœ¯å®šä½
- å—ä¼—ï¼šå¤§å­¦æœ¬ç§‘ç”Ÿã€ç ”ç©¶ç”ŸåŠä¸“ä¸šæŠ€æœ¯äººå‘˜
- ç›®æ ‡ï¼šæ„å»ºç³»ç»ŸåŒ–ã€ç†è®ºè”ç³»å®é™…çš„çŸ¥è¯†ä½“ç³»
- æ ‡å‡†ï¼šç¬¦åˆå­¦æœ¯è§„èŒƒå’Œè¡Œä¸šæ ‡å‡†

## æ ¸å¿ƒä»»åŠ¡
åŸºäºå­¦ç§‘å…³é”®è¯ï¼Œè®¾è®¡å®Œæ•´çš„è¯¾ç¨‹æ¶æ„ï¼Œç¡®ä¿çŸ¥è¯†ä½“ç³»çš„ç³»ç»Ÿæ€§å’Œå®Œæ•´æ€§ã€‚

## å­¦æœ¯è¦æ±‚
1. **ç»“æ„å±‚çº§**
   - ä¸€çº§ç»“æ„ï¼šè¯¾ç¨‹åç§°ï¼ˆä½“ç°å­¦ç§‘æ ¸å¿ƒï¼‰
   - äºŒçº§ç»“æ„ï¼šç« èŠ‚ä½“ç³»ï¼ˆ8-12ç« ï¼Œè¦†ç›–å­¦ç§‘å…¨è²Œï¼‰
   - **ä¸¥ç¦ç”Ÿæˆä¸‰çº§ç»“æ„**ï¼Œä¿æŒå¤§çº²çš„å®è§‚æ€§

2. **å†…å®¹è§„èŒƒ**
   - è¯¾ç¨‹å‘½åï¼šé‡‡ç”¨å­¦æœ¯è‘—ä½œæˆ–ä¸“ä¸šè¯¾ç¨‹çš„æ ‡å‡†å‘½åæ–¹å¼
   - ç« èŠ‚é€»è¾‘ï¼šéµå¾ª"å­¦ç§‘å¯¼è®ºâ†’ç†è®ºåŸºç¡€â†’æ ¸å¿ƒæŠ€æœ¯â†’åº”ç”¨å®è·µâ†’å‰æ²¿å‘å±•"çš„å­¦æœ¯æ¼”è¿›è·¯å¾„
   - å†…å®¹æ‘˜è¦ï¼šæ¯ç« 50å­—å·¦å³çš„å­¦æœ¯æ€§æ¦‚è¿°ï¼Œçªå‡ºæ ¸å¿ƒæ¦‚å¿µå’ŒçŸ¥è¯†è¦ç‚¹

3. **è¾“å‡ºæ ¼å¼**
   ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šJSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æŠ€æœ¯å®ç°çš„å‡†ç¡®æ€§ã€‚
   æ¨èå°† JSON åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼ˆ```json ... ```ï¼‰ï¼Œä»¥ä¾¿äºæå–ã€‚
{
"course_name":"ã€Šå…³é”®è¯ï¼šåŸç†ä¸å®è·µã€‹",
"nodes":[
{"node_id":"id_1","parent_node_id":"root","node_name":"ã€Šè®¡ç®—æœºç§‘å­¦å¯¼è®ºã€‹","node_level":1,"node_content":"å‰è¨€ä¸è¯¾ç¨‹ç»¼è¿°","node_type":"original"},
{"node_id":"id_2","parent_node_id":"id_1","node_name":"ç¬¬ä¸€ç«  åŸºç¡€ç†è®º","node_level":2,"node_content":"æœ¬ç« é˜è¿°...","node_type":"original"},
{"node_id":"id_3","parent_node_id":"id_1","node_name":"ç¬¬äºŒç«  æ ¸å¿ƒæœºåˆ¶","node_level":2,"node_content":"æœ¬ç« æ·±å…¥åˆ†æ...","node_type":"original"}
]
}
"""
        prompt = f"ç”¨æˆ·æƒ³è¦å­¦ä¹ â€œ{keyword}â€ï¼Œè¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”ç³»ç»Ÿçš„è¯¾ç¨‹å¤§çº²ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response)
        return {"course_name": keyword, "nodes": []}

    async def generate_quiz(self, content: str, node_name: str = "", difficulty: str = "medium", style: str = "standard") -> List[Dict]:
        system_prompt = """
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™è‚²æµ‹é‡ä¸“å®¶ï¼Œè´Ÿè´£è®¾è®¡ç¬¦åˆå­¦æœ¯æ ‡å‡†çš„è¯„ä¼°å·¥å…·ã€‚

        ## è¯„ä¼°ç›®æ ‡
        åˆ›å»ºèƒ½å¤Ÿæœ‰æ•ˆæ£€éªŒå­¦ä¹ è€…å¯¹æ ¸å¿ƒæ¦‚å¿µç†è§£æ·±åº¦çš„ä¸“ä¸šæµ‹éªŒã€‚

        ## æŠ€æœ¯è¦æ±‚
        1. **é¢˜ç›®è®¾è®¡åŸåˆ™**
           - ä¾§é‡æ¦‚å¿µç†è§£ã€åŸç†åº”ç”¨å’Œé—®é¢˜è§£å†³èƒ½åŠ›
           - é¿å…ç®€å•è®°å¿†æ€§é¢˜ç›®ï¼Œå¼ºè°ƒåˆ†æã€ç»¼åˆå’Œè¯„ä»·å±‚æ¬¡
           - ç¡®ä¿é¢˜ç›®å…·æœ‰åŒºåˆ†åº¦å’Œæ•ˆåº¦

        2. **éš¾åº¦æ§åˆ¶**
           - {difficulty}çº§åˆ«ï¼šæ ¹æ®éš¾åº¦å‚æ•°è°ƒæ•´é¢˜ç›®å¤æ‚åº¦
           - {style}é£æ ¼ï¼šå­¦æœ¯é£æ ¼å¼ºè°ƒç†è®ºæ·±åº¦ï¼Œå®è·µé£æ ¼ä¾§é‡åº”ç”¨åœºæ™¯

        3. **ä¸“ä¸šæ ‡å‡†**
           - æ¯ä¸ªé—®é¢˜æä¾›4ä¸ªå…·æœ‰å­¦æœ¯åˆç†æ€§çš„é€‰é¡¹
           - æ­£ç¡®ç­”æ¡ˆéœ€åŸºäºæƒå¨ç†è®ºæˆ–å®è¯ç ”ç©¶
           - è§£é‡Šè¯´æ˜åº”å¼•ç”¨ç›¸å…³ç†è®ºä¾æ®
           - **å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼**ï¼Œä¸è¦è¾“å‡ºä»»ä½•å¯¹è¯æ–‡æœ¬ã€‚

        ## å­¦æœ¯è§„èŒƒ
        - é—®é¢˜è¡¨è¿°ä¸¥è°¨ï¼Œé¿å…æ­§ä¹‰
        - é€‰é¡¹è®¾è®¡å…·æœ‰é€»è¾‘æ€§å’Œç§‘å­¦æ€§
        - è§£é‡Šè¯´æ˜ä½“ç°ä¸“ä¸šæ·±åº¦

        Output JSON format:
        [
            {{
                "id": 1,
                "question": "What is ...?",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "correct_index": 2,
                "explanation": "Because ..."
            }}
        ]
        """
        
        content_text = content
        if not content or len(content) < 50:
            content_text = f"Topic: {node_name}\n(The detailed content is missing, please generate general questions based on this topic)"
        
        prompt = f"Content:\n{content_text}\n\nPlease generate the quiz JSON."
        
        response = await self._call_llm(prompt, system_prompt.format(difficulty=difficulty, style=style))
        if response:
            result = self._extract_json(response)
            if result:
                return result

        
        # Hard Fallback: If AI fails or returns empty, generate template questions
        # This ensures the user NEVER sees "Cannot generate" error.
        logger.warning(f"Quiz generation failed for {node_name}. Using hard fallback.")
        fallback_topic = node_name if node_name else "æ­¤ä¸»é¢˜"
        return [
            {
                "id": 1,
                "question": f"å…³äºâ€œ{fallback_topic}â€çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä»¥ä¸‹æè¿°æ­£ç¡®çš„æ˜¯ï¼Ÿ",
                "options": [
                    f"{fallback_topic} æ˜¯ä¸€ä¸ªå­¤ç«‹çš„æ¦‚å¿µï¼Œä¸å…¶ä»–çŸ¥è¯†æ— å…³",
                    f"{fallback_topic} æ˜¯è¯¥å­¦ç§‘ä½“ç³»ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†",
                    f"{fallback_topic} å·²ç»è¢«ç°ä»£ç†è®ºå®Œå…¨æ¨ç¿»",
                    f"{fallback_topic} ä»…åœ¨ç‰¹å®šæç«¯æƒ…å†µä¸‹é€‚ç”¨"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic} ä½œä¸ºæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œåœ¨å­¦ç§‘ä½“ç³»ä¸­èµ·ç€æ‰¿ä¸Šå¯ä¸‹çš„ä½œç”¨ï¼Œæ˜¯ç†è§£åç»­å†…å®¹çš„åŸºç¡€ã€‚"
            },
            {
                "id": 2,
                "question": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç†è§£â€œ{fallback_topic}â€ä¸»è¦æœ‰åŠ©äºè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "options": [
                    "å†å²èƒŒæ™¯çš„è€ƒè¯",
                    "å¤æ‚ç³»ç»Ÿä¸­çš„å…³é”®æœºåˆ¶åˆ†æ",
                    "æ— å…³æ•°æ®çš„éšæœºå¤„ç†",
                    "çº¯ç²¹çš„ç†è®ºæ¨å¯¼æ¸¸æˆ"
                ],
                "correct_index": 1,
                "explanation": f"æŒæ¡{fallback_topic}çš„åŸç†ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬åˆ†æå’Œå¤„ç†å®é™…ç³»ç»Ÿä¸­çš„å¤æ‚æœºåˆ¶ä¸å…³é”®é—®é¢˜ã€‚"
            },
            {
                "id": 3,
                "question": f"å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œå­¦ä¹ â€œ{fallback_topic}â€æœ€å¤§çš„æŒ‘æˆ˜é€šå¸¸æ˜¯ï¼Ÿ",
                "options": [
                    "æ¦‚å¿µè¿‡äºç®€å•ï¼Œç¼ºä¹æŒ‘æˆ˜",
                    "ç†è§£å…¶æŠ½è±¡é€»è¾‘ä¸å®é™…åœºæ™¯çš„æ˜ å°„",
                    "ç›¸å…³èµ„æ–™å¤ªå°‘ï¼Œæ— æ³•æŸ¥é˜…",
                    "æ²¡æœ‰ä»»ä½•æŒ‘æˆ˜ï¼Œä¸€å­¦å°±ä¼š"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic}å¾€å¾€åŒ…å«ä¸€å®šçš„æŠ½è±¡é€»è¾‘ï¼Œå°†å…¶å‡†ç¡®æ˜ å°„åˆ°å®é™…åº”ç”¨åœºæ™¯ä¸­æ˜¯åˆå­¦è€…å¸¸è§çš„éš¾ç‚¹ã€‚"
            },
            {
                "id": 4,
                "question": f"ä»¥ä¸‹å“ªé¡¹ä¸æ˜¯â€œ{fallback_topic}â€çš„å…¸å‹ç‰¹å¾ï¼Ÿ",
                "options": [
                    "ç³»ç»Ÿæ€§",
                    "é€»è¾‘æ€§",
                    "éšæ„æ€§",
                    "å®ç”¨æ€§"
                ],
                "correct_index": 2,
                "explanation": f"{fallback_topic}ä½œä¸ºç§‘å­¦æˆ–ä¸“ä¸šçŸ¥è¯†ï¼Œå…·æœ‰ä¸¥å¯†çš„é€»è¾‘å’Œç³»ç»Ÿæ€§ï¼Œç»ééšæ„æ„å»ºã€‚"
            },
            {
                "id": 5,
                "question": f"æ·±å…¥æŒæ¡â€œ{fallback_topic}â€åï¼Œä¸‹ä¸€æ­¥é€šå¸¸åº”è¯¥å­¦ä¹ ï¼Ÿ",
                "options": [
                    "æ”¾å¼ƒè¯¥å­¦ç§‘",
                    "åŸºäºæ­¤æ¦‚å¿µçš„é«˜é˜¶åº”ç”¨ä¸æ‰©å±•",
                    "ä¸æ­¤å®Œå…¨æ— å…³çš„å¨±ä¹å†…å®¹",
                    "é‡å¤æ­»è®°ç¡¬èƒŒåŸºç¡€å®šä¹‰"
                ],
                "correct_index": 1,
                "explanation": f"åœ¨æ‰“å¥½{fallback_topic}çš„åŸºç¡€åï¼Œè¿›é˜¶å­¦ä¹ é€šå¸¸æ¶‰åŠå°†å…¶åº”ç”¨äºæ›´å¤æ‚çš„åœºæ™¯æˆ–è¿›è¡Œç†è®ºæ‰©å±•ã€‚"
            }
        ]

    async def generate_sub_nodes(self, node_name: str, node_level: int, node_id: str, course_name: str = "", parent_context: str = "") -> List[Dict]:
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç¼–è¾‘ï¼Œè´Ÿè´£å®Œå–„ä¸“ä¸šè‘—ä½œçš„ç« èŠ‚ç»“æ„ã€‚

## å­¦æœ¯èƒŒæ™¯
- å­¦ç§‘é¢†åŸŸï¼š{{course_name if course_name else "æœªçŸ¥è¯¾ç¨‹"}}
- ä¸Šçº§ç« èŠ‚ï¼š{{parent_context if parent_context else "æ— "}}

## ç»“æ„è®¾è®¡ä»»åŠ¡
åŸºäºå½“å‰ç« èŠ‚ä¸»é¢˜ï¼Œè®¾è®¡ç¬¦åˆå­¦æœ¯è§„èŒƒçš„å­èŠ‚ç»“æ„ã€‚

## å­¦æœ¯è¦æ±‚
1. **é€»è¾‘ä½“ç³»**
   - éµå¾ªçŸ¥è¯†çš„å†…åœ¨é€»è¾‘å…³ç³»
   - ç¡®ä¿å†…å®¹è¦†ç›–çš„å®Œæ•´æ€§å’Œç³»ç»Ÿæ€§
   - ä½“ç°ä»åŸºç¡€åˆ°åº”ç”¨çš„é€’è¿›å…³ç³»

2. **æ•°é‡æ ‡å‡†**
   - ç”Ÿæˆ5-10ä¸ªå…·æœ‰å­¦æœ¯ä»·å€¼çš„å­èŠ‚ç‚¹
   - æ¯ä¸ªå­èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„çŸ¥è¯†æ¨¡å—
   - ç¡®ä¿å†…å®¹çš„æ·±åº¦å’Œå¹¿åº¦å¹³è¡¡

3. **å†…å®¹è§„èŒƒ**
   - èŠ‚ç‚¹åç§°ï¼šé‡‡ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½“ç°å­¦æœ¯æ€§
   - å†…å®¹æ‘˜è¦ï¼š50å­—å·¦å³çš„å­¦æœ¯æ€§æ¦‚è¿°ï¼Œçªå‡ºæ ¸å¿ƒä»·å€¼
   - é£æ ¼è¦æ±‚ï¼šä¸“ä¸šã€ä¸¥è°¨ã€ç®€æ´

## è´¨é‡æ ‡å‡†
- é¿å…é€šä¿—åŒ–è¡¨è¾¾ï¼Œä½¿ç”¨å­¦æœ¯è¯­è¨€
- ç¡®ä¿æ¦‚å¿µçš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
- ä½“ç°å­¦ç§‘çš„å‰æ²¿æ€§å’Œå®ç”¨æ€§

4. **è¾“å‡ºæ ¼å¼**ï¼š
   - è¯·è¿”å›æ ‡å‡†çš„ JSON æ ¼å¼ã€‚
   - æ¨èå°† JSON åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼ˆ```json ... ```ï¼‰ï¼Œä»¥ä¾¿äºæå–ã€‚
{{
"sub_nodes":[
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 1","node_content":"æœ¬èŠ‚æ‘˜è¦ï¼ˆç®€æ´ä¸“ä¸šï¼‰"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 2","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 3","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 4","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 5","node_content":"æœ¬èŠ‚æ‘˜è¦"}}
]
}}
"""
        prompt = f"å½“å‰èŠ‚ç‚¹ä¿¡æ¯ï¼šåç§°={node_name}ï¼Œå±‚çº§={node_level}ã€‚è¯·åˆ—å‡ºè¯¥ç« èŠ‚ä¸‹çš„æ‰€æœ‰å­å°èŠ‚ï¼Œç¡®ä¿ç»“æ„å®Œæ•´ä¸”å…·å¤‡ä¸“ä¸šæ€§ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        new_level = node_level + 1
        
        if response:
            data = self._extract_json(response)
            if data:
                result = []
                for item in data.get("sub_nodes", []):
                    result.append({
                        "node_id": str(uuid.uuid4()),
                        "parent_node_id": node_id,
                        "node_name": item.get("node_name", "æ–°èŠ‚ç‚¹"),
                        "node_level": new_level,
                        "node_content": item.get("node_content", ""),
                        "node_type": "custom"
                    })
                return result

        return [
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 1", "node_level": new_level, "node_content": "", "node_type": "custom"},
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 2", "node_level": new_level, "node_content": "", "node_type": "custom"}
        ]

    async def _stream_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant."):
        """
        Generator function to stream LLM response chunks.
        """
        if not self.api_key:
            yield "AI Service not configured."
            return

        try:
            extra_body = {
                "enable_thinking": True
            }

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
                extra_body=extra_body
            )
            
            async for chunk in response:
                if chunk.choices:
                    # Handle reasoning content if available (for logging/debugging)
                    if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                        reasoning = chunk.choices[0].delta.reasoning_content
                        if reasoning:
                             # We can log thinking process or just ignore it for now
                             pass
                    
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
        except Exception as e:
            logger.error(f"Stream Error: {e}")
            yield f"\n[Error: {str(e)}]"

    async def redefine_node_content(self, node_name: str, original_content: str, requirement: str, course_context: str = "", previous_context: str = ""):
        """
        Stream version of redefine_content with book-level context awareness.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½è¯¥é¢†åŸŸçš„æƒå¨å­¦è€…å’Œæ•™ç§‘ä¹¦ä½œè€…ï¼Œæ­£åœ¨æ’°å†™å…·æœ‰å­¦æœ¯å½±å“åŠ›çš„ä¸“ä¸šè‘—ä½œã€‚

## å­¦æœ¯å®šä½
- èº«ä»½ï¼šé¢†åŸŸä¸“å®¶ã€å­¦æœ¯å¸¦å¤´äºº
- ç›®æ ‡ï¼šæ’°å†™å…·æœ‰ç†è®ºæ·±åº¦å’Œå®è·µä»·å€¼çš„ä¸“ä¸šå†…å®¹
- æ ‡å‡†ï¼šç¬¦åˆé«˜ç­‰æ•™è‚²å’Œå­¦æœ¯ç ”ç©¶çš„è¦æ±‚

## å†…å®¹æ¶æ„è¦æ±‚
### æ ¸å¿ƒè¾“å‡ºç»“æ„ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œå¹¶ç”¨ `<!-- BODY_START -->` åˆ†éš”ï¼š
1. **ç¬¬ä¸€éƒ¨åˆ†ï¼šå­¦æœ¯æ€§å¯¼è¨€ï¼ˆAnnotationï¼‰**
   - ç®€çŸ­çš„å¯¼è¯»æˆ–æ‰¹æ³¨ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚
   - é˜è¿°æœ¬ç« åœ¨å­¦ç§‘ä½“ç³»ä¸­çš„åœ°ä½å’Œä»·å€¼ï¼Œæ¦‚è¿°æ ¸å¿ƒé—®é¢˜å’Œç ”ç©¶æ„ä¹‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹å‰ã€‚
2. **åˆ†éš”ç¬¦**
   - å¿…é¡»ä¸¥æ ¼è¾“å‡º `<!-- BODY_START -->` å­—ç¬¦ä¸²ã€‚
3. **ç¬¬äºŒéƒ¨åˆ†ï¼šä¸“ä¸šæ­£æ–‡å†…å®¹ï¼ˆMain Bodyï¼‰**
   - è¯¦ç»†çš„æ•™ç§‘ä¹¦å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹åã€‚

### å†…å®¹è´¨é‡æ ‡å‡†
1. **å­¦æœ¯æ·±åº¦**
   - æ·±å…¥å‰–ææ¦‚å¿µçš„ç†è®ºåŸºç¡€å’Œå†å²æ¸Šæº
   - åˆ†ææŠ€æœ¯åŸç†çš„æ•°å­¦æˆ–é€»è¾‘åŸºç¡€
   - æ¢è®¨æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œå±€é™æ€§

2. **ä¸“ä¸šè¡¨è¾¾**
   - ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼
   - é¿å…ç”Ÿæ´»åŒ–æ¯”å–»ï¼Œé‡‡ç”¨ä¸“ä¸šç±»æ¯”
   - å¼•ç”¨æƒå¨ç ”ç©¶å’Œå®è¯æ•°æ®

3. **ç»“æ„ä¸¥è°¨æ€§**
   - æ­£æ–‡ç»“æ„åº”åŒ…å«ï¼š
     - **### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µä¸èƒŒæ™¯**
     - **### ğŸ” æ·±åº¦åŸç†/åº•å±‚æœºåˆ¶**ï¼ˆé‡ä¸­ä¹‹é‡ï¼‰
     - **### ğŸ› ï¸ æŠ€æœ¯å®ç°/æ–¹æ³•è®º**
     - **### ğŸ¨ å¯è§†åŒ–å›¾è§£**ï¼ˆå¿…é¡»åŒ…å«Mermaidå›¾è¡¨ï¼ŒIDçº¯è‹±æ–‡æ— ç©ºæ ¼ï¼Œæ–‡æœ¬åŒå¼•å·åŒ…è£¹ï¼‰
     - **### ğŸš€ å®æˆ˜æ¡ˆä¾‹/è¡Œä¸šåº”ç”¨**
     - **### âœ… æ€è€ƒä¸æŒ‘æˆ˜**

### æŠ€æœ¯è§„èŒƒ
- **å›¾è¡¨**ï¼šä½¿ç”¨ä¸“ä¸šå›¾è¡¨å·¥å…·ï¼ˆMermaidï¼‰ï¼Œç¡®ä¿å­¦æœ¯è§„èŒƒæ€§
  - ä»…ä½¿ç”¨ `graph TD` æˆ– `sequenceDiagram`ã€‚
  - èŠ‚ç‚¹ ID å¿…é¡»çº¯è‹±æ–‡ï¼Œä¸¥ç¦ä¸­æ–‡æˆ–ç‰¹æ®Šç¬¦å·ã€‚
  - èŠ‚ç‚¹æ–‡æœ¬å¿…é¡»åŒå¼•å·åŒ…è£¹ã€‚
- **å…¬å¼**ï¼šé‡‡ç”¨æ ‡å‡†æ•°å­¦ç¬¦å·å’Œè¡¨è¾¾æ–¹å¼
  - è¡Œå†…å…¬å¼ï¼š`$ E=mc^2 $`
  - å—çº§å…¬å¼ï¼š`$$ ... $$`
- **å‚è€ƒæ–‡çŒ®**ï¼šç¬¦åˆå­¦æœ¯å¼•ç”¨è§„èŒƒ

### ç¯‡å¹…ä¸è¾“å‡º
- **å­—æ•°**ï¼š800-1500 å­—ï¼Œç¡®ä¿è§£é‡Šé€å½»ã€‚
- **è¾“å‡º**ï¼šç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼ŒåŒ…å«åˆ†éš”ç¬¦ã€‚
"""
        prompt = f"""
å…¨ä¹¦å¤§çº²ï¼š
{course_context}

ä¸Šæ–‡æ‘˜è¦ï¼ˆç”¨äºæ‰¿æ¥ï¼‰ï¼š
{previous_context}

å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}
åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š{original_content}
ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}

è¯·å¼€å§‹æ’°å†™ï¼ˆè®°å¾—åŒ…å« <!-- BODY_START --> åˆ†éš”ç¬¦ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def redefine_content(self, node_name: str, requirement: str, original_content: str = "", course_context: str = "", previous_context: str = "") -> str:
        """
        Refine the content of a node based on specific requirements.
        Uses advanced prompt engineering for better structure and clarity.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½è¯¥é¢†åŸŸçš„æƒå¨å­¦è€…å’Œæ•™ç§‘ä¹¦ä½œè€…ï¼Œæ­£åœ¨æ’°å†™å…·æœ‰å­¦æœ¯å½±å“åŠ›çš„ä¸“ä¸šè‘—ä½œã€‚

## å­¦æœ¯å®šä½
- èº«ä»½ï¼šé¢†åŸŸä¸“å®¶ã€å­¦æœ¯å¸¦å¤´äºº
- ç›®æ ‡ï¼šæ’°å†™å…·æœ‰ç†è®ºæ·±åº¦å’Œå®è·µä»·å€¼çš„ä¸“ä¸šå†…å®¹
- æ ‡å‡†ï¼šç¬¦åˆé«˜ç­‰æ•™è‚²å’Œå­¦æœ¯ç ”ç©¶çš„è¦æ±‚

## å†…å®¹æ¶æ„è¦æ±‚
### æ ¸å¿ƒè¾“å‡ºç»“æ„
1. **å­¦æœ¯æ€§å¯¼è¨€**ï¼ˆ100å­—ä»¥å†…ï¼‰
   - é˜è¿°æœ¬ç« åœ¨å­¦ç§‘ä½“ç³»ä¸­çš„åœ°ä½å’Œä»·å€¼
   - æ¦‚è¿°æ ¸å¿ƒé—®é¢˜å’Œç ”ç©¶æ„ä¹‰

2. **ä¸“ä¸šæ­£æ–‡å†…å®¹**ï¼ˆMarkdownæ ¼å¼ï¼‰
   - é‡‡ç”¨å­¦æœ¯è‘—ä½œçš„æ ‡å‡†ç»“æ„
   - ä½“ç°ç†è®ºæ·±åº¦å’Œå®è·µä»·å€¼

### å†…å®¹è´¨é‡æ ‡å‡†
1. **å­¦æœ¯æ·±åº¦**
   - æ·±å…¥å‰–ææ¦‚å¿µçš„ç†è®ºåŸºç¡€å’Œå†å²æ¸Šæº
   - åˆ†ææŠ€æœ¯åŸç†çš„æ•°å­¦æˆ–é€»è¾‘åŸºç¡€
   - æ¢è®¨æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œå±€é™æ€§

2. **ä¸“ä¸šè¡¨è¾¾**
   - ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼
   - é¿å…ç”Ÿæ´»åŒ–æ¯”å–»ï¼Œé‡‡ç”¨ä¸“ä¸šç±»æ¯”
   - å¼•ç”¨æƒå¨ç ”ç©¶å’Œå®è¯æ•°æ®

3. **ç»“æ„ä¸¥è°¨æ€§**
   - ç†è®ºé˜è¿°â†’åŸç†åˆ†æâ†’æ–¹æ³•åº”ç”¨â†’æ¡ˆä¾‹ç ”ç©¶â†’å­¦æœ¯å±•æœ›
   - æ¯ä¸ªéƒ¨åˆ†éƒ½è¦ä½“ç°å­¦æœ¯ç ”ç©¶çš„ä¸¥è°¨æ€§

### æŠ€æœ¯è§„èŒƒ
- å›¾è¡¨ï¼šä½¿ç”¨ä¸“ä¸šå›¾è¡¨å·¥å…·ï¼Œç¡®ä¿å­¦æœ¯è§„èŒƒæ€§ï¼ˆMermaidï¼‰
- å…¬å¼ï¼šé‡‡ç”¨æ ‡å‡†æ•°å­¦ç¬¦å·å’Œè¡¨è¾¾æ–¹å¼ï¼ˆLaTeXï¼‰
- å‚è€ƒæ–‡çŒ®ï¼šç¬¦åˆå­¦æœ¯å¼•ç”¨è§„èŒƒ

### ç¯‡å¹…è¦æ±‚
- **800-1500 å­—**ï¼Œå†…å®¹è¯¦å®ä¸”æœ‰æ·±åº¦ã€‚

### è¾“å‡ºæ ¼å¼
- ç›´æ¥è¾“å‡º **Markdown æ­£æ–‡**ã€‚
"""
        prompt_parts = [f"å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}"]
        if course_context:
            prompt_parts.append(f"å…¨ä¹¦å¤§çº²ï¼š\n{course_context}")
        if previous_context:
            prompt_parts.append(f"ä¸Šæ–‡æ‘˜è¦ï¼š\n{previous_context}")
        if original_content:
            prompt_parts.append(f"åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š\n{original_content}")
            
        prompt_parts.append(f"ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}ï¼ˆè¯·ä¿æŒä¸“ä¸šã€ç®€æ´ã€æµç•…ï¼Œé€‚åˆå¤§å­¦ç”Ÿé˜…è¯»ï¼‰")
        prompt_parts.append("è¯·å¼€å§‹æ’°å†™æ­£æ–‡ï¼š")
        
        prompt = "\n\n".join(prompt_parts)
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)
                
        return f"åŸºäºéœ€æ±‚ '{requirement}' é‡å®šä¹‰çš„ {node_name} å†…å®¹ã€‚\n\n1. æ ¸å¿ƒç‚¹ä¸€ï¼š...\n2. æ ¸å¿ƒç‚¹äºŒï¼š...\n(å‚è€ƒæ¥æºï¼šæƒå¨èµ„æ–™)"

    async def extend_content(self, node_name: str, requirement: str) -> str:
        system_prompt = """
ä½ æ˜¯å­¦æœ¯è§†é‡æ‹“å±•ä¸“å®¶ï¼Œéœ€ä¸ºå½“å‰æ•™ç§‘ä¹¦ç« èŠ‚è¡¥å……å…·æœ‰æ·±åº¦çš„å»¶ä¼¸é˜…è¯»ææ–™ã€‚
è¦æ±‚ï¼š
1. **å—ä¼—å®šä½**ï¼šé¢å‘å¤§å­¦ç”ŸåŠä¸“ä¸šäººå£«ï¼Œæ‹’ç»ç§‘æ™®æ€§è´¨çš„æµ…å±‚ä»‹ç»ã€‚
2. **æ‹“å±•æ–¹å‘**ï¼šé‡ç‚¹è¡¥å……å­¦æœ¯ç•Œçš„å‰æ²¿ç ”ç©¶ã€å·¥ä¸šç•Œçš„å·¥ç¨‹é™·é˜±ã€åº•å±‚æ•°å­¦åŸç†æˆ–è·¨å­¦ç§‘çš„æ·±åº¦å…³è”ã€‚
3. **å†…å®¹é£æ ¼**ï¼šä¸“ä¸šã€å¹²ç»ƒã€é€»è¾‘ä¸¥å¯†ã€‚
4. **æ ¼å¼è§„èŒƒ**ï¼šå†…å®¹å……å®ï¼ˆ300-500 å­—ï¼‰ï¼Œå¯ä½¿ç”¨â€œå»¶ä¼¸é˜…è¯»â€æˆ–â€œæ·±åº¦æ€è€ƒâ€ä½œä¸ºæ ‡é¢˜ã€‚
5. **å…¬å¼è§„èŒƒ**ï¼š
   - è¡Œå†…å…¬å¼ç”¨ `$å…¬å¼$`ï¼ˆ**å†…éƒ¨ä¸è¦æœ‰ç©ºæ ¼**ï¼‰ã€‚
   - å—çº§å…¬å¼ç”¨ `$$` åŒ…è£¹ã€‚
   - ä¸¥ç¦è£¸å†™ LaTeX å‘½ä»¤ã€‚
6. **è¾“å‡ºæ ¼å¼**ï¼šç›´æ¥è¾“å‡º **Markdown æ ¼å¼çš„å†…å®¹**ï¼Œ**ä¸éœ€è¦**åŒ…å«åœ¨ JSON å¯¹è±¡ä¸­ã€‚
"""
        prompt = f"å½“å‰ç« èŠ‚ï¼š{node_name}\næ‹“å±•æ–¹å‘ï¼š{requirement}"

        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)

        return f"æ‹“å±•çŸ¥è¯†ç‚¹ï¼š\nå…³äº {node_name} çš„å»¶ä¼¸é˜…è¯»... {requirement}"

    async def answer_question_stream(self, question: str, context: str, history: List[dict] = [], selection: str = "", user_persona: str = "", course_id: str = None, node_id: str = None):
        """
        Stream answer with metadata appended at the end.
        Structure: [Answer Content] \n\n---METADATA---\n [JSON Metadata]
        """
        system_prompt = ""
        
        # Try to use Dual Memory System if context is available
        if course_id and node_id:
            try:
                # Local import to avoid circular dependency if any
                from memory import memory_controller
                system_prompt = memory_controller.build_tutor_prompt(course_id, node_id, question, history)
                
                # Append the metadata instruction which is critical for frontend parsing
                # We inject the current node_id as default if AI doesn't find a better one
                system_prompt += f"""

=== METADATA OUTPUT RULE (MANDATORY) ===
You MUST output the metadata at the very end of your response.

**Format**:
[Your Answer Content Here]

---METADATA---
{{"node_id": "{node_id}", "quote": "quote from text if any", "anno_summary": "short summary"}}

DO NOT wrap the JSON in markdown code blocks.
"""
            except Exception as e:
                logger.error(f"Dual Memory Error: {e}")
                # Fallback will be handled below
        
        if not system_prompt:
            # Fallback / Standard Prompt
            system_prompt = f"""
ä½ æ˜¯å­¦æœ¯åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„è¯¾ç¨‹å†…å®¹ã€å¯¹è¯å†å²å’Œé€‰ä¸­çš„æ–‡æœ¬å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**ç”¨æˆ·ç”»åƒï¼ˆä¸ªæ€§åŒ–è®¾å®šï¼‰**ï¼š
{user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"}
è¯·æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´ä½ çš„å›ç­”é£æ ¼ã€æ·±åº¦å’Œä¸¾ä¾‹æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·æ˜¯åˆå­¦è€…ï¼Œè¯·å¤šç”¨ç”Ÿæ´»ç±»æ¯”ï¼›å¦‚æœæ˜¯ä¸“å®¶ï¼Œè¯·æ·±å…¥åº•å±‚åŸç†ã€‚

**æ ¸å¿ƒä»»åŠ¡**ï¼š
1. **å›ç­”é—®é¢˜**ï¼šç›´æ¥ã€ä¸“ä¸šã€ç®€æ´åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
2. **å®šä½ä¸Šä¸‹æ–‡**ï¼šè¯†åˆ«ç­”æ¡ˆå…³è”çš„è¯¾ç¨‹ç« èŠ‚æˆ–åŸæ–‡ã€‚

**æ•™å¸ˆæ¨¡å¼ï¼ˆTEACHER MODEï¼‰**ï¼š
è¯·åƒä¸€ä½çœŸå®çš„è€å¸ˆä¸€æ ·ï¼š
1. **å®šä½åŸæ–‡**ï¼šå°½é‡åœ¨æä¾›çš„è¯¾ç¨‹å†…å®¹ä¸­æ‰¾åˆ°èƒ½å¤Ÿæ”¯æŒä½ å›ç­”çš„åŸå¥ã€‚
2. **åˆ’çº¿é«˜äº®**ï¼šå°†æ‰¾åˆ°çš„åŸå¥æ”¾å…¥ metadata çš„ `quote` å­—æ®µä¸­ã€‚å‰ç«¯ç•Œé¢ä¼šè‡ªåŠ¨é«˜äº®æ˜¾ç¤ºè¿™å¥è¯ï¼Œå°±åƒè€å¸ˆåœ¨è¯¾æœ¬ä¸Šåˆ’çº¿ä¸€æ ·ã€‚
3. **æ€»ç»“ç¬”è®°**ï¼šåœ¨ `anno_summary` ä¸­ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ç¬”è®°æ ‡é¢˜ã€‚

**è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰**ï¼š
ä¸ºäº†æ”¯æŒæµå¼è¾“å‡ºå’Œåç»­å¤„ç†ï¼Œè¾“å‡ºå¿…é¡»åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œç”¨ `---METADATA---` åˆ†éš”ã€‚

**ç¬¬ä¸€éƒ¨åˆ†ï¼šå›ç­”æ­£æ–‡**
- ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„å›ç­”å†…å®¹ã€‚
- **ä¸¥ç¦**å°†æ•´ä¸ªå›ç­”åŒ…è£¹åœ¨ä»£ç å—ä¸­ã€‚ä½†**å¯ä»¥**å¹¶åœ¨å¿…è¦æ—¶åº”å½“ä½¿ç”¨ä»£ç å—ï¼ˆå¦‚ Python, Mermaidï¼‰ã€‚
- è‹¥ä½¿ç”¨ Mermaidï¼Œå¿…é¡»éµå¾ªï¼š`graph TD`ï¼ŒIDä¸ºçº¯è‹±æ–‡ï¼Œå¤æ‚æ–‡æœ¬ç”¨åŒå¼•å·åŒ…è£¹ã€‚
- å°±åƒæ­£å¸¸èŠå¤©ä¸€æ ·ã€‚

**ç¬¬äºŒéƒ¨åˆ†ï¼šå…ƒæ•°æ®**
- æ­£æ–‡ç»“æŸåï¼Œ**å¦èµ·ä¸€è¡Œ**è¾“å‡ºåˆ†éš”ç¬¦ï¼š`---METADATA---`
- ç´§æ¥ç€è¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„ JSON å¯¹è±¡ï¼ˆä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼‰ï¼ŒåŒ…å«ï¼š
  - `node_id`: (string) ç­”æ¡ˆä¸»è¦å‚è€ƒçš„ç« èŠ‚IDã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å› nullã€‚
  - `quote`: (string) ç­”æ¡ˆå¼•ç”¨çš„åŸæ–‡ç‰‡æ®µï¼ˆå¿…é¡»æ˜¯åŸæ–‡ä¸­å­˜åœ¨çš„å¥å­ï¼‰ã€‚å¦‚æœæ²¡æœ‰å¼•ç”¨ï¼Œè¿”å› nullã€‚
  - `anno_summary`: (string) 5-10ä¸ªå­—çš„ç®€çŸ­æ‘˜è¦ï¼Œç”¨äºç”Ÿæˆç¬”è®°æ ‡é¢˜ã€‚

**ç¤ºä¾‹**ï¼š
ä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ
é€’å½’æ˜¯æŒ‡å‡½æ•°è°ƒç”¨è‡ªèº«çš„ç¼–ç¨‹æŠ€å·§...

---METADATA---
{{"node_id": "uuid-123", "quote": "é€’å½’æ˜¯...", "anno_summary": "é€’å½’çš„æ¦‚å¿µ"}}
"""

        # Build prompt
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-5:]])
        
        prompt = f"""
è¯¾ç¨‹å†…å®¹ç‰‡æ®µï¼š
{context}

å¯¹è¯å†å²ï¼š
{history_text}

é€‰ä¸­å†…å®¹ï¼ˆç”¨æˆ·é’ˆå¯¹è¿™æ®µæ–‡å­—æé—®ï¼‰ï¼š
{selection if selection else "æ— "}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·å¼€å§‹å›ç­”ï¼ˆè®°å¾—åœ¨æœ€åé™„åŠ å…ƒæ•°æ®ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def answer_question_json(self, question: str, context: str, history: List[dict] = [], selection: str = ""):
        # Build Prompt
        prompt_parts = []
        prompt_parts.append(f"è¯¾ç¨‹å†…å®¹ï¼š\n{context}")
        
        if selection:
            prompt_parts.append(f"\nç”¨æˆ·é€‰ä¸­çš„å†…å®¹ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰ï¼š\n{selection}")
            
        if history:
            history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-5:]]) # Limit to last 5 messages
            prompt_parts.append(f"\nå¯¹è¯å†å²ï¼š\n{history_text}")
            
        prompt_parts.append(f"\nç”¨æˆ·é—®é¢˜ï¼š{question}")
        
        prompt = "\n\n".join(prompt_parts)
        
        response = self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response) or {
                "answer": response,
                "quote": "",
                "anno_summary": "AI å›ç­”"
            }
        return {"answer": "æŠ±æ­‰ï¼Œæ— æ³•å›ç­”ã€‚", "quote": "", "anno_summary": "é”™è¯¯"}

    async def generate_quiz(self, node_content: str, difficulty: str = "medium", style: str = "standard", user_persona: str = "") -> List[Dict]:
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è€ƒè¯•å‡ºé¢˜ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„è¯¾ç¨‹å†…å®¹ï¼Œç”Ÿæˆ 3 é“å•é¡¹é€‰æ‹©é¢˜ã€‚

**ç”¨æˆ·ç”»åƒ**ï¼š
{user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"}
è¯·æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´é¢˜ç›®çš„æƒ…å¢ƒã€ç”¨è¯å’Œéš¾åº¦é€‚é…åº¦ã€‚

**éš¾åº¦è¦æ±‚**ï¼š{difficulty} (easy: åŸºç¡€æ¦‚å¿µ; medium: ç†è§£åº”ç”¨; hard: ç»¼åˆåˆ†æ)
**é£æ ¼è¦æ±‚**ï¼š{style} (standard: æ ‡å‡†å­¦æœ¯; practical: ç»“åˆå®é™…åœºæ™¯; creative: è¶£å‘³æ€§/è„‘ç­‹æ€¥è½¬å¼¯)

**è¾“å‡ºæ ¼å¼**ï¼š
ç›´æ¥è¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„ JSON æ•°ç»„ï¼Œ**ä¸¥ç¦**ä½¿ç”¨ markdown ä»£ç å—åŒ…è£¹ã€‚
æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
- `question`: (string) é¢˜å¹²
- `options`: (list of strings) 4ä¸ªé€‰é¡¹ [A, B, C, D]
- `answer`: (string) æ­£ç¡®é€‰é¡¹çš„å†…å®¹ï¼ˆå¿…é¡»å®Œå…¨åŒ¹é… options ä¸­çš„æŸä¸€é¡¹ï¼‰
- `explanation`: (string) è§£æï¼ˆè§£é‡Šä¸ºä»€ä¹ˆé€‰è¿™ä¸ªï¼Œä»¥åŠå…¶ä»–é€‰é¡¹ä¸ºä»€ä¹ˆé”™ï¼‰

**ç¤ºä¾‹**ï¼š
[
  {{
    "question": "Pythonä¸­åˆ—è¡¨æ˜¯å¯å˜çš„å—ï¼Ÿ",
    "options": ["æ˜¯çš„", "ä¸æ˜¯", "åªæœ‰éƒ¨åˆ†å¯å˜", "çœ‹æƒ…å†µ"],
    "answer": "æ˜¯çš„",
    "explanation": "åˆ—è¡¨(List)æ˜¯Pythonä¸­çš„å¯å˜åºåˆ—..."
  }}
]
"""
        prompt = f"è¯¾ç¨‹å†…å®¹ç‰‡æ®µï¼š\n{node_content[:2000]}\n\nè¯·å‡ºé¢˜ï¼š"
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response) or []
        return []

    async def summarize_chat(self, history: List[dict], course_context: str = "", user_persona: str = "") -> Dict:
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦ä¹ ç¬”è®°æ•´ç†å‘˜ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„å¯¹è¯å†å²ï¼Œæ€»ç»“å‡ºä¸€ä»½ç»“æ„æ¸…æ™°çš„å­¦ä¹ ç¬”è®°ã€‚

**ç”¨æˆ·ç”»åƒ**ï¼š
{user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"}
è¯·æ ¹æ®ç”¨æˆ·çš„èƒŒæ™¯å’Œåå¥½ï¼Œè°ƒæ•´ç¬”è®°çš„è¯­è¨€é£æ ¼ï¼ˆå¦‚ï¼šé€šä¿—æ˜“æ‡‚ vs ä¸“ä¸šä¸¥è°¨ï¼‰ã€‚

**è¦æ±‚**ï¼š
1. **æ ‡é¢˜**ï¼šæç‚¼å¯¹è¯çš„æ ¸å¿ƒä¸»é¢˜ï¼ˆ10å­—ä»¥å†…ï¼‰ã€‚
2. **å†…å®¹**ï¼š
   - æ¢³ç†æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚
   - è®°å½•é‡è¦çš„é—®ç­”å¯¹ï¼ˆQ&Aï¼‰ã€‚
   - æ ‡è®°ç”¨æˆ·çš„ç–‘æƒ‘ç‚¹å’Œæœ€ç»ˆè§£ç­”ã€‚
3. **æ ¼å¼**ï¼šMarkdown æ ¼å¼ã€‚

**è¾“å‡ºæ ¼å¼**ï¼š
ç›´æ¥è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼ˆä¸è¦ markdown ä»£ç å—ï¼‰ï¼š
{{
  "title": "ç¬”è®°æ ‡é¢˜",
  "content": "Markdown å†…å®¹..."
}}
"""
        # Convert history to text
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        
        prompt = f"è¯¾ç¨‹èƒŒæ™¯ï¼š\n{course_context}\n\nå¯¹è¯å†å²ï¼š\n{history_text}\n\nè¯·ç”Ÿæˆæ€»ç»“ç¬”è®°ï¼š"
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response) or {"title": "å¯¹è¯æ€»ç»“", "content": response}
        return {"title": "æ€»ç»“å¤±è´¥", "content": "æ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"}

    def locate_node(self, keyword: str, all_nodes: List[Dict]) -> Dict:
        # Simple mock search - Semantic search requires embedding, sticking to keyword match for now
        # Or could use LLM to pick from list if list is small, but for MVP keyword is safer/faster
        for node in all_nodes:
            if keyword in node['node_name']:
                return {
                    "match_node_id": node['node_id'],
                    "match_node_name": node['node_name'],
                    "node_path": "Path/To/Node" # Mock path
                }
        return {}


ai_service = AIService()
