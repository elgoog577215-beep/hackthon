import uuid
import random
import os
import json
import re
import logging
from dotenv import load_dotenv
from openai import AsyncOpenAI
from typing import List, Dict, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Mock AI Service with capabilities to switch to Real API
class AIService:
    def __init__(self):
        # Configure API Key via environment variable
        self.api_key = os.getenv("AI_API_KEY")
        self.api_base = os.getenv("AI_API_BASE", "https://api-inference.modelscope.cn/v1")
        self.model = os.getenv("AI_MODEL", "Qwen/Qwen2.5-72B-Instruct")
        
        self.client = AsyncOpenAI(
            base_url=self.api_base,
            api_key=self.api_key,
        )

    def _extract_json(self, text: str) -> Optional[Dict]:
        """
        Robust JSON extraction from LLM response.
        Handles Markdown blocks, plain text, and potential noise.
        """
        try:
            # First try direct parsing
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Try to find JSON block in markdown
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find any code block
        code_match = re.search(r'```\s*(\{.*?\})\s*```', text, re.DOTALL)
        if code_match:
            try:
                return json.loads(code_match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find the first '{' and the last '}'
        try:
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx+1]
                return json.loads(json_str)
        except json.JSONDecodeError:
            pass

        logger.warning(f"Failed to extract JSON from: {text[:100]}...")
        return None

    def _clean_mermaid_syntax(self, text: str) -> str:
        """
        Fix common Mermaid syntax errors in the text.
        """
        # Regex to find mermaid blocks
        pattern = r'```mermaid(.*?)```'
        
        def fix_mermaid_block(match):
            content = match.group(1)
            
            def quote_if_needed(text, type_char):
                # Check if already quoted (simple check)
                if text.startswith('"') and text.endswith('"'):
                    return text
                
                # Escape quotes inside the text
                text = text.replace('"', '\\"')
                return f'"{text}"'

            # Fix 1: [Text] -> ["Text"] (Rectangular nodes)
            # Exclude content starting with (, [, /, \, < to avoid breaking shapes
            content = re.sub(r'\[(?![(\[/\\<])([^\[\]\n]+?)\]', 
                             lambda m: f'[{quote_if_needed(m.group(1), "[")}]', 
                             content)
            
            # Fix 2: (Text) -> ("Text") (Round nodes)
            # Exclude content starting with ( to avoid breaking shapes
            content = re.sub(r'\((?!\()([^()\n]+?)\)', 
                             lambda m: f'({quote_if_needed(m.group(1), "(")})', 
                             content)
            
            # Fix 3: {Text} -> {"Text"} (Rhombus nodes)
            # Exclude {{Hexagon}}
            content = re.sub(r'\{(?![{!])([^{}\n]+?)\}', 
                             lambda m: f'{{{quote_if_needed(m.group(1), "{")}}}', 
                             content)
            
            return f'```mermaid{content}```'

        return re.sub(pattern, fix_mermaid_block, text, flags=re.DOTALL)

    def clean_response_text(self, text: str) -> str:
        """
        Cleans LLM response: strips markdown wrapper and fixes LaTeX and Mermaid.
        """
        clean_text = text.strip()
        # Strip ```markdown wrapper
        if clean_text.startswith("```markdown") and clean_text.endswith("```"):
            clean_text = clean_text[11:-3].strip()
            
        # Fix LaTeX
        pattern = r'(?<!\$)(?<!\$\$)\\begin\{(matrix|pmatrix|bmatrix|vmatrix|Vmatrix|array|align|equation|cases)\}.*?\\end\{\1\}(?!\$)(?!\$\$)'
        clean_text = re.sub(pattern, lambda m: f"\n$$\n{m.group(0)}\n$$\n", clean_text, flags=re.DOTALL)
        
        # Fix Mermaid
        clean_text = self._clean_mermaid_syntax(clean_text)
        
        return clean_text

    async def _call_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.") -> str:
        """
        Generic function to call LLM using OpenAI client.
        """
        if not self.api_key:
            return None # Signal to use mock fallback
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True, 
            )
            
            full_content = ""
            async for chunk in response:
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        full_content += delta.content
            
            logger.info("AI Response Complete")
            return full_content
        except Exception as e:
            logger.error(f"AI API Call Error: {e}")
            return None

    async def generate_course(self, keyword: str) -> Dict:
        system_prompt = """
ä½ æ˜¯èµ„æ·±å¤§å­¦æ•™æˆå’Œè¯¾ç¨‹æ¶æ„å¸ˆï¼Œä¸“æ³¨äºä¸ºå¤§å­¦ç”Ÿå’Œä¸“ä¸šäººå£«è®¾è®¡é«˜æ°´å‡†çš„ä¸“ä¸šè¯¾ç¨‹ã€‚
ä»»åŠ¡ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯ï¼Œç”Ÿæˆè¯¾ç¨‹å¤§çº²éª¨æ¶ï¼ˆä»…åŒ…å«ä¹¦åå’Œç« ï¼‰ã€‚

è¦æ±‚ï¼š
1. **ä»…ç”Ÿæˆå¤§çº²éª¨æ¶**ï¼š
   - **ä¸¥ç¦ç”Ÿæˆä»»ä½•ä¸‰çº§å­èŠ‚ç‚¹ï¼ˆèŠ‚ï¼‰**ã€‚
   - **åªç”Ÿæˆ 1 çº§ï¼ˆè¯¾ç¨‹åï¼‰å’Œ 2 çº§ï¼ˆç« ï¼‰**ã€‚
   - 2 çº§ç« èŠ‚ï¼ˆç« ï¼‰æ•°é‡ï¼š**8-12 ç« **ï¼Œè¦†ç›–å…¨ä¹¦å†…å®¹ã€‚
   - **å†…å®¹æ‘˜è¦**ï¼šä¸ºæ¯ä¸€ç« ç”Ÿæˆç®€çŸ­çš„å¯¼è¯»ï¼ˆ50å­—å·¦å³ï¼‰ï¼Œè¯­è¨€ç®€æ´ç²¾ç‚¼ï¼Œæ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹ã€‚

2. **é€»è¾‘ä¸ç»“æ„**ï¼š
   - **å­¦æœ¯ä¸¥è°¨æ€§**ï¼šç¡®ä¿å†…å®¹è¦†ç›–è¯¥å­¦ç§‘çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œä½“ç³»å®Œæ•´ï¼Œæ— é‡å¤§é—æ¼ã€‚
   - **é€»è¾‘é€’è¿›**ï¼šç« èŠ‚é¡ºåºå¿…é¡»éµå¾ªâ€œåŸºç¡€ç†è®º -> æ ¸å¿ƒæœºåˆ¶ -> é«˜é˜¶åº”ç”¨ -> å‰æ²¿æ‹“å±•â€çš„å­¦æœ¯é€»è¾‘ã€‚
   - **å¯¼è®ºå…ˆè¡Œ**ï¼šç¬¬ä¸€ç« å¿…é¡»æ˜¯è¯¥å­¦ç§‘çš„å¯¼è®ºæˆ–ç³»ç»Ÿæ€§æ¦‚è¿°ã€‚

3. **å±‚çº§è§„èŒƒ**ï¼š
   - 1 çº§ï¼ˆè¯¾ç¨‹åç§°ï¼‰ï¼šæ ‡å‡†æ•™ç§‘ä¹¦ä¹¦åæˆ–ä¸“ä¸šè¯¾ç¨‹åç§°ã€‚
   - 2 çº§ï¼ˆç« ï¼‰ï¼šä¸»è¦çŸ¥è¯†æ¨¡å—ã€‚

4. **å—ä¼—å®šä½**ï¼š
   - ç›®æ ‡ç”¨æˆ·ï¼šå¤§å­¦ç”ŸåŠä¸“ä¸šé¢†åŸŸå­¦ä¹ è€…ã€‚
   - é£æ ¼ï¼šä¸“ä¸šã€ç®€æ´ã€æµç•…ï¼Œé¿å…ä½å¹¼åŒ–çš„æ¯”å–»ï¼Œä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æˆ–è¡Œä¸šæœ¯è¯­ï¼Œä½†ä¿æŒè§£é‡Šæ¸…æ™°ã€‚

5. **è¾“å‡ºæ ¼å¼**ï¼š
   - è¯·è¿”å›æ ‡å‡†çš„ JSON æ ¼å¼ã€‚
   - æ¨èå°† JSON åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼ˆ```json ... ```ï¼‰ï¼Œä»¥ä¾¿äºæå–ã€‚
{
"course_name":"ã€Šå…³é”®è¯ï¼šåŸç†ä¸å®è·µã€‹",
"nodes":[
{"node_id":"id_1","parent_node_id":"root","node_name":"ã€Šè®¡ç®—æœºç§‘å­¦å¯¼è®ºã€‹","node_level":1,"node_content":"å‰è¨€ä¸è¯¾ç¨‹ç»¼è¿°","node_type":"original"},
{"node_id":"id_2","parent_node_id":"id_1","node_name":"ç¬¬ä¸€ç«  åŸºç¡€ç†è®º","node_level":2,"node_content":"æœ¬ç« é˜è¿°...","node_type":"original"},
{"node_id":"id_3","parent_node_id":"id_1","node_name":"ç¬¬äºŒç«  æ ¸å¿ƒæœºåˆ¶","node_level":2,"node_content":"æœ¬ç« æ·±å…¥åˆ†æ...","node_type":"original"}
]
}
"""
        prompt = f"ç”¨æˆ·æƒ³è¦å­¦ä¹ â€œ{keyword}â€ï¼Œè¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”ç³»ç»Ÿçš„è¯¾ç¨‹å¤§çº²ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response)
        return {"course_name": keyword, "nodes": []}

    async def generate_quiz(self, content: str, node_name: str = "", difficulty: str = "medium", style: str = "standard") -> List[Dict]:
        system_prompt = """
        You are an expert educator creating a quiz to test understanding of the provided content or topic.
        Create 5 multiple-choice questions based on the key concepts in the text or the topic provided.
        
        Requirements:
        1. Questions should challenge the learner's understanding, not just memory.
        2. Difficulty level: {difficulty}
        3. Style: {style} (if 'creative', use scenarios; if 'practical', use real-world problems; if 'standard', use academic style).
        4. Provide 4 options for each question.
        5. Provide the correct answer index (0-3).
        6. Provide a brief explanation for why the answer is correct.
        7. IMPORTANT: You MUST return valid JSON. Do not output conversational text.
        
        Output JSON format:
        [
            {{
                "id": 1,
                "question": "What is ...?",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "correct_index": 2,
                "explanation": "Because ..."
            }}
        ]
        """
        
        content_text = content
        if not content or len(content) < 50:
            content_text = f"Topic: {node_name}\n(The detailed content is missing, please generate general questions based on this topic)"
        
        prompt = f"Content:\n{content_text}\n\nPlease generate the quiz JSON."
        
        response = await self._call_llm(prompt, system_prompt.format(difficulty=difficulty, style=style))
        if response:
            result = self._extract_json(response)
            if result:
                return result
        
        # Hard Fallback: If AI fails or returns empty, generate template questions
        # This ensures the user NEVER sees "Cannot generate" error.
        logger.warning(f"Quiz generation failed for {node_name}. Using hard fallback.")
        fallback_topic = node_name if node_name else "æ­¤ä¸»é¢˜"
        return [
            {
                "id": 1,
                "question": f"å…³äºâ€œ{fallback_topic}â€çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä»¥ä¸‹æè¿°æ­£ç¡®çš„æ˜¯ï¼Ÿ",
                "options": [
                    f"{fallback_topic} æ˜¯ä¸€ä¸ªå­¤ç«‹çš„æ¦‚å¿µï¼Œä¸å…¶ä»–çŸ¥è¯†æ— å…³",
                    f"{fallback_topic} æ˜¯è¯¥å­¦ç§‘ä½“ç³»ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†",
                    f"{fallback_topic} å·²ç»è¢«ç°ä»£ç†è®ºå®Œå…¨æ¨ç¿»",
                    f"{fallback_topic} ä»…åœ¨ç‰¹å®šæç«¯æƒ…å†µä¸‹é€‚ç”¨"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic} ä½œä¸ºæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œåœ¨å­¦ç§‘ä½“ç³»ä¸­èµ·ç€æ‰¿ä¸Šå¯ä¸‹çš„ä½œç”¨ï¼Œæ˜¯ç†è§£åç»­å†…å®¹çš„åŸºç¡€ã€‚"
            },
            {
                "id": 2,
                "question": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç†è§£â€œ{fallback_topic}â€ä¸»è¦æœ‰åŠ©äºè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "options": [
                    "å†å²èƒŒæ™¯çš„è€ƒè¯",
                    "å¤æ‚ç³»ç»Ÿä¸­çš„å…³é”®æœºåˆ¶åˆ†æ",
                    "æ— å…³æ•°æ®çš„éšæœºå¤„ç†",
                    "çº¯ç²¹çš„ç†è®ºæ¨å¯¼æ¸¸æˆ"
                ],
                "correct_index": 1,
                "explanation": f"æŒæ¡{fallback_topic}çš„åŸç†ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬åˆ†æå’Œå¤„ç†å®é™…ç³»ç»Ÿä¸­çš„å¤æ‚æœºåˆ¶ä¸å…³é”®é—®é¢˜ã€‚"
            },
            {
                "id": 3,
                "question": f"å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œå­¦ä¹ â€œ{fallback_topic}â€æœ€å¤§çš„æŒ‘æˆ˜é€šå¸¸æ˜¯ï¼Ÿ",
                "options": [
                    "æ¦‚å¿µè¿‡äºç®€å•ï¼Œç¼ºä¹æŒ‘æˆ˜",
                    "ç†è§£å…¶æŠ½è±¡é€»è¾‘ä¸å®é™…åœºæ™¯çš„æ˜ å°„",
                    "ç›¸å…³èµ„æ–™å¤ªå°‘ï¼Œæ— æ³•æŸ¥é˜…",
                    "æ²¡æœ‰ä»»ä½•æŒ‘æˆ˜ï¼Œä¸€å­¦å°±ä¼š"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic}å¾€å¾€åŒ…å«ä¸€å®šçš„æŠ½è±¡é€»è¾‘ï¼Œå°†å…¶å‡†ç¡®æ˜ å°„åˆ°å®é™…åº”ç”¨åœºæ™¯ä¸­æ˜¯åˆå­¦è€…å¸¸è§çš„éš¾ç‚¹ã€‚"
            },
            {
                "id": 4,
                "question": f"ä»¥ä¸‹å“ªé¡¹ä¸æ˜¯â€œ{fallback_topic}â€çš„å…¸å‹ç‰¹å¾ï¼Ÿ",
                "options": [
                    "ç³»ç»Ÿæ€§",
                    "é€»è¾‘æ€§",
                    "éšæ„æ€§",
                    "å®ç”¨æ€§"
                ],
                "correct_index": 2,
                "explanation": f"{fallback_topic}ä½œä¸ºç§‘å­¦æˆ–ä¸“ä¸šçŸ¥è¯†ï¼Œå…·æœ‰ä¸¥å¯†çš„é€»è¾‘å’Œç³»ç»Ÿæ€§ï¼Œç»ééšæ„æ„å»ºã€‚"
            },
            {
                "id": 5,
                "question": f"æ·±å…¥æŒæ¡â€œ{fallback_topic}â€åï¼Œä¸‹ä¸€æ­¥é€šå¸¸åº”è¯¥å­¦ä¹ ï¼Ÿ",
                "options": [
                    "æ”¾å¼ƒè¯¥å­¦ç§‘",
                    "åŸºäºæ­¤æ¦‚å¿µçš„é«˜é˜¶åº”ç”¨ä¸æ‰©å±•",
                    "ä¸æ­¤å®Œå…¨æ— å…³çš„å¨±ä¹å†…å®¹",
                    "é‡å¤æ­»è®°ç¡¬èƒŒåŸºç¡€å®šä¹‰"
                ],
                "correct_index": 1,
                "explanation": f"åœ¨æ‰“å¥½{fallback_topic}çš„åŸºç¡€åï¼Œè¿›é˜¶å­¦ä¹ é€šå¸¸æ¶‰åŠå°†å…¶åº”ç”¨äºæ›´å¤æ‚çš„åœºæ™¯æˆ–è¿›è¡Œç†è®ºæ‰©å±•ã€‚"
            }
        ]

    async def generate_sub_nodes(self, node_name: str, node_level: int, node_id: str, course_name: str = "", parent_context: str = "") -> List[Dict]:
        system_prompt = f"""
ä½ æ˜¯ä¸¥è°¨çš„å­¦æœ¯åŠ©æ•™ï¼Œéœ€å®Œå–„å½“å‰ç« èŠ‚çš„è¯¦ç»†ç›®å½•ç»“æ„ã€‚
å½“å‰è¯¾ç¨‹ä¸»é¢˜ï¼šã€Š{{course_name if course_name else "æœªçŸ¥è¯¾ç¨‹"}}ã€‹
ä¸Šçº§ç« èŠ‚æ‘˜è¦ï¼š{{parent_context if parent_context else "æ— "}}
ä»»åŠ¡ï¼šåŸºäºå½“å‰èŠ‚ç‚¹ï¼ˆç« æˆ–èŠ‚ï¼‰ï¼Œç”Ÿæˆä¸‹çº§å­èŠ‚ç‚¹ï¼ˆç›®å½•ï¼‰ã€‚

è¦æ±‚ï¼š
1. **é€»è¾‘ä¸¥å¯†**ï¼šæŒ‰ç…§å¾ªåºæ¸è¿›çš„å­¦ä¹ é€»è¾‘ï¼Œè¡¥å……è¯¥ä¸»é¢˜ä¸‹å¿…é¡»åŒ…å«çš„æ‰€æœ‰å­è¯é¢˜ã€‚
2. **æ•°é‡å¼ºåˆ¶**ï¼š**å¿…é¡»ç”Ÿæˆ 5-10 ä¸ªå­èŠ‚ç‚¹**ï¼Œä¸¥ç¦åªç”Ÿæˆ 2-3 ä¸ªã€‚
3. **å†…å®¹é£æ ¼**ï¼šæ‘˜è¦å†…å®¹è¦ç®€æ´æµç•…ï¼Œé€‚åˆå¤§å­¦ç”Ÿå’Œä¸“ä¸šäººå£«é˜…è¯»ï¼Œä½“ç°å­¦æœ¯æ·±åº¦ã€‚
4. **è¾“å‡ºæ ¼å¼**ï¼š
   - è¯·è¿”å›æ ‡å‡†çš„ JSON æ ¼å¼ã€‚
   - æ¨èå°† JSON åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼ˆ```json ... ```ï¼‰ï¼Œä»¥ä¾¿äºæå–ã€‚
{{
"sub_nodes":[
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 1","node_content":"æœ¬èŠ‚æ‘˜è¦ï¼ˆç®€æ´ä¸“ä¸šï¼‰"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 2","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 3","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 4","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 5","node_content":"æœ¬èŠ‚æ‘˜è¦"}}
]
}}
"""
        prompt = f"å½“å‰èŠ‚ç‚¹ä¿¡æ¯ï¼šåç§°={node_name}ï¼Œå±‚çº§={node_level}ã€‚è¯·åˆ—å‡ºè¯¥ç« èŠ‚ä¸‹çš„æ‰€æœ‰å­å°èŠ‚ï¼Œç¡®ä¿ç»“æ„å®Œæ•´ä¸”å…·å¤‡ä¸“ä¸šæ€§ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        new_level = node_level + 1
        
        if response:
            data = self._extract_json(response)
            if data:
                result = []
                for item in data.get("sub_nodes", []):
                    result.append({
                        "node_id": str(uuid.uuid4()),
                        "parent_node_id": node_id,
                        "node_name": item.get("node_name", "æ–°èŠ‚ç‚¹"),
                        "node_level": new_level,
                        "node_content": item.get("node_content", ""),
                        "node_type": "custom"
                    })
                return result

        return [
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 1", "node_level": new_level, "node_content": "", "node_type": "custom"},
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 2", "node_level": new_level, "node_content": "", "node_type": "custom"}
        ]

    async def _stream_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant."):
        """
        Generator function to stream LLM response chunks.
        """
        if not self.api_key:
            yield "AI Service not configured."
            return

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True
            )
            
            async for chunk in response:
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
        except Exception as e:
            logger.error(f"Stream Error: {e}")
            yield f"\n[Error: {str(e)}]"

    async def redefine_node_content(self, node_name: str, original_content: str, requirement: str, course_context: str = "", previous_context: str = ""):
        """
        Stream version of redefine_content with book-level context awareness.
        """
        system_prompt = """
ä½ æ˜¯è¯¥é¢†åŸŸçš„èµ„æ·±ä¸“å®¶å’Œé‡‘ç‰Œå¤§å­¦è®²å¸ˆï¼Œæ­£åœ¨æ’°å†™ä¸€æœ¬ä¸“ä¸šæ•™ç§‘ä¹¦ã€‚
ä»»åŠ¡ï¼šä¸ºå½“å‰ç« èŠ‚æ’°å†™**ç« èŠ‚ç®€ä»‹**å’Œ**æ­£æ–‡å†…å®¹**ã€‚

### æ ¸å¿ƒè¾“å‡ºç»“æ„ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œå¹¶ç”¨ `<!-- BODY_START -->` åˆ†éš”ï¼š
1. **ç¬¬ä¸€éƒ¨åˆ†ï¼šç« èŠ‚ç®€ä»‹ï¼ˆAnnotationï¼‰**
   - ç®€çŸ­çš„å¯¼è¯»æˆ–æ‰¹æ³¨ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹å‰ã€‚
2. **åˆ†éš”ç¬¦**
   - å¿…é¡»ä¸¥æ ¼è¾“å‡º `<!-- BODY_START -->` å­—ç¬¦ä¸²ã€‚
3. **ç¬¬äºŒéƒ¨åˆ†ï¼šæ•™ç§‘ä¹¦æ­£æ–‡ï¼ˆMain Bodyï¼‰**
   - è¯¦ç»†çš„æ•™ç§‘ä¹¦å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹åã€‚

### 1. æ ¸å¿ƒåŸºè°ƒä¸é£æ ¼
- **æ·±åº¦ä¸å¯å‘**ï¼šæ‹’ç»ç…§æœ¬å®£ç§‘ã€‚åœ¨ç»™å‡ºå®šä¹‰å‰ï¼Œå…ˆé˜è¿°â€œä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ¦‚å¿µâ€æˆ–â€œå®ƒè§£å†³äº†ä»€ä¹ˆæ ¸å¿ƒé—®é¢˜â€ã€‚
- **ä¸“ä¸šä¸”ç”ŸåŠ¨**ï¼šä½¿ç”¨å­¦æœ¯æœ¯è¯­ï¼Œä½†é…åˆç›´è§‚çš„ç±»æ¯”ï¼ˆAnalogyï¼‰è¾…åŠ©ç†è§£ã€‚
- **å…¨ä¹¦è¿è´¯æ€§**ï¼šå¿…é¡»æ‰¿æ¥ä¸Šæ–‡é€»è¾‘ï¼Œé¿å…å­¤ç«‹å†™ä½œã€‚

### 2. åŠ¨æ€ç»“æ„åŒ–å†™ä½œï¼ˆæ­£æ–‡éƒ¨åˆ†ï¼‰
æ­£æ–‡éƒ¨åˆ†ï¼ˆ`<!-- BODY_START -->` ä¹‹åï¼‰å¿…é¡»åŒ…å«ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š
- **### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µä¸èƒŒæ™¯**ï¼šæ¸…æ™°å®šä¹‰ + äº§ç”ŸèƒŒæ™¯/æ ¸å¿ƒä»·å€¼ã€‚
- **### ğŸ” æ·±åº¦åŸç†/åº•å±‚æœºåˆ¶**ï¼šå‰–æå·¥ä½œåŸç†ã€æ•°å­¦æ¨¡å‹æˆ–æ¼”åŒ–é€»è¾‘ï¼ˆé‡ä¸­ä¹‹é‡ï¼‰ã€‚
- **### ğŸ› ï¸ æŠ€æœ¯å®ç°/æ–¹æ³•è®º**ï¼šå…·ä½“çš„æ¨å¯¼ã€ç®—æ³•æ­¥éª¤æˆ–æ‰§è¡Œç»†èŠ‚ã€‚
- **### ğŸ¨ å¯è§†åŒ–å›¾è§£**ï¼š**å¿…é¡»**åŒ…å«è‡³å°‘ä¸€ä¸ª Mermaid å›¾è¡¨ã€‚
  - **Mermaid è§„èŒƒ**ï¼š
    - ä»…ä½¿ç”¨ `graph TD` (æµç¨‹å›¾) æˆ– `sequenceDiagram` (æ—¶åºå›¾)ã€‚
    - **èŠ‚ç‚¹ ID è§„èŒƒ**ï¼šå¿…é¡»æ˜¯çº¯è‹±æ–‡ä¸”æ— ç©ºæ ¼ï¼ˆå¦‚ `NodeA`ï¼‰ï¼Œ**ä¸¥ç¦ä½¿ç”¨ä¸­æ–‡æˆ–ç‰¹æ®Šç¬¦å·ä½œä¸º ID**ã€‚
    - **èŠ‚ç‚¹æ–‡æœ¬è§„èŒƒ**ï¼š**å¿…é¡»**ä½¿ç”¨åŒå¼•å·åŒ…è£¹æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œä¾‹å¦‚ `A["æ–‡æœ¬"]`ï¼Œä»¥é˜²æ­¢ç‰¹æ®Šç¬¦å·å¯¼è‡´è¯­æ³•é”™è¯¯ã€‚
    - ä¸¥ç¦åœ¨èŠ‚ç‚¹ ID ä¸­ä½¿ç”¨ `(` `)` `[` `]`ã€‚
- **### ğŸš€ å®æˆ˜æ¡ˆä¾‹/è¡Œä¸šåº”ç”¨**ï¼šç»“åˆçœŸå®äº§ä¸šç•Œçš„è½åœ°æ¡ˆä¾‹åˆ†æã€‚
- **### âœ… æ€è€ƒä¸æŒ‘æˆ˜**ï¼šæä¾› 1-2 ä¸ªèƒ½å¼•å‘æ·±åº¦æ€è€ƒçš„é—®é¢˜ã€‚

### 3. ä¸¥æ ¼æ ¼å¼è§„èŒƒï¼ˆå…³é”®ï¼ï¼‰
- **å…¬å¼æ’ç‰ˆ**ï¼š
  - è¡Œå†…å…¬å¼ï¼šä»…ä½¿ç”¨ `$ E=mc^2 $`ï¼ˆå‰åä¿ç•™ç©ºæ ¼ï¼‰ã€‚
  - å—çº§å…¬å¼ï¼šä»…ä½¿ç”¨ `$$` åŒ…è£¹ã€‚
  - **ä¸¥ç¦ä½¿ç”¨** `\( ... \)` æˆ– `\[ ... \]`ã€‚
  - æ‰€æœ‰ LaTeX ç¯å¢ƒï¼ˆå¦‚ `\\begin{matrix}`ï¼‰å¿…é¡»åŒ…è£¹åœ¨ `$$` ä¸­ã€‚
- **æ’ç‰ˆç»†èŠ‚**ï¼šå…³é”®æœ¯è¯­ä½¿ç”¨ **åŠ ç²—**ï¼›é‡è¦ç»“è®ºä½¿ç”¨ > å¼•ç”¨å—ã€‚

### 4. ç¯‡å¹…ä¸è¾“å‡º
- **å­—æ•°**ï¼š800-1500 å­—ï¼Œç¡®ä¿è§£é‡Šé€å½»ã€‚
- **è¾“å‡º**ï¼šç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼ŒåŒ…å«åˆ†éš”ç¬¦ã€‚**ä¸¥ç¦**ä½¿ç”¨ ```markdown åŒ…è£¹å…¨æ–‡ã€‚
"""
        prompt = f"""
å…¨ä¹¦å¤§çº²ï¼š
{course_context}

ä¸Šæ–‡æ‘˜è¦ï¼ˆç”¨äºæ‰¿æ¥ï¼‰ï¼š
{previous_context}

å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}
åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š{original_content}
ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}

è¯·å¼€å§‹æ’°å†™ï¼ˆè®°å¾—åŒ…å« <!-- BODY_START --> åˆ†éš”ç¬¦ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def redefine_content(self, node_name: str, requirement: str, original_content: str = "", course_context: str = "", previous_context: str = "") -> str:
        """
        Refine the content of a node based on specific requirements.
        Uses advanced prompt engineering for better structure and clarity.
        """
        system_prompt = """
        ä½ æ˜¯è¯¥é¢†åŸŸçš„èµ„æ·±ä¸“å®¶å’Œé‡‘ç‰Œå¤§å­¦è®²å¸ˆã€‚
        ä»»åŠ¡ï¼šä¸ºå½“å‰èŠ‚ç‚¹æ’°å†™ä¸€æ®µ**é€‚åˆå¤§å­¦ç”Ÿå’Œä¸“ä¸šäººå£«é˜…è¯»çš„æ•™ç§‘ä¹¦æ­£æ–‡**ã€‚

        è¦æ±‚ï¼š
        1. **æ ¸å¿ƒæ•™å­¦é£æ ¼**ï¼š
           - **ç®€æ´æµç•…**ï¼šè¡Œæ–‡å¹²ç»ƒï¼Œé€»è¾‘æ¸…æ™°ï¼Œæ‹’ç»å†—ä½™å’Œä½å¹¼åŒ–è¡¨è¾¾ã€‚
           - **ä¸“ä¸šä¸¥è°¨**ï¼šå‡†ç¡®ä½¿ç”¨å­¦æœ¯æœ¯è¯­ï¼Œå®šä¹‰æ¸…æ™°ï¼Œæ¨å¯¼ä¸¥å¯†ã€‚
           - **æ·±åº¦è§£æ**ï¼šä¸ä»…ä»…åœç•™åœ¨è¡¨é¢å®šä¹‰ï¼Œè¦æ·±å…¥å‰–æèƒŒåçš„åŸç†å’Œæœºåˆ¶ã€‚
           - **åœºæ™¯åŒ–è§£é‡Š**ï¼šä½¿ç”¨å…·ä½“çš„è¡Œä¸šåº”ç”¨åœºæ™¯æˆ–æŠ€æœ¯åœºæ™¯æ¥è¾…åŠ©è§£é‡Šï¼Œè€Œéç®€å•çš„ç”Ÿæ´»ç±»æ¯”ã€‚

        2. **ç»“æ„åŒ–å†™ä½œ**ï¼ˆMarkdown æ ¼å¼ï¼‰ï¼š
           - **### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ**ï¼šæ¸…æ™°ã€ä¸“ä¸šçš„å®šä¹‰ã€‚å¿…è¦æ—¶è¡¥å……èƒŒæ™¯çŸ¥è¯†ã€‚
             - **æ’ç‰ˆè¦æ±‚**ï¼šå…³é”®åè¯ä½¿ç”¨ **åŠ ç²—** å¼ºè°ƒã€‚
           - **### ğŸ” åŸç†ä¸æœºåˆ¶**ï¼šæ·±å…¥è§£æå·¥ä½œåŸç†ã€åº•å±‚é€»è¾‘æˆ–æ•°å­¦æ¨¡å‹ã€‚
           - **### ğŸ› ï¸ å…³é”®æŠ€æœ¯/æ–¹æ³•**ï¼šå…·ä½“çš„æ¨å¯¼è¿‡ç¨‹ã€ç®—æ³•æ­¥éª¤æˆ–æŠ€æœ¯ç»†èŠ‚ã€‚
             - **å…¬å¼è§„èŒƒï¼ˆç»å¯¹ä¸¥æ ¼æ‰§è¡Œï¼‰**ï¼š
               - **è¡Œå†…å…¬å¼**ï¼šå¿…é¡»ä½¿ç”¨ `$å…¬å¼$` æ ¼å¼ï¼ˆä¾‹å¦‚ `$E=mc^2$`ï¼‰ã€‚
               - **å—çº§å…¬å¼**ï¼šå¿…é¡»ä½¿ç”¨ `$$` åŒ…è£¹ï¼Œä¸”ç‹¬å ä¸€è¡Œã€‚
               - **LaTeX ç¯å¢ƒ**ï¼šæ‰€æœ‰çŸ©é˜µã€æ–¹ç¨‹ç»„ï¼ˆå¦‚ `\\begin{matrix}`ï¼‰**å¿…é¡»**åŒ…è£¹åœ¨ `$$` ä¸­ã€‚
   - **### ğŸ¨ æ¶æ„/æµç¨‹å›¾ç¤º**ï¼šä½¿ç”¨ Mermaid è¯­æ³•ç»˜åˆ¶ä¸“ä¸šçš„æµç¨‹å›¾æˆ–æ¶æ„å›¾ã€‚å¿…é¡»ä½¿ç”¨ ```mermaid ä»£ç å—åŒ…è£¹ã€‚
   - **###  è¡Œä¸šåº”ç”¨æ¡ˆä¾‹**ï¼šç»“åˆå®é™…äº§ä¸šç•Œçš„çœŸå®åº”ç”¨æ¡ˆä¾‹è¿›è¡Œåˆ†æã€‚
   - **### âœ… æ€è€ƒä¸æ‹“å±•**ï¼šæä¾› 1-2 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ€è€ƒé¢˜æˆ–è¿›é˜¶é˜…è¯»æ–¹å‘ã€‚

3. **ç¯‡å¹…è¦æ±‚**ï¼š**800-1500 å­—**ï¼Œå†…å®¹è¯¦å®ä¸”æœ‰æ·±åº¦ã€‚
4. **è¾“å‡ºæ ¼å¼**ï¼šç›´æ¥è¾“å‡º **Markdown æ­£æ–‡**ã€‚
"""
        prompt_parts = [f"å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}"]
        if course_context:
            prompt_parts.append(f"å…¨ä¹¦å¤§çº²ï¼š\n{course_context}")
        if previous_context:
            prompt_parts.append(f"ä¸Šæ–‡æ‘˜è¦ï¼š\n{previous_context}")
        if original_content:
            prompt_parts.append(f"åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š\n{original_content}")
            
        prompt_parts.append(f"ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}ï¼ˆè¯·ä¿æŒä¸“ä¸šã€ç®€æ´ã€æµç•…ï¼Œé€‚åˆå¤§å­¦ç”Ÿé˜…è¯»ï¼‰")
        prompt_parts.append("è¯·å¼€å§‹æ’°å†™æ­£æ–‡ï¼š")
        
        prompt = "\n\n".join(prompt_parts)
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)
                
        return f"åŸºäºéœ€æ±‚ '{requirement}' é‡å®šä¹‰çš„ {node_name} å†…å®¹ã€‚\n\n1. æ ¸å¿ƒç‚¹ä¸€ï¼š...\n2. æ ¸å¿ƒç‚¹äºŒï¼š...\n(å‚è€ƒæ¥æºï¼šæƒå¨èµ„æ–™)"

    async def extend_content(self, node_name: str, requirement: str) -> str:
        system_prompt = """
ä½ æ˜¯å­¦æœ¯è§†é‡æ‹“å±•ä¸“å®¶ï¼Œéœ€ä¸ºå½“å‰æ•™ç§‘ä¹¦ç« èŠ‚è¡¥å……å…·æœ‰æ·±åº¦çš„å»¶ä¼¸é˜…è¯»ææ–™ã€‚
è¦æ±‚ï¼š
1. **å—ä¼—å®šä½**ï¼šé¢å‘å¤§å­¦ç”ŸåŠä¸“ä¸šäººå£«ï¼Œæ‹’ç»ç§‘æ™®æ€§è´¨çš„æµ…å±‚ä»‹ç»ã€‚
2. **æ‹“å±•æ–¹å‘**ï¼šé‡ç‚¹è¡¥å……å­¦æœ¯ç•Œçš„å‰æ²¿ç ”ç©¶ã€å·¥ä¸šç•Œçš„å·¥ç¨‹é™·é˜±ã€åº•å±‚æ•°å­¦åŸç†æˆ–è·¨å­¦ç§‘çš„æ·±åº¦å…³è”ã€‚
3. **å†…å®¹é£æ ¼**ï¼šä¸“ä¸šã€å¹²ç»ƒã€é€»è¾‘ä¸¥å¯†ã€‚
4. **æ ¼å¼è§„èŒƒ**ï¼šå†…å®¹å……å®ï¼ˆ300-500 å­—ï¼‰ï¼Œå¯ä½¿ç”¨â€œå»¶ä¼¸é˜…è¯»â€æˆ–â€œæ·±åº¦æ€è€ƒâ€ä½œä¸ºæ ‡é¢˜ã€‚
5. **å…¬å¼è§„èŒƒ**ï¼š
   - è¡Œå†…å…¬å¼ç”¨ `$å…¬å¼$`ï¼ˆ**å†…éƒ¨ä¸è¦æœ‰ç©ºæ ¼**ï¼‰ã€‚
   - å—çº§å…¬å¼ç”¨ `$$` åŒ…è£¹ã€‚
   - ä¸¥ç¦è£¸å†™ LaTeX å‘½ä»¤ã€‚
6. **è¾“å‡ºæ ¼å¼**ï¼šç›´æ¥è¾“å‡º **Markdown æ ¼å¼çš„å†…å®¹**ï¼Œ**ä¸éœ€è¦**åŒ…å«åœ¨ JSON å¯¹è±¡ä¸­ã€‚
"""
        prompt = f"å½“å‰ç« èŠ‚ï¼š{node_name}\næ‹“å±•æ–¹å‘ï¼š{requirement}"

        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)

        return f"æ‹“å±•çŸ¥è¯†ç‚¹ï¼š\nå…³äº {node_name} çš„å»¶ä¼¸é˜…è¯»... {requirement}"

    async def answer_question_stream(self, question: str, context: str, history: List[dict] = [], selection: str = "", user_persona: str = ""):
        """
        Stream answer with metadata appended at the end.
        Structure: [Answer Content] \n\n---METADATA---\n [JSON Metadata]
        """
        system_prompt = f"""
ä½ æ˜¯å­¦æœ¯åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„è¯¾ç¨‹å†…å®¹ã€å¯¹è¯å†å²å’Œé€‰ä¸­çš„æ–‡æœ¬å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**ç”¨æˆ·ç”»åƒï¼ˆä¸ªæ€§åŒ–è®¾å®šï¼‰**ï¼š
{user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"}
è¯·æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´ä½ çš„å›ç­”é£æ ¼ã€æ·±åº¦å’Œä¸¾ä¾‹æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·æ˜¯åˆå­¦è€…ï¼Œè¯·å¤šç”¨ç”Ÿæ´»ç±»æ¯”ï¼›å¦‚æœæ˜¯ä¸“å®¶ï¼Œè¯·æ·±å…¥åº•å±‚åŸç†ã€‚

**æ ¸å¿ƒä»»åŠ¡**ï¼š
1. **å›ç­”é—®é¢˜**ï¼šç›´æ¥ã€ä¸“ä¸šã€ç®€æ´åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
2. **å®šä½ä¸Šä¸‹æ–‡**ï¼šè¯†åˆ«ç­”æ¡ˆå…³è”çš„è¯¾ç¨‹ç« èŠ‚æˆ–åŸæ–‡ã€‚

**è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰**ï¼š
ä¸ºäº†æ”¯æŒæµå¼è¾“å‡ºå’Œåç»­å¤„ç†ï¼Œè¾“å‡ºå¿…é¡»åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œç”¨ `---METADATA---` åˆ†éš”ã€‚

**ç¬¬ä¸€éƒ¨åˆ†ï¼šå›ç­”æ­£æ–‡**
- ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„å›ç­”å†…å®¹ã€‚
- **ä¸¥ç¦**å°†æ•´ä¸ªå›ç­”åŒ…è£¹åœ¨ä»£ç å—ä¸­ã€‚ä½†**å¯ä»¥**å¹¶åœ¨å¿…è¦æ—¶åº”å½“ä½¿ç”¨ä»£ç å—ï¼ˆå¦‚ Python, Mermaidï¼‰ã€‚
- è‹¥ä½¿ç”¨ Mermaidï¼Œå¿…é¡»éµå¾ªï¼š`graph TD`ï¼ŒIDä¸ºçº¯è‹±æ–‡ï¼Œå¤æ‚æ–‡æœ¬ç”¨åŒå¼•å·åŒ…è£¹ã€‚
- å°±åƒæ­£å¸¸èŠå¤©ä¸€æ ·ã€‚

**ç¬¬äºŒéƒ¨åˆ†ï¼šå…ƒæ•°æ®**
- æ­£æ–‡ç»“æŸåï¼Œ**å¦èµ·ä¸€è¡Œ**è¾“å‡ºåˆ†éš”ç¬¦ï¼š`---METADATA---`
- ç´§æ¥ç€è¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„ JSON å¯¹è±¡ï¼ˆä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼‰ï¼ŒåŒ…å«ï¼š
  - `node_id`: (string) ç­”æ¡ˆä¸»è¦å‚è€ƒçš„ç« èŠ‚IDã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å› nullã€‚
  - `quote`: (string) ç­”æ¡ˆå¼•ç”¨çš„åŸæ–‡ç‰‡æ®µã€‚å¦‚æœæ²¡æœ‰å¼•ç”¨ï¼Œè¿”å› nullã€‚
  - `anno_summary`: (string) 5-10ä¸ªå­—çš„ç®€çŸ­æ‘˜è¦ï¼Œç”¨äºç”Ÿæˆç¬”è®°æ ‡é¢˜ã€‚

**ç¤ºä¾‹**ï¼š
ä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ
é€’å½’æ˜¯æŒ‡å‡½æ•°è°ƒç”¨è‡ªèº«çš„ç¼–ç¨‹æŠ€å·§...

---METADATA---
{{"node_id": "uuid-123", "quote": "é€’å½’æ˜¯...", "anno_summary": "é€’å½’çš„æ¦‚å¿µ"}}
"""
        # Build prompt
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-5:]])
        
        prompt = f"""
è¯¾ç¨‹å†…å®¹ç‰‡æ®µï¼š
{context}

å¯¹è¯å†å²ï¼š
{history_text}

é€‰ä¸­å†…å®¹ï¼ˆç”¨æˆ·é’ˆå¯¹è¿™æ®µæ–‡å­—æé—®ï¼‰ï¼š
{selection if selection else "æ— "}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·å¼€å§‹å›ç­”ï¼ˆè®°å¾—åœ¨æœ€åé™„åŠ å…ƒæ•°æ®ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def answer_question_json(self, question: str, context: str, history: List[dict] = [], selection: str = ""):
        # Build Prompt
        prompt_parts = []
        prompt_parts.append(f"è¯¾ç¨‹å†…å®¹ï¼š\n{context}")
        
        if selection:
            prompt_parts.append(f"\nç”¨æˆ·é€‰ä¸­çš„å†…å®¹ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰ï¼š\n{selection}")
            
        if history:
            history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-5:]]) # Limit to last 5 messages
            prompt_parts.append(f"\nå¯¹è¯å†å²ï¼š\n{history_text}")
            
        prompt_parts.append(f"\nç”¨æˆ·é—®é¢˜ï¼š{question}")
        
        prompt = "\n\n".join(prompt_parts)
        
        response = self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response) or {
                "answer": response,
                "quote": "",
                "anno_summary": "AI å›ç­”"
            }
        return {"answer": "æŠ±æ­‰ï¼Œæ— æ³•å›ç­”ã€‚", "quote": "", "anno_summary": "é”™è¯¯"}

    def locate_node(self, keyword: str, all_nodes: List[Dict]) -> Dict:
        # Simple mock search - Semantic search requires embedding, sticking to keyword match for now
        # Or could use LLM to pick from list if list is small, but for MVP keyword is safer/faster
        for node in all_nodes:
            if keyword in node['node_name']:
                return {
                    "match_node_id": node['node_id'],
                    "match_node_name": node['node_name'],
                    "node_path": "Path/To/Node" # Mock path
                }
        return {}


ai_service = AIService()
