import uuid
import random
import os
import json
import re
import logging
from dotenv import load_dotenv
from openai import AsyncOpenAI
from typing import List, Dict, Optional

# Import prompt templates
from prompts import (
    get_prompt,
    TUTOR_SYSTEM_BASE,
    TUTOR_METADATA_RULE,
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Mock AI Service with capabilities to switch to Real API
class AIService:
    def __init__(self):
        # Configure API Key via environment variable
        self.api_key = os.getenv("AI_API_KEY")
        self.api_base = os.getenv("AI_API_BASE", "https://api-inference.modelscope.cn/v1")
        
        # Hybrid Model Strategy
        # Smart Model: For complex reasoning, creative writing, and detailed explanations.
        self.model_smart = os.getenv("AI_MODEL", "Qwen/Qwen2.5-72B-Instruct")
        
        # Fast Model: For summarization, classification, and simple tasks.
        # Default to a smaller, faster model if not specified.
        self.model_fast = os.getenv("AI_MODEL_FAST", "Qwen/Qwen2.5-7B-Instruct")
        
        self.client = AsyncOpenAI(
            base_url=self.api_base,
            api_key=self.api_key,
        )

    def _extract_json(self, text: str) -> Optional[Dict]:
        """
        Robust JSON extraction from LLM response.
        Handles Markdown blocks, plain text, and potential noise.
        """
        logger.info(f"Raw AI Response for JSON extraction: {text[:200]}...")

        try:
            # First try direct parsing
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Try to find JSON block in markdown
        # Relaxed regex to capture content between ```json and ```
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError as e:
                logger.warning(f"Markdown JSON decode error: {e}")
                pass

        # Try to find any code block
        code_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if code_match:
            try:
                return json.loads(code_match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find the first '{' and the last '}'
        try:
            start_idx = text.find('{')
            end_idx = text.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = text[start_idx:end_idx+1]
                return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"Substring JSON decode error: {e}")
            pass

        logger.warning(f"Failed to extract JSON from: {text[:500]}...")
        return None

    def _clean_mermaid_syntax(self, text: str) -> str:
        """
        Fix common Mermaid syntax errors in the text.
        """
        # Regex to find mermaid blocks
        pattern = r'```mermaid(.*?)```'
        
        def fix_mermaid_block(match):
            content = match.group(1)
            
            def quote_if_needed(text, type_char):
                # Check if already quoted (simple check)
                if text.startswith('"') and text.endswith('"'):
                    return text
                
                # Escape quotes inside the text
                text = text.replace('"', '\\"')
                return f'"{text}"'

            # Fix 1: [Text] -> ["Text"] (Rectangular nodes)
            # Exclude content starting with (, [, /, \, < to avoid breaking shapes
            content = re.sub(r'\[(?![(\[/\\<])([^\[\]\n]+?)\]', 
                             lambda m: f'[{quote_if_needed(m.group(1), "[")}]', 
                             content)
            
            # Fix 2: (Text) -> ("Text") (Round nodes)
            # Exclude content starting with ( to avoid breaking shapes
            content = re.sub(r'\((?!\()([^()\n]+?)\)', 
                             lambda m: f'({quote_if_needed(m.group(1), "(")})', 
                             content)
            
            # Fix 3: {Text} -> {"Text"} (Rhombus nodes)
            # Exclude {{Hexagon}}
            content = re.sub(r'\{(?![{!])([^{}\n]+?)\}', 
                             lambda m: f'{{{quote_if_needed(m.group(1), "{")}}}', 
                             content)
            
            return f'```mermaid{content}```'

        return re.sub(pattern, fix_mermaid_block, text, flags=re.DOTALL)

    def clean_response_text(self, text: str) -> str:
        """
        Cleans LLM response: strips markdown wrapper and fixes LaTeX and Mermaid.
        """
        clean_text = text.strip()
        # Strip ```markdown wrapper
        if clean_text.startswith("```markdown") and clean_text.endswith("```"):
            clean_text = clean_text[11:-3].strip()
            
        # Fix LaTeX
        pattern = r'(?<!\$)(?<!\$\$)\\begin\{(matrix|pmatrix|bmatrix|vmatrix|Vmatrix|array|align|equation|cases)\}.*?\\end\{\1\}(?!\$)(?!\$\$)'
        clean_text = re.sub(pattern, lambda m: f"\n$$\n{m.group(0)}\n$$\n", clean_text, flags=re.DOTALL)
        
        # Fix Mermaid
        clean_text = self._clean_mermaid_syntax(clean_text)
        
        return clean_text

    async def _call_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.", use_fast_model: bool = False) -> str:
        """
        Generic function to call LLM using OpenAI client.
        Supports Model Routing (Smart vs Fast).
        """
        if not self.api_key:
            return None # Signal to use mock fallback
        
        try:
            extra_body = {
                "enable_thinking": True
            }
            
            # Select Model
            model_id = self.model_fast if use_fast_model else self.model_smart
            
            response = await self.client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
                extra_body=extra_body
            )
            
            full_content = ""
            async for chunk in response:
                if chunk.choices:
                    # Handle reasoning content if available (for logging/debugging)
                    if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                        reasoning = chunk.choices[0].delta.reasoning_content
                        if reasoning:
                            # Log thinking process to console to match user expectation
                            print(reasoning, end='', flush=True)
                            
                    delta = chunk.choices[0].delta
                    if delta.content:
                        full_content += delta.content
            
            logger.info(f"AI Response Complete (Model: {model_id})")
            return full_content
        except Exception as e:
            logger.error(f"AI API Call Error: {e}")
            return None

    async def generate_course(self, keyword: str, difficulty: str = "medium", style: str = "academic", requirements: str = "") -> Dict:
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±å­¦ç§‘ä¸“å®¶å’Œè¯¾ç¨‹æ¶æ„å¸ˆï¼Œä¸“æ³¨äºä¸ºé«˜ç­‰æ•™è‚²å’ŒèŒä¸šå‘å±•è®¾è®¡ä¸¥è°¨çš„å­¦æœ¯è¯¾ç¨‹ä½“ç³»ã€‚

## è¯¾ç¨‹é…ç½®
- éš¾åº¦ç­‰çº§ï¼š{difficulty} (beginner/medium/advanced)
- æ•™å­¦é£æ ¼ï¼š{style}
- é¢å¤–è¦æ±‚ï¼š{requirements if requirements else "æ— "}

## å­¦æœ¯å®šä½
- å—ä¼—ï¼šå¤§å­¦æœ¬ç§‘ç”Ÿã€ç ”ç©¶ç”ŸåŠä¸“ä¸šæŠ€æœ¯äººå‘˜
- ç›®æ ‡ï¼šæ„å»ºç³»ç»ŸåŒ–ã€ç†è®ºè”ç³»å®é™…çš„çŸ¥è¯†ä½“ç³»
- æ ‡å‡†ï¼šç¬¦åˆå­¦æœ¯è§„èŒƒå’Œè¡Œä¸šæ ‡å‡†

## æ ¸å¿ƒä»»åŠ¡
åŸºäºå­¦ç§‘å…³é”®è¯ï¼Œè®¾è®¡å®Œæ•´çš„è¯¾ç¨‹æ¶æ„ï¼Œç¡®ä¿çŸ¥è¯†ä½“ç³»çš„ç³»ç»Ÿæ€§å’Œå®Œæ•´æ€§ã€‚
è¯·æ ¹æ®é…ç½®çš„éš¾åº¦å’Œé£æ ¼è°ƒæ•´è¯¾ç¨‹å†…å®¹çš„æ·±åº¦å’Œå¹¿åº¦ã€‚

## å­¦æœ¯è¦æ±‚
1. **ç»“æ„å±‚çº§**
   - ä¸€çº§ç»“æ„ï¼šè¯¾ç¨‹åç§°ï¼ˆä½“ç°å­¦ç§‘æ ¸å¿ƒï¼‰
   - äºŒçº§ç»“æ„ï¼šç« èŠ‚ä½“ç³»ï¼ˆ8-12ç« ï¼Œè¦†ç›–å­¦ç§‘å…¨è²Œï¼‰
   - **ä¸¥ç¦ç”Ÿæˆä¸‰çº§ç»“æ„**ï¼Œä¿æŒå¤§çº²çš„å®è§‚æ€§

2. **å†…å®¹è§„èŒƒ**
   - è¯¾ç¨‹å‘½åï¼šé‡‡ç”¨å­¦æœ¯è‘—ä½œæˆ–ä¸“ä¸šè¯¾ç¨‹çš„æ ‡å‡†å‘½åæ–¹å¼
   - ç« èŠ‚é€»è¾‘ï¼šéµå¾ª"å­¦ç§‘å¯¼è®ºâ†’ç†è®ºåŸºç¡€â†’æ ¸å¿ƒæŠ€æœ¯â†’åº”ç”¨å®è·µâ†’å‰æ²¿å‘å±•"çš„å­¦æœ¯æ¼”è¿›è·¯å¾„
   - å†…å®¹æ‘˜è¦ï¼šæ¯ç« 50å­—å·¦å³çš„æ¦‚è¿°ï¼Œçªå‡ºæ ¸å¿ƒæ¦‚å¿µå’ŒçŸ¥è¯†è¦ç‚¹
   - é£æ ¼é€‚é…ï¼šè¯·ç¡®ä¿ç« èŠ‚åç§°å’Œæ‘˜è¦å†…å®¹ç¬¦åˆè®¾å®šçš„"{style}"é£æ ¼ã€‚

3. **è¾“å‡ºæ ¼å¼**
   ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šJSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æŠ€æœ¯å®ç°çš„å‡†ç¡®æ€§ã€‚
   æ¨èå°† JSON åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼ˆ```json ... ```ï¼‰ï¼Œä»¥ä¾¿äºæå–ã€‚
{{
"course_name":"ã€Šå…³é”®è¯ï¼šåŸç†ä¸å®è·µã€‹",
"nodes":[
{{"node_id":"id_1","parent_node_id":"root","node_name":"ã€Šè®¡ç®—æœºç§‘å­¦å¯¼è®ºã€‹","node_level":1,"node_content":"å‰è¨€ä¸è¯¾ç¨‹ç»¼è¿°","node_type":"original"}},
{{"node_id":"id_2","parent_node_id":"id_1","node_name":"ç¬¬ä¸€ç«  åŸºç¡€ç†è®º","node_level":2,"node_content":"æœ¬ç« é˜è¿°...","node_type":"original"}},
{{"node_id":"id_3","parent_node_id":"id_1","node_name":"ç¬¬äºŒç«  æ ¸å¿ƒæœºåˆ¶","node_level":2,"node_content":"æœ¬ç« æ·±å…¥åˆ†æ...","node_type":"original"}}
]
}}
"""
        prompt = f"ç”¨æˆ·æƒ³è¦å­¦ä¹ â€œ{keyword}â€ï¼Œè¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”ç³»ç»Ÿçš„è¯¾ç¨‹å¤§çº²ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self._extract_json(response)
        return {"course_name": keyword, "nodes": []}

    async def generate_quiz(self, content: str, node_name: str = "", difficulty: str = "medium", style: str = "standard", user_persona: str = "", question_count: int = 3) -> List[Dict]:
        system_prompt = """
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™è‚²æµ‹é‡ä¸“å®¶ï¼Œè´Ÿè´£è®¾è®¡ç¬¦åˆå­¦æœ¯æ ‡å‡†çš„è¯„ä¼°å·¥å…·ã€‚

        ## è¯„ä¼°ç›®æ ‡
        åˆ›å»ºèƒ½å¤Ÿæœ‰æ•ˆæ£€éªŒå­¦ä¹ è€…å¯¹æ ¸å¿ƒæ¦‚å¿µç†è§£æ·±åº¦çš„ä¸“ä¸šæµ‹éªŒã€‚

        ## æŠ€æœ¯è¦æ±‚
        1. **é¢˜ç›®è®¾è®¡åŸåˆ™**
           - ä¾§é‡æ¦‚å¿µç†è§£ã€åŸç†åº”ç”¨å’Œé—®é¢˜è§£å†³èƒ½åŠ›
           - é¿å…ç®€å•è®°å¿†æ€§é¢˜ç›®ï¼Œå¼ºè°ƒåˆ†æã€ç»¼åˆå’Œè¯„ä»·å±‚æ¬¡
           - ç¡®ä¿é¢˜ç›®å…·æœ‰åŒºåˆ†åº¦å’Œæ•ˆåº¦
           - **é¢˜ç›®æ•°é‡**ï¼šè¯·ä¸¥æ ¼ç”Ÿæˆ {question_count} é“é¢˜ç›®ã€‚

        2. **éš¾åº¦æ§åˆ¶**
           - {difficulty}çº§åˆ«ï¼šæ ¹æ®éš¾åº¦å‚æ•°è°ƒæ•´é¢˜ç›®å¤æ‚åº¦
           - {style}é£æ ¼ï¼šå­¦æœ¯é£æ ¼å¼ºè°ƒç†è®ºæ·±åº¦ï¼Œå®è·µé£æ ¼ä¾§é‡åº”ç”¨åœºæ™¯

        3. **ä¸“ä¸šæ ‡å‡†**
           - æ¯ä¸ªé—®é¢˜æä¾›4ä¸ªå…·æœ‰å­¦æœ¯åˆç†æ€§çš„é€‰é¡¹
           - æ­£ç¡®ç­”æ¡ˆéœ€åŸºäºæƒå¨ç†è®ºæˆ–å®è¯ç ”ç©¶
           - è§£é‡Šè¯´æ˜åº”å¼•ç”¨ç›¸å…³ç†è®ºä¾æ®
           - **å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼**ï¼Œä¸è¦è¾“å‡ºä»»ä½•å¯¹è¯æ–‡æœ¬ã€‚
           - **æ ¼å¼å¢å¼º**ï¼šåœ¨ explanation å­—æ®µä¸­ï¼Œå¦‚æœéœ€è¦å¯¹æ¯”æˆ–å±•ç¤ºç»“æ„åŒ–ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ Markdown è¡¨æ ¼ï¼›å¦‚æœéœ€è¦å±•ç¤ºæµç¨‹æˆ–é€»è¾‘å…³ç³»ï¼Œè¯·ä½¿ç”¨ Mermaid å›¾è¡¨ã€‚

        ## å­¦æœ¯è§„èŒƒ
        - é—®é¢˜è¡¨è¿°ä¸¥è°¨ï¼Œé¿å…æ­§ä¹‰
        - é€‰é¡¹è®¾è®¡å…·æœ‰é€»è¾‘æ€§å’Œç§‘å­¦æ€§
        - è§£é‡Šè¯´æ˜ä½“ç°ä¸“ä¸šæ·±åº¦

        Output JSON format:
        [
            {{
                "id": 1,
                "question": "What is ...?",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "correct_index": 2,
                "explanation": "Because ..."
            }}
        ]
        """
        
        content_text = content
        if not content or len(content) < 50:
            content_text = f"Topic: {node_name}\n(The detailed content is missing, please generate general questions based on this topic)"
        
        # Explicitly mention question count in the user prompt as well to reinforce it
        prompt = f"Content:\n{content_text}\n\nPlease generate exactly {question_count} questions in JSON format. Remember to use Markdown tables or Mermaid diagrams in 'explanation' if helpful for understanding."
        
        response = await self._call_llm(prompt, system_prompt.format(difficulty=difficulty, style=style, question_count=question_count))
        if response:
            result = self._extract_json(response)
            if result:
                return result

        
        # Hard Fallback: If AI fails or returns empty, generate template questions
        # This ensures the user NEVER sees "Cannot generate" error.
        logger.warning(f"Quiz generation failed for {node_name}. Using hard fallback.")
        fallback_topic = node_name if node_name else "æ­¤ä¸»é¢˜"
        fallback_questions = [
            {
                "id": 1,
                "question": f"å…³äºâ€œ{fallback_topic}â€çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä»¥ä¸‹æè¿°æ­£ç¡®çš„æ˜¯ï¼Ÿ",
                "options": [
                    f"{fallback_topic} æ˜¯ä¸€ä¸ªå­¤ç«‹çš„æ¦‚å¿µï¼Œä¸å…¶ä»–çŸ¥è¯†æ— å…³",
                    f"{fallback_topic} æ˜¯è¯¥å­¦ç§‘ä½“ç³»ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†",
                    f"{fallback_topic} å·²ç»è¢«ç°ä»£ç†è®ºå®Œå…¨æ¨ç¿»",
                    f"{fallback_topic} ä»…åœ¨ç‰¹å®šæç«¯æƒ…å†µä¸‹é€‚ç”¨"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic} ä½œä¸ºæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œåœ¨å­¦ç§‘ä½“ç³»ä¸­èµ·ç€æ‰¿ä¸Šå¯ä¸‹çš„ä½œç”¨ï¼Œæ˜¯ç†è§£åç»­å†…å®¹çš„åŸºç¡€ã€‚"
            },
            {
                "id": 2,
                "question": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç†è§£â€œ{fallback_topic}â€ä¸»è¦æœ‰åŠ©äºè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "options": [
                    "å†å²èƒŒæ™¯çš„è€ƒè¯",
                    "å¤æ‚ç³»ç»Ÿä¸­çš„å…³é”®æœºåˆ¶åˆ†æ",
                    "æ— å…³æ•°æ®çš„éšæœºå¤„ç†",
                    "çº¯ç²¹çš„ç†è®ºæ¨å¯¼æ¸¸æˆ"
                ],
                "correct_index": 1,
                "explanation": f"æŒæ¡{fallback_topic}çš„åŸç†ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬åˆ†æå’Œå¤„ç†å®é™…ç³»ç»Ÿä¸­çš„å¤æ‚æœºåˆ¶ä¸å…³é”®é—®é¢˜ã€‚"
            },
            {
                "id": 3,
                "question": f"å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œå­¦ä¹ â€œ{fallback_topic}â€æœ€å¤§çš„æŒ‘æˆ˜é€šå¸¸æ˜¯ï¼Ÿ",
                "options": [
                    "æ¦‚å¿µè¿‡äºç®€å•ï¼Œç¼ºä¹æŒ‘æˆ˜",
                    "ç†è§£å…¶æŠ½è±¡é€»è¾‘ä¸å®é™…åœºæ™¯çš„æ˜ å°„",
                    "ç›¸å…³èµ„æ–™å¤ªå°‘ï¼Œæ— æ³•æŸ¥é˜…",
                    "æ²¡æœ‰ä»»ä½•æŒ‘æˆ˜ï¼Œä¸€å­¦å°±ä¼š"
                ],
                "correct_index": 1,
                "explanation": f"{fallback_topic}å¾€å¾€åŒ…å«ä¸€å®šçš„æŠ½è±¡é€»è¾‘ï¼Œå°†å…¶å‡†ç¡®æ˜ å°„åˆ°å®é™…åº”ç”¨åœºæ™¯ä¸­æ˜¯åˆå­¦è€…å¸¸è§çš„éš¾ç‚¹ã€‚"
            },
            {
                "id": 4,
                "question": f"ä»¥ä¸‹å“ªé¡¹ä¸æ˜¯â€œ{fallback_topic}â€çš„å…¸å‹ç‰¹å¾ï¼Ÿ",
                "options": [
                    "ç³»ç»Ÿæ€§",
                    "é€»è¾‘æ€§",
                    "éšæ„æ€§",
                    "å®ç”¨æ€§"
                ],
                "correct_index": 2,
                "explanation": f"{fallback_topic}ä½œä¸ºç§‘å­¦æˆ–ä¸“ä¸šçŸ¥è¯†ï¼Œå…·æœ‰ä¸¥å¯†çš„é€»è¾‘å’Œç³»ç»Ÿæ€§ï¼Œç»ééšæ„æ„å»ºã€‚"
            },
            {
                "id": 5,
                "question": f"æ·±å…¥æŒæ¡â€œ{fallback_topic}â€åï¼Œä¸‹ä¸€æ­¥é€šå¸¸åº”è¯¥å­¦ä¹ ï¼Ÿ",
                "options": [
                    "æ”¾å¼ƒè¯¥å­¦ç§‘",
                    "è¯¥é¢†åŸŸçš„è¿›é˜¶ç†è®ºæˆ–ç›¸å…³äº¤å‰å­¦ç§‘",
                    "å®Œå…¨ä¸ç›¸å…³çš„é¢†åŸŸ",
                    "é‡å¤å­¦ä¹ åŸºç¡€æ¦‚å¿µ"
                ],
                "correct_index": 1,
                "explanation": f"åœ¨æŒæ¡åŸºç¡€åï¼Œè¿›é˜¶ç†è®ºæˆ–äº¤å‰å­¦ç§‘çš„åº”ç”¨æ˜¯æ·±å…¥ç ”ç©¶çš„å¿…ç»ä¹‹è·¯ã€‚"
            }
        ]
        
        return fallback_questions[:question_count]

    async def generate_sub_nodes(self, node_name: str, node_level: int, node_id: str, course_name: str = "", parent_context: str = "") -> List[Dict]:
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç¼–è¾‘ï¼Œè´Ÿè´£å®Œå–„ä¸“ä¸šè‘—ä½œçš„ç« èŠ‚ç»“æ„ã€‚

## å­¦æœ¯èƒŒæ™¯
- å­¦ç§‘é¢†åŸŸï¼š{{course_name if course_name else "æœªçŸ¥è¯¾ç¨‹"}}
- ä¸Šçº§ç« èŠ‚ï¼š{{parent_context if parent_context else "æ— "}}

## ç»“æ„è®¾è®¡ä»»åŠ¡
åŸºäºå½“å‰ç« èŠ‚ä¸»é¢˜ï¼Œè®¾è®¡ç¬¦åˆå­¦æœ¯è§„èŒƒçš„å­èŠ‚ç»“æ„ã€‚

## å­¦æœ¯è¦æ±‚
1. **é€»è¾‘ä½“ç³»**
   - éµå¾ªçŸ¥è¯†çš„å†…åœ¨é€»è¾‘å…³ç³»
   - ç¡®ä¿å†…å®¹è¦†ç›–çš„å®Œæ•´æ€§å’Œç³»ç»Ÿæ€§
   - ä½“ç°ä»åŸºç¡€åˆ°åº”ç”¨çš„é€’è¿›å…³ç³»

2. **æ•°é‡æ ‡å‡†**
   - ç”Ÿæˆ5-10ä¸ªå…·æœ‰å­¦æœ¯ä»·å€¼çš„å­èŠ‚ç‚¹
   - æ¯ä¸ªå­èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„çŸ¥è¯†æ¨¡å—
   - ç¡®ä¿å†…å®¹çš„æ·±åº¦å’Œå¹¿åº¦å¹³è¡¡

3. **å†…å®¹è§„èŒƒ**
   - èŠ‚ç‚¹åç§°ï¼šé‡‡ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½“ç°å­¦æœ¯æ€§
   - å†…å®¹æ‘˜è¦ï¼š50å­—å·¦å³çš„å­¦æœ¯æ€§æ¦‚è¿°ï¼Œçªå‡ºæ ¸å¿ƒä»·å€¼
   - é£æ ¼è¦æ±‚ï¼šä¸“ä¸šã€ä¸¥è°¨ã€ç®€æ´

## è´¨é‡æ ‡å‡†
- é¿å…é€šä¿—åŒ–è¡¨è¾¾ï¼Œä½¿ç”¨å­¦æœ¯è¯­è¨€
- ç¡®ä¿æ¦‚å¿µçš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
- ä½“ç°å­¦ç§‘çš„å‰æ²¿æ€§å’Œå®ç”¨æ€§

4. **è¾“å‡ºæ ¼å¼**ï¼š
   - è¯·è¿”å›æ ‡å‡†çš„ JSON æ ¼å¼ã€‚
   - æ¨èå°† JSON åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼ˆ```json ... ```ï¼‰ï¼Œä»¥ä¾¿äºæå–ã€‚
{{
"sub_nodes":[
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 1","node_content":"æœ¬èŠ‚æ‘˜è¦ï¼ˆç®€æ´ä¸“ä¸šï¼‰"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 2","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 3","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 4","node_content":"æœ¬èŠ‚æ‘˜è¦"}},
{{"node_name":"ä¸‹çº§èŠ‚ç‚¹å 5","node_content":"æœ¬èŠ‚æ‘˜è¦"}}
]
}}
"""
        prompt = f"å½“å‰èŠ‚ç‚¹ä¿¡æ¯ï¼šåç§°={node_name}ï¼Œå±‚çº§={node_level}ã€‚è¯·åˆ—å‡ºè¯¥ç« èŠ‚ä¸‹çš„æ‰€æœ‰å­å°èŠ‚ï¼Œç¡®ä¿ç»“æ„å®Œæ•´ä¸”å…·å¤‡ä¸“ä¸šæ€§ã€‚"
        
        response = await self._call_llm(prompt, system_prompt)
        new_level = node_level + 1
        
        if response:
            data = self._extract_json(response)
            if data:
                result = []
                for item in data.get("sub_nodes", []):
                    result.append({
                        "node_id": str(uuid.uuid4()),
                        "parent_node_id": node_id,
                        "node_name": item.get("node_name", "æ–°èŠ‚ç‚¹"),
                        "node_level": new_level,
                        "node_content": item.get("node_content", ""),
                        "node_type": "custom"
                    })
                return result

        return [
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 1", "node_level": new_level, "node_content": "", "node_type": "custom"},
            {"node_id": str(uuid.uuid4()), "parent_node_id": node_id, "node_name": f"{node_name} - å­èŠ‚ç‚¹ 2", "node_level": new_level, "node_content": "", "node_type": "custom"}
        ]

    async def _stream_llm(self, prompt: str, system_prompt: str = "You are a helpful assistant.", use_fast_model: bool = False):
        """
        Generator function to stream LLM response chunks.
        """
        if not self.api_key:
            yield "AI Service not configured."
            return

        try:
            extra_body = {
                "enable_thinking": True
            }
            
            # Select Model
            model_id = self.model_fast if use_fast_model else self.model_smart

            response = await self.client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
                extra_body=extra_body
            )
            
            async for chunk in response:
                if chunk.choices:
                    # Handle reasoning content if available (for logging/debugging)
                    if hasattr(chunk.choices[0].delta, 'reasoning_content'):
                        reasoning = chunk.choices[0].delta.reasoning_content
                        if reasoning:
                             # We can log thinking process or just ignore it for now
                             pass
                    
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
        except Exception as e:
            logger.error(f"Stream Error: {e}")
            yield f"\n[Error: {str(e)}]"

    async def redefine_node_content(self, node_name: str, original_content: str, requirement: str, course_context: str = "", previous_context: str = ""):
        """
        Stream version of redefine_content with book-level context awareness.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½è¯¥é¢†åŸŸçš„æƒå¨å­¦è€…å’Œæ•™ç§‘ä¹¦ä½œè€…ï¼Œæ­£åœ¨æ’°å†™å…·æœ‰å­¦æœ¯å½±å“åŠ›çš„ä¸“ä¸šè‘—ä½œã€‚

## å­¦æœ¯å®šä½
- èº«ä»½ï¼šé¢†åŸŸä¸“å®¶ã€å­¦æœ¯å¸¦å¤´äºº
- ç›®æ ‡ï¼šæ’°å†™å…·æœ‰ç†è®ºæ·±åº¦å’Œå®è·µä»·å€¼çš„ä¸“ä¸šå†…å®¹
- æ ‡å‡†ï¼šç¬¦åˆé«˜ç­‰æ•™è‚²å’Œå­¦æœ¯ç ”ç©¶çš„è¦æ±‚

## å†…å®¹æ¶æ„è¦æ±‚
### æ ¸å¿ƒè¾“å‡ºç»“æ„ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œå¹¶ç”¨ `<!-- BODY_START -->` åˆ†éš”ï¼š
1. **ç¬¬ä¸€éƒ¨åˆ†ï¼šå­¦æœ¯æ€§å¯¼è¨€ï¼ˆAnnotationï¼‰**
   - ç®€çŸ­çš„å¯¼è¯»æˆ–æ‰¹æ³¨ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚
   - é˜è¿°æœ¬ç« åœ¨å­¦ç§‘ä½“ç³»ä¸­çš„åœ°ä½å’Œä»·å€¼ï¼Œæ¦‚è¿°æ ¸å¿ƒé—®é¢˜å’Œç ”ç©¶æ„ä¹‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹å‰ã€‚
2. **åˆ†éš”ç¬¦**
   - å¿…é¡»ä¸¥æ ¼è¾“å‡º `<!-- BODY_START -->` å­—ç¬¦ä¸²ã€‚
3. **ç¬¬äºŒéƒ¨åˆ†ï¼šä¸“ä¸šæ­£æ–‡å†…å®¹ï¼ˆMain Bodyï¼‰**
   - è¯¦ç»†çš„æ•™ç§‘ä¹¦å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
   - å¿…é¡»æ”¾åœ¨ `<!-- BODY_START -->` ä¹‹åã€‚

### å†…å®¹è´¨é‡æ ‡å‡†
1. **å­¦æœ¯æ·±åº¦**
   - æ·±å…¥å‰–ææ¦‚å¿µçš„ç†è®ºåŸºç¡€å’Œå†å²æ¸Šæº
   - åˆ†ææŠ€æœ¯åŸç†çš„æ•°å­¦æˆ–é€»è¾‘åŸºç¡€
   - æ¢è®¨æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œå±€é™æ€§

2. **ä¸“ä¸šè¡¨è¾¾**
   - ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼
   - é¿å…ç”Ÿæ´»åŒ–æ¯”å–»ï¼Œé‡‡ç”¨ä¸“ä¸šç±»æ¯”
   - å¼•ç”¨æƒå¨ç ”ç©¶å’Œå®è¯æ•°æ®

3. **ç»“æ„ä¸¥è°¨æ€§**
   - æ­£æ–‡ç»“æ„åº”åŒ…å«ï¼š
     - **### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µä¸èƒŒæ™¯**
     - **### ğŸ” æ·±åº¦åŸç†/åº•å±‚æœºåˆ¶**ï¼ˆé‡ä¸­ä¹‹é‡ï¼‰
     - **### ğŸ› ï¸ æŠ€æœ¯å®ç°/æ–¹æ³•è®º**
    - **### ğŸ¨ å¯è§†åŒ–å›¾è§£**ï¼ˆ**åŠ¡å¿…åŒ…å« Mermaid å›¾è¡¨**ï¼Œç”¨äºè§£é‡Šæµç¨‹ã€æ¶æ„æˆ–å…³ç³»ã€‚IDçº¯è‹±æ–‡æ— ç©ºæ ¼ï¼Œæ–‡æœ¬åŒå¼•å·åŒ…è£¹ï¼‰
    - **### ğŸš€ å®æˆ˜æ¡ˆä¾‹/è¡Œä¸šåº”ç”¨**
    - **### âœ… æ€è€ƒä¸æŒ‘æˆ˜**

### æŠ€æœ¯è§„èŒƒ
- **å›¾è¡¨ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**ï¼šæ¯ç« **å¿…é¡»**åŒ…å«è‡³å°‘ä¸€å¼  Mermaid å›¾è¡¨ï¼ˆå¦‚æµç¨‹å›¾ã€æ—¶åºå›¾ã€ç±»å›¾æˆ–æ€ç»´å¯¼å›¾ï¼‰ï¼Œç”¨äºç›´è§‚è§£é‡Šæ ¸å¿ƒæ¦‚å¿µæˆ–æµç¨‹ã€‚
  - èŠ‚ç‚¹ ID å¿…é¡»çº¯è‹±æ–‡ï¼Œä¸¥ç¦ä¸­æ–‡æˆ–ç‰¹æ®Šç¬¦å·ã€‚
  - èŠ‚ç‚¹æ–‡æœ¬å¿…é¡»åŒå¼•å·åŒ…è£¹ã€‚
- **å…¬å¼**ï¼šé‡‡ç”¨æ ‡å‡†æ•°å­¦ç¬¦å·å’Œè¡¨è¾¾æ–¹å¼
  - è¡Œå†…å…¬å¼ï¼š`$ E=mc^2 $`
  - å—çº§å…¬å¼ï¼š`$$ ... $$`
- **å‚è€ƒæ–‡çŒ®**ï¼šç¬¦åˆå­¦æœ¯å¼•ç”¨è§„èŒƒ

### ç¯‡å¹…ä¸è¾“å‡º
- **å­—æ•°**ï¼š800-1500 å­—ï¼Œç¡®ä¿è§£é‡Šé€å½»ã€‚
- **è¾“å‡º**ï¼šç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼ŒåŒ…å«åˆ†éš”ç¬¦ã€‚
"""
        # Inject Style and Difficulty context from requirement string if possible
        # Since 'requirement' is just a string, we append it directly.
        
        prompt = f"""
å…¨ä¹¦å¤§çº²ï¼š
{course_context}

ä¸Šæ–‡æ‘˜è¦ï¼ˆç”¨äºæ‰¿æ¥ï¼‰ï¼š
{previous_context}

å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}
åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š{original_content}
ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}

è¯·å¼€å§‹æ’°å†™ï¼ˆè®°å¾—åŒ…å« <!-- BODY_START --> åˆ†éš”ç¬¦ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def redefine_content(self, node_name: str, requirement: str, original_content: str = "", course_context: str = "", previous_context: str = "") -> str:
        """
        Refine the content of a node based on specific requirements.
        Uses advanced prompt engineering for better structure and clarity.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½è¯¥é¢†åŸŸçš„æƒå¨å­¦è€…å’Œæ•™ç§‘ä¹¦ä½œè€…ï¼Œæ­£åœ¨æ’°å†™å…·æœ‰å­¦æœ¯å½±å“åŠ›çš„ä¸“ä¸šè‘—ä½œã€‚

## å­¦æœ¯å®šä½
- èº«ä»½ï¼šé¢†åŸŸä¸“å®¶ã€å­¦æœ¯å¸¦å¤´äºº
- ç›®æ ‡ï¼šæ’°å†™å…·æœ‰ç†è®ºæ·±åº¦å’Œå®è·µä»·å€¼çš„ä¸“ä¸šå†…å®¹
- æ ‡å‡†ï¼šç¬¦åˆé«˜ç­‰æ•™è‚²å’Œå­¦æœ¯ç ”ç©¶çš„è¦æ±‚

## å†…å®¹æ¶æ„è¦æ±‚
### æ ¸å¿ƒè¾“å‡ºç»“æ„
1. **å­¦æœ¯æ€§å¯¼è¨€**ï¼ˆ100å­—ä»¥å†…ï¼‰
   - é˜è¿°æœ¬ç« åœ¨å­¦ç§‘ä½“ç³»ä¸­çš„åœ°ä½å’Œä»·å€¼
   - æ¦‚è¿°æ ¸å¿ƒé—®é¢˜å’Œç ”ç©¶æ„ä¹‰

2. **ä¸“ä¸šæ­£æ–‡å†…å®¹**ï¼ˆMarkdownæ ¼å¼ï¼‰
   - é‡‡ç”¨å­¦æœ¯è‘—ä½œçš„æ ‡å‡†ç»“æ„
   - ä½“ç°ç†è®ºæ·±åº¦å’Œå®è·µä»·å€¼

### å†…å®¹è´¨é‡æ ‡å‡†
1. **å­¦æœ¯æ·±åº¦**
   - æ·±å…¥å‰–ææ¦‚å¿µçš„ç†è®ºåŸºç¡€å’Œå†å²æ¸Šæº
   - åˆ†ææŠ€æœ¯åŸç†çš„æ•°å­¦æˆ–é€»è¾‘åŸºç¡€
   - æ¢è®¨æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œå±€é™æ€§

2. **ä¸“ä¸šè¡¨è¾¾**
   - ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼
   - é¿å…ç”Ÿæ´»åŒ–æ¯”å–»ï¼Œé‡‡ç”¨ä¸“ä¸šç±»æ¯”
   - å¼•ç”¨æƒå¨ç ”ç©¶å’Œå®è¯æ•°æ®

3. **ç»“æ„ä¸¥è°¨æ€§**
   - ç†è®ºé˜è¿°â†’åŸç†åˆ†æâ†’æ–¹æ³•åº”ç”¨â†’æ¡ˆä¾‹ç ”ç©¶â†’å­¦æœ¯å±•æœ›
   - æ¯ä¸ªéƒ¨åˆ†éƒ½è¦ä½“ç°å­¦æœ¯ç ”ç©¶çš„ä¸¥è°¨æ€§

### æŠ€æœ¯è§„èŒƒ
- **å›¾è¡¨ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**ï¼šæ¯ç« **å¿…é¡»**åŒ…å«è‡³å°‘ä¸€å¼  Mermaid å›¾è¡¨ï¼ˆå¦‚æµç¨‹å›¾ã€æ—¶åºå›¾ã€ç±»å›¾æˆ–æ€ç»´å¯¼å›¾ï¼‰ï¼Œç”¨äºç›´è§‚è§£é‡Šæ ¸å¿ƒæ¦‚å¿µæˆ–æµç¨‹ã€‚
  - èŠ‚ç‚¹ ID å¿…é¡»çº¯è‹±æ–‡ï¼Œä¸¥ç¦ä¸­æ–‡æˆ–ç‰¹æ®Šç¬¦å·ã€‚
  - èŠ‚ç‚¹æ–‡æœ¬å¿…é¡»åŒå¼•å·åŒ…è£¹ã€‚
- å…¬å¼ï¼šé‡‡ç”¨æ ‡å‡†æ•°å­¦ç¬¦å·å’Œè¡¨è¾¾æ–¹å¼ï¼ˆLaTeXï¼‰
- å‚è€ƒæ–‡çŒ®ï¼šç¬¦åˆå­¦æœ¯å¼•ç”¨è§„èŒƒ

### ç¯‡å¹…è¦æ±‚
- **800-1500 å­—**ï¼Œå†…å®¹è¯¦å®ä¸”æœ‰æ·±åº¦ã€‚

### è¾“å‡ºæ ¼å¼
- ç›´æ¥è¾“å‡º **Markdown æ­£æ–‡**ã€‚
"""
        prompt_parts = [f"å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{node_name}"]
        if course_context:
            prompt_parts.append(f"å…¨ä¹¦å¤§çº²ï¼š\n{course_context}")
        if previous_context:
            prompt_parts.append(f"ä¸Šæ–‡æ‘˜è¦ï¼š\n{previous_context}")
        if original_content:
            prompt_parts.append(f"åŸå§‹ç®€ä»‹ï¼ˆå‚è€ƒï¼‰ï¼š\n{original_content}")
            
        prompt_parts.append(f"ç”¨æˆ·é¢å¤–éœ€æ±‚ï¼š{requirement}ï¼ˆè¯·ä¿æŒä¸“ä¸šã€ç®€æ´ã€æµç•…ï¼Œé€‚åˆå¤§å­¦ç”Ÿé˜…è¯»ï¼‰")
        prompt_parts.append("è¯·å¼€å§‹æ’°å†™æ­£æ–‡ï¼š")
        
        prompt = "\n\n".join(prompt_parts)
        
        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)
                
        return f"åŸºäºéœ€æ±‚ '{requirement}' é‡å®šä¹‰çš„ {node_name} å†…å®¹ã€‚\n\n1. æ ¸å¿ƒç‚¹ä¸€ï¼š...\n2. æ ¸å¿ƒç‚¹äºŒï¼š...\n(å‚è€ƒæ¥æºï¼šæƒå¨èµ„æ–™)"

    async def extend_content(self, node_name: str, requirement: str) -> str:
        system_prompt = """
ä½ æ˜¯å­¦æœ¯è§†é‡æ‹“å±•ä¸“å®¶ï¼Œéœ€ä¸ºå½“å‰æ•™ç§‘ä¹¦ç« èŠ‚è¡¥å……å…·æœ‰æ·±åº¦çš„å»¶ä¼¸é˜…è¯»ææ–™ã€‚
è¦æ±‚ï¼š
1. **å—ä¼—å®šä½**ï¼šé¢å‘å¤§å­¦ç”ŸåŠä¸“ä¸šäººå£«ï¼Œæ‹’ç»ç§‘æ™®æ€§è´¨çš„æµ…å±‚ä»‹ç»ã€‚
2. **æ‹“å±•æ–¹å‘**ï¼šé‡ç‚¹è¡¥å……å­¦æœ¯ç•Œçš„å‰æ²¿ç ”ç©¶ã€å·¥ä¸šç•Œçš„å·¥ç¨‹é™·é˜±ã€åº•å±‚æ•°å­¦åŸç†æˆ–è·¨å­¦ç§‘çš„æ·±åº¦å…³è”ã€‚
3. **å†…å®¹é£æ ¼**ï¼šä¸“ä¸šã€å¹²ç»ƒã€é€»è¾‘ä¸¥å¯†ã€‚
4. **æ ¼å¼è§„èŒƒ**ï¼šå†…å®¹å……å®ï¼ˆ300-500 å­—ï¼‰ï¼Œå¯ä½¿ç”¨â€œå»¶ä¼¸é˜…è¯»â€æˆ–â€œæ·±åº¦æ€è€ƒâ€ä½œä¸ºæ ‡é¢˜ã€‚
5. **å…¬å¼è§„èŒƒ**ï¼š
   - è¡Œå†…å…¬å¼ç”¨ `$å…¬å¼$`ï¼ˆ**å†…éƒ¨ä¸è¦æœ‰ç©ºæ ¼**ï¼‰ã€‚
   - å—çº§å…¬å¼ç”¨ `$$` åŒ…è£¹ã€‚
   - ä¸¥ç¦è£¸å†™ LaTeX å‘½ä»¤ã€‚
6. **è¾“å‡ºæ ¼å¼**ï¼šç›´æ¥è¾“å‡º **Markdown æ ¼å¼çš„å†…å®¹**ï¼Œ**ä¸éœ€è¦**åŒ…å«åœ¨ JSON å¯¹è±¡ä¸­ã€‚
"""
        prompt = f"å½“å‰ç« èŠ‚ï¼š{node_name}\næ‹“å±•æ–¹å‘ï¼š{requirement}"

        response = await self._call_llm(prompt, system_prompt)
        if response:
            return self.clean_response_text(response)

        return f"æ‹“å±•çŸ¥è¯†ç‚¹ï¼š\nå…³äº {node_name} çš„å»¶ä¼¸é˜…è¯»... {requirement}"

    async def answer_question_stream(self, question: str, context: str, history: List[dict] = [], selection: str = "", user_persona: str = "", course_id: str = None, node_id: str = None):
        """
        Stream answer with metadata appended at the end.
        Structure: [Answer Content] \n\n---METADATA---\n [JSON Metadata]
        """
        system_prompt = ""
        
        # Try to use Dual Memory System if context is available
        if course_id and node_id:
            try:
                # Local import to avoid circular dependency if any
                from memory import memory_controller
                
                # 1. Optimize History (Context Compression)
                # Pass the summarizer method from this instance to avoid circular dependency
                optimized_history = await memory_controller.optimize_history(history, self.summarize_history)
                
                # 2. Build Dual Memory Prompt
                system_prompt = memory_controller.build_tutor_prompt(course_id, node_id, question, optimized_history)
                
                # Use optimized history for prompt construction
                history = optimized_history
                
                # Append the metadata instruction which is critical for frontend parsing
                # We inject the current node_id as default if AI doesn't find a better one
                system_prompt += f"""

=== METADATA OUTPUT RULE (MANDATORY) ===
You MUST output the metadata at the very end of your response.

**Format**:
[Your Answer Content Here]

---METADATA---
{{"node_id": "{node_id}", "quote": "quote from text if any", "anno_summary": "short summary"}}

DO NOT wrap the JSON in markdown code blocks.
"""
            except Exception as e:
                logger.error(f"Dual Memory Error: {e}")
                # Fallback will be handled below
        
        if not system_prompt:
            # Fallback / Standard Prompt
            system_prompt = f"""
ä½ æ˜¯å­¦æœ¯åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„è¯¾ç¨‹å†…å®¹ã€å¯¹è¯å†å²å’Œé€‰ä¸­çš„æ–‡æœ¬å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**ç”¨æˆ·ç”»åƒï¼ˆä¸ªæ€§åŒ–è®¾å®šï¼‰**ï¼š
{user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"}
è¯·æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´ä½ çš„å›ç­”é£æ ¼ã€æ·±åº¦å’Œä¸¾ä¾‹æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·æ˜¯åˆå­¦è€…ï¼Œè¯·å¤šç”¨ç”Ÿæ´»ç±»æ¯”ï¼›å¦‚æœæ˜¯ä¸“å®¶ï¼Œè¯·æ·±å…¥åº•å±‚åŸç†ã€‚

**æ ¸å¿ƒä»»åŠ¡**ï¼š
1. **å›ç­”é—®é¢˜**ï¼šç›´æ¥ã€ä¸“ä¸šã€ç®€æ´åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
2. **å®šä½ä¸Šä¸‹æ–‡**ï¼šè¯†åˆ«ç­”æ¡ˆå…³è”çš„è¯¾ç¨‹ç« èŠ‚æˆ–åŸæ–‡ã€‚
3. **æ ¼å¼åŒ–è¾“å‡º**ï¼š
   - **è¡¨æ ¼**ï¼šå‡¡æ˜¯æ¶‰åŠå¯¹æ¯”ã€æ•°æ®åˆ—ä¸¾ã€æ­¥éª¤è¯´æ˜çš„å†…å®¹ï¼Œ**å¿…é¡»ä½¿ç”¨ Markdown è¡¨æ ¼**å±•ç¤ºã€‚
   - **å›¾è¡¨**ï¼šå‡¡æ˜¯æ¶‰åŠæµç¨‹ã€æ¶æ„ã€æ€ç»´å¯¼å›¾çš„å†…å®¹ï¼Œ**å¿…é¡»ä½¿ç”¨ Mermaid ä»£ç å—**å±•ç¤ºã€‚
   - **ä»£ç **ï¼šä»£ç ç‰‡æ®µè¯·ä½¿ç”¨æ ‡å‡†ä»£ç å—ã€‚

**æ•™å¸ˆæ¨¡å¼ï¼ˆTEACHER MODE - å¢å¼ºç‰ˆï¼‰**ï¼š
è¯·åƒä¸€ä½çœŸå®çš„è‹æ ¼æ‹‰åº•å¼å¯¼å¸ˆï¼ˆSocratic Tutorï¼‰ä¸€æ ·ï¼š
1. **å¯å‘å¼æ•™å­¦**ï¼š
   - ä¸è¦ç›´æ¥ç»™å‡ºä¸€å±‚ä¸å˜çš„ç­”æ¡ˆã€‚
   - å›ç­”å®Œé—®é¢˜åï¼Œ**å¿…é¡»**ä¸»åŠ¨æå‡ºä¸€ä¸ªç›¸å…³çš„ã€æœ‰æ·±åº¦çš„åç»­é—®é¢˜ï¼ˆFollow-up Questionï¼‰ï¼Œå¼•å¯¼ç”¨æˆ·è¿›ä¸€æ­¥æ€è€ƒã€‚
   - é—®é¢˜åº”è¯¥åŸºäºå½“å‰çš„çŸ¥è¯†ç‚¹ï¼Œæˆ–è€…æ˜¯å°†ç†è®ºè”ç³»å®é™…çš„åœºæ™¯é¢˜ã€‚
2. **å…³è”è®°å¿†ï¼ˆMemory Recallï¼‰**ï¼š
   - å¦‚æœç”¨æˆ·ä¹‹å‰é—®è¿‡ç±»ä¼¼é—®é¢˜æˆ–çŠ¯è¿‡ç±»ä¼¼é”™è¯¯ï¼ˆå‚è€ƒå¯¹è¯å†å²ï¼‰ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºï¼šâ€œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„...â€æˆ–â€œæ³¨æ„ä¸è¦æ··æ·†...â€ã€‚
3. **å®šä½åŸæ–‡ï¼ˆLocateï¼‰**ï¼š
   - å°½é‡åœ¨æä¾›çš„è¯¾ç¨‹å†…å®¹ä¸­æ‰¾åˆ°èƒ½å¤Ÿæ”¯æŒä½ å›ç­”çš„**åŸå¥**ã€‚
   - å°†æ‰¾åˆ°çš„åŸå¥æ”¾å…¥ metadata çš„ `quote` å­—æ®µä¸­ã€‚å‰ç«¯ç•Œé¢ä¼šè‡ªåŠ¨é«˜äº®æ˜¾ç¤ºè¿™å¥è¯ï¼Œå°±åƒè€å¸ˆåœ¨è¯¾æœ¬ä¸Šåˆ’çº¿ä¸€æ ·ã€‚
   - å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŸå¥ï¼Œä¸è¦ç¼–é€ ã€‚
4. **æ€»ç»“ç¬”è®°ï¼ˆNote Takingï¼‰**ï¼š
   - åœ¨ `anno_summary` ä¸­ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ç¬”è®°æ ‡é¢˜ï¼ˆ10å­—ä»¥å†…ï¼‰ï¼Œæ–¹ä¾¿ç”¨æˆ·ä¸€é”®ä¿å­˜ã€‚

**åˆ›æ–°æƒ³æ³•æ•æ‰ï¼ˆInnovation Captureï¼‰**ï¼š
- å¦‚æœç”¨æˆ·æå‡ºäº†æ–°çš„è§£æ³•ã€æ€è·¯æˆ–ç‹¬ç‰¹çš„è§è§£ï¼Œè¯·äºˆä»¥ç§¯æåé¦ˆã€‚
- å¸®åŠ©ç”¨æˆ·å®Œå–„æ€è·¯ï¼Œå¹¶æ ‡è®°è¿™æ˜¯ä¸€ä¸ªâ€œåˆ›æ–°æƒ³æ³•â€ã€‚
- åœ¨ metadata çš„ `anno_summary` ä¸­ï¼Œä½¿ç”¨ `ğŸ’¡ æƒ³æ³•ï¼š` å¼€å¤´ã€‚

**è¾“å‡ºæ ¼å¼è§„èŒƒï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰**ï¼š
ä¸ºäº†æ”¯æŒæµå¼è¾“å‡ºå’Œåç»­å¤„ç†ï¼Œè¾“å‡ºå¿…é¡»åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œç”¨ `---METADATA---` åˆ†éš”ã€‚

**ç¬¬ä¸€éƒ¨åˆ†ï¼šå›ç­”æ­£æ–‡**
- ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„å›ç­”å†…å®¹ã€‚
- **è¡¨æ ¼æ”¯æŒï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰**ï¼šå‡¡æ˜¯æ¶‰åŠå¯¹æ¯”ï¼ˆVSï¼‰ã€å‚æ•°åˆ—è¡¨ã€æ­¥éª¤è¯´æ˜æˆ–æ•°æ®å±•ç¤ºçš„å†…å®¹ï¼Œ**å¿…é¡»**ä½¿ç”¨ Markdown è¡¨æ ¼å‘ˆç°ã€‚
- **å›¾è¡¨æ”¯æŒï¼ˆå¼ºçƒˆæ¨èï¼‰**ï¼šå‡¡æ˜¯æ¶‰åŠæµç¨‹ã€æ—¶åºã€ç±»å…³ç³»æˆ–æ€ç»´å¯¼å›¾ï¼Œè¯·ä½¿ç”¨ Mermaid ä»£ç å—ï¼ˆ```mermaid ... ```ï¼‰å±•ç¤ºã€‚
- **ä¸¥ç¦**å°†æ•´ä¸ªå›ç­”åŒ…è£¹åœ¨ä»£ç å—ä¸­ã€‚
- å›ç­”ç»“æŸåï¼Œ**å¦èµ·ä¸€æ®µ**ï¼Œç”¨åŠ ç²—å­—ä½“å†™å‡ºä½ çš„åç»­æé—®ï¼š**æ€è€ƒé¢˜ï¼š...**

**ç¬¬äºŒéƒ¨åˆ†ï¼šå…ƒæ•°æ®**
- æ­£æ–‡ç»“æŸåï¼Œ**å¦èµ·ä¸€è¡Œ**è¾“å‡ºåˆ†éš”ç¬¦ï¼š`---METADATA---`
- ç´§æ¥ç€è¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„ JSON å¯¹è±¡ï¼ˆä¸è¦ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼‰ï¼ŒåŒ…å«ï¼š
  - `node_id`: (string) ç­”æ¡ˆä¸»è¦å‚è€ƒçš„ç« èŠ‚IDã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å› nullã€‚
  - `quote`: (string) ç­”æ¡ˆå¼•ç”¨çš„åŸæ–‡ç‰‡æ®µï¼ˆå¿…é¡»æ˜¯åŸæ–‡ä¸­å­˜åœ¨çš„å¥å­ï¼‰ã€‚å¦‚æœæ²¡æœ‰å¼•ç”¨ï¼Œè¿”å› nullã€‚
  - `anno_summary`: (string) 5-10ä¸ªå­—çš„ç®€çŸ­æ‘˜è¦ï¼Œç”¨äºç”Ÿæˆç¬”è®°æ ‡é¢˜ã€‚

**ç¤ºä¾‹**ï¼š
ä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ
é€’å½’æ˜¯æŒ‡å‡½æ•°è°ƒç”¨è‡ªèº«çš„ç¼–ç¨‹æŠ€å·§...ï¼ˆè§£é‡Šå†…å®¹ï¼‰

**æ€è€ƒé¢˜ï¼šä½ èƒ½æƒ³åˆ°ç”Ÿæ´»ä¸­æœ‰ä»€ä¹ˆç°è±¡æ˜¯ç±»ä¼¼äºé€’å½’çš„å—ï¼Ÿ**

---METADATA---
{{"node_id": "uuid-123", "quote": "é€’å½’æ˜¯...", "anno_summary": "é€’å½’çš„æ¦‚å¿µ"}}
"""

        # Build prompt
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-5:]])
        
        prompt = f"""
è¯¾ç¨‹å†…å®¹ç‰‡æ®µï¼š
{context}

å¯¹è¯å†å²ï¼š
{history_text}

é€‰ä¸­å†…å®¹ï¼ˆç”¨æˆ·é’ˆå¯¹è¿™æ®µæ–‡å­—æé—®ï¼‰ï¼š
{selection if selection else "æ— "}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·å¼€å§‹å›ç­”ï¼ˆè®°å¾—åœ¨æœ€åé™„åŠ å…ƒæ•°æ®ï¼‰ï¼š
"""
        async for chunk in self._stream_llm(prompt, system_prompt):
            yield chunk

    async def summarize_note(self, content: str) -> str:
        """
        Generate a concise title/summary for a note content.
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¬”è®°æ•´ç†å‘˜ã€‚è¯·ä¸ºç»™å®šçš„ç¬”è®°å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­ã€æ ¸å¿ƒçš„æ ‡é¢˜ï¼ˆSummaryï¼‰ã€‚

è¦æ±‚ï¼š
1. **ç²¾ç®€**ï¼šå­—æ•°æ§åˆ¶åœ¨ 10-20 å­—ä»¥å†…ã€‚
2. **æ ¸å¿ƒ**ï¼šç›´æ¥æ¦‚æ‹¬ç¬”è®°çš„æ ¸å¿ƒè§‚ç‚¹æˆ–çŸ¥è¯†ç‚¹ã€‚
3. **æ ¼å¼**ï¼šç›´æ¥è¾“å‡ºæ ‡é¢˜æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•å‰ç¼€æˆ–ç¬¦å·ã€‚
"""
        # If content contains Q&A structure, try to summarize the Question primarily
        prompt = f"ç¬”è®°å†…å®¹ï¼š\n{content[:2000]}\n\nè¯·ç”Ÿæˆæ ‡é¢˜ï¼š"
        
        # Use Fast Model
        response = await self._call_llm(prompt, system_prompt, use_fast_model=True)
        return response if response else (content[:20] + "...")

    async def summarize_chat(self, history: List[dict], course_context: str = "", user_persona: str = "") -> Dict:
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦ä¹ å¤ç›˜ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„å¯¹è¯å†å²ï¼Œç”Ÿæˆä¸€ä»½é«˜è´¨é‡çš„**å­¦ä¹ å¤ç›˜æŠ¥å‘Š**ã€‚

**ç”¨æˆ·ç”»åƒ**ï¼š
{user_persona if user_persona else "é€šç”¨å­¦ä¹ è€…"}

**æ ¸å¿ƒè¦æ±‚**ï¼š
1. **çœŸå®å…¨é¢**ï¼šå¿½ç•¥å¯’æš„å’Œæ— ç”¨ä¿¡æ¯ï¼Œç²¾å‡†æ•æ‰æ ¸å¿ƒå†…å®¹ã€‚
2. **å†…å®¹è¯¦å®**ï¼šæ¯ä¸ªéƒ¨åˆ†éƒ½è¦è¯¦ç»†å±•å¼€ï¼Œä¸è¦ç®€å•æ¦‚æ‹¬ã€‚
   - å¡ç‚¹ï¼šè¯¦ç»†æè¿°ç”¨æˆ·çš„é—®é¢˜èƒŒæ™¯ã€å…·ä½“å›°æƒ‘ç‚¹ã€å°è¯•è¿‡çš„è§£å†³æ€è·¯
   - è§£ç­”ï¼šå®Œæ•´é˜è¿°æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼ŒåŒ…æ‹¬åŸç†ã€é€»è¾‘ã€å…³é”®æ­¥éª¤ï¼Œå¿…è¦æ—¶ä¸¾ä¾‹è¯´æ˜
   - å¯å‘ï¼šæ·±å…¥åˆ†æå»¶ä¼¸æ€è€ƒï¼Œæä¾›å®é™…åº”ç”¨åœºæ™¯å’Œå­¦ä¹ å»ºè®®
3. **ç»“æ„åŒ–è¾“å‡º**ï¼šå¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š
   - **ğŸ”´ å¡ç‚¹ (Stuck Point)**ï¼šç”¨æˆ·æœ€åˆé‡åˆ°çš„å›°éš¾ã€è¯¯åŒºæˆ–ç–‘æƒ‘æ˜¯ä»€ä¹ˆï¼Ÿ
   - **ğŸŸ¢ è§£ç­” (Solution)**ï¼šæœ€ç»ˆè§£å†³é—®é¢˜çš„å…³é”®çŸ¥è¯†ç‚¹ã€é€»è¾‘æˆ–æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ
   - **âœ¨ å¯å‘ (Inspiration)**ï¼šä»è¿™ä¸ªé—®é¢˜ä¸­å»¶ä¼¸å‡ºçš„æ€è€ƒã€ä¸¾ä¸€åä¸‰çš„åº”ç”¨æˆ–å¯¹æœªæ¥çš„æŒ‡å¯¼æ„ä¹‰ã€‚
4. **å­—æ•°è¦æ±‚**ï¼šcontent å­—æ®µè‡³å°‘ 300-500 å­—ï¼Œç¡®ä¿å†…å®¹å……å®æœ‰ä»·å€¼ã€‚
5. **å‡ºé¢˜å»ºè®®**ï¼šåŸºäºæœ¬è½®å¯¹è¯çš„çŸ¥è¯†ç‚¹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å¿…è¦è¿›è¡Œæµ‹éªŒã€‚

**è¾“å‡ºæ ¼å¼**ï¼š
ç›´æ¥è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼ˆä¸è¦ markdown ä»£ç å—ï¼‰ï¼š
{{
  "title": "å¤ç›˜ï¼š[æ ¸å¿ƒä¸»é¢˜]",
  "content": "Markdown æ ¼å¼çš„è¯¦ç»†å¤ç›˜å†…å®¹ï¼ŒåŒ…å«å®Œæ•´çš„çŸ¥è¯†ç‚¹é˜è¿°ã€åŸç†è§£é‡Šå’Œå®é™…åº”ç”¨...",
  "stuck_point": "è¯¦ç»†æè¿°å¡ç‚¹",
  "solution": "è¯¦ç»†æè¿°è§£ç­”",
  "inspiration": "è¯¦ç»†æè¿°å¯å‘",
  "suggest_quiz": true/false
}}
"""
        # Convert history to text
        history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        
        prompt = f"è¯¾ç¨‹èƒŒæ™¯ï¼š\n{course_context}\n\nå¯¹è¯å†å²ï¼š\n{history_text}\n\nè¯·ç”Ÿæˆè¯¦ç»†çš„å¤ç›˜æŠ¥å‘Šï¼Œç¡®ä¿å†…å®¹ä¸°å¯Œå……å®ï¼š"
        
        # Use standard model for better quality summary
        response = await self._call_llm(prompt, system_prompt, use_fast_model=False)
        if response:
            return self._extract_json(response) or {"title": "å¯¹è¯æ€»ç»“", "content": response}
        return {"title": "æ€»ç»“å¤±è´¥", "content": "æ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"}

    async def summarize_history(self, history: List[Dict]) -> str:
        """
        Summarizes conversation history using LLM.
        """
        system_prompt = """
You are a Conversation Summarizer.
Your task is to condense the provided conversation history into a concise summary that preserves key context, user intent, and important details.
The summary will be used as "Long-term Memory" for an AI assistant.

Requirements:
1. Identify the main topic(s) discussed.
2. Preserve any specific user questions and the core of the answers.
3. Keep it dense and information-rich (avoid fluff).
4. Use third-person perspective (e.g., "User asked about X, AI explained Y").
"""
        history_text = "\n".join([f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" for msg in history])
        
        prompt = f"Please summarize the following conversation:\n\n{history_text}"
        
        # Use Fast Model for summarization
        response = await self._call_llm(prompt, system_prompt, use_fast_model=True)
        return response if response else "Previous conversation summary (auto-generated failed)."

    async def generate_knowledge_graph(self, course_name: str, course_context: str, nodes: List[Dict]) -> Dict:
        """
        Generate a knowledge graph structure based on course content.
        
        Args:
            course_name: Name of the course
            course_context: Full course outline/context
            nodes: List of course nodes with their content
            
        Returns:
            Dictionary containing nodes and edges for the knowledge graph
        """
        from prompts import get_prompt
        
        # Build course context summary
        nodes_summary = []
        for node in nodes[:20]:  # Limit to first 20 nodes to avoid token limit
            nodes_summary.append({
                "id": node.get("node_id", ""),
                "name": node.get("node_name", ""),
                "level": node.get("node_level", 1),
                "content": node.get("node_content", "")[:100]  # Truncate content
            })
        
        context_text = f"""
è¯¾ç¨‹åç§°ï¼š{course_name}

è¯¾ç¨‹å¤§çº²ï¼š
{course_context}

ç« èŠ‚åˆ—è¡¨ï¼š
{json.dumps(nodes_summary, ensure_ascii=False, indent=2)}
"""
        
        # Get the knowledge graph prompt template
        prompt_template = get_prompt("generate_knowledge_graph")
        system_prompt = prompt_template.format(
            course_name=course_name,
            course_context=context_text
        )
        
        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹è¯¾ç¨‹å†…å®¹ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼š

è¯¾ç¨‹åç§°ï¼š{course_name}

ä¸»è¦ç« èŠ‚ï¼š
{chr(10).join([f"- {n.get('node_name', '')}: {n.get('node_content', '')[:50]}..." for n in nodes_summary[:15]])}

è¯·ç”ŸæˆåŒ…å«èŠ‚ç‚¹å’Œå…³ç³»çš„çŸ¥è¯†å›¾è°±JSONã€‚"""
        
        response = await self._call_llm(user_prompt, system_prompt)
        
        if response:
            result = self._extract_json(response)
            if result and "nodes" in result and "edges" in result:
                return result
        
        # Fallback: Generate a simple graph based on node hierarchy
        logger.warning("Knowledge graph generation failed, using fallback")
        return self._generate_fallback_knowledge_graph(nodes)
    
    def _generate_fallback_knowledge_graph(self, nodes: List[Dict]) -> Dict:
        """
        Generate a simple fallback knowledge graph based on node hierarchy.
        """
        graph_nodes = []
        graph_edges = []
        
        # Create nodes
        for node in nodes[:15]:
            node_id = node.get("node_id", str(uuid.uuid4()))
            node_level = node.get("node_level", 1)
            
            # Determine node type based on level
            if node_level == 1:
                node_type = "core"
            elif node_level == 2:
                node_type = "basic"
            else:
                node_type = "advanced"
            
            graph_nodes.append({
                "id": node_id,
                "label": node.get("node_name", "Unknown"),
                "type": node_type,
                "description": node.get("node_content", "")[:50],
                "chapter_id": node_id
            })
        
        # Create edges based on parent-child relationships
        node_map = {n["id"]: n for n in graph_nodes}
        for node in nodes[:15]:
            node_id = node.get("node_id", "")
            parent_id = node.get("parent_node_id", "")
            
            if parent_id and parent_id in node_map and node_id in node_map:
                graph_edges.append({
                    "source": parent_id,
                    "target": node_id,
                    "relation": "contains",
                    "label": "åŒ…å«"
                })
        
        # Add some cross-references between same-level nodes
        level_groups = {}
        for node in graph_nodes:
            level = node.get("type", "basic")
            if level not in level_groups:
                level_groups[level] = []
            level_groups[level].append(node)
        
        # Connect nodes within same level
        for level, group in level_groups.items():
            for i in range(len(group) - 1):
                if len(graph_edges) < 30:  # Limit total edges
                    graph_edges.append({
                        "source": group[i]["id"],
                        "target": group[i + 1]["id"],
                        "relation": "related",
                        "label": "å…³è”"
                    })
        
        return {
            "nodes": graph_nodes,
            "edges": graph_edges
        }

    def locate_node(self, keyword: str, all_nodes: List[Dict]) -> Dict:
        # Simple mock search - Semantic search requires embedding, sticking to keyword match for now
        # Or could use LLM to pick from list if list is small, but for MVP keyword is safer/faster
        for node in all_nodes:
            if keyword in node['node_name']:
                return {
                    "match_node_id": node['node_id'],
                    "match_node_name": node['node_name'],
                    "node_path": "Path/To/Node" # Mock path
                }
        return {}


ai_service = AIService()
